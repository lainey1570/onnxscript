graph(
    name=main_graph,
    inputs=(
        %"input_ids"<INT64,[batch_size,sequence_length]>,
        %"attention_mask"<INT64,[batch_size,total_sequence_length]>,
        %"past_key_values.0.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.0.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.1.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.1.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.2.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.2.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.3.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.3.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.4.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.4.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.5.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.5.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.6.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.6.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.7.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.7.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.8.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.8.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.9.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.9.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.10.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.10.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.11.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.11.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.12.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.12.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.13.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.13.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.14.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.14.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.15.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.15.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.16.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.16.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.17.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.17.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.18.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.18.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.19.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.19.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.20.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.20.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.21.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.21.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.22.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.22.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.23.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.23.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.24.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.24.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.25.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.25.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.26.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.26.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.27.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.27.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.28.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.28.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.29.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.29.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.30.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.30.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.31.key"<FLOAT16,[batch_size,8,past_sequence_length,128]>,
        %"past_key_values.31.value"<FLOAT16,[batch_size,8,past_sequence_length,128]>
    ),
    outputs=(
        %"logits"<FLOAT16,[batch_size,sequence_length,128256]>,
        %"present.0.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.0.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.1.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.1.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.2.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.2.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.3.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.3.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.4.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.4.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.5.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.5.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.6.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.6.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.7.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.7.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.8.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.8.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.9.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.9.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.10.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.10.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.11.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.11.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.12.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.12.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.13.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.13.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.14.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.14.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.15.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.15.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.16.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.16.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.17.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.17.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.18.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.18.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.19.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.19.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.20.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.20.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.21.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.21.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.22.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.22.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.23.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.23.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.24.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.24.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.25.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.25.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.26.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.26.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.27.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.27.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.28.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.28.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.29.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.29.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.30.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.30.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.31.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>,
        %"present.31.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>
    ),
    initializers=(
        %"model.embed_tokens.weight"<FLOAT16,[128256,4096]>,
        %"model.layers.0.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.0.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"cos_cache"<FLOAT16,[8192,64]>,
        %"sin_cache"<FLOAT16,[8192,64]>,
        %"model.layers.0.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.0.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.0.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.0.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.0.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.1.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.1.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.1.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.1.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.1.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.1.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.1.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.2.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.2.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.2.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.2.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.2.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.2.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.2.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.3.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.3.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.3.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.3.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.3.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.3.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.3.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.4.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.4.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.4.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.4.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.4.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.4.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.4.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.5.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.5.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.5.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.5.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.5.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.5.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.5.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.6.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.6.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.6.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.6.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.6.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.6.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.6.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.7.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.7.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.7.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.7.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.7.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.7.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.7.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.8.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.8.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.8.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.8.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.8.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.8.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.8.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.9.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.9.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.9.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.9.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.9.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.9.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.9.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.10.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.10.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.10.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.10.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.10.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.10.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.10.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.11.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.11.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.11.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.11.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.11.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.11.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.11.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.12.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.12.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.12.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.12.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.12.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.12.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.12.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.13.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.13.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.13.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.13.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.13.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.13.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.13.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.14.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.14.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.14.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.14.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.14.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.14.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.14.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.15.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.15.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.15.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.15.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.15.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.15.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.15.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.16.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.16.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.16.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.16.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.16.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.16.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.16.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.17.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.17.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.17.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.17.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.17.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.17.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.17.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.18.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.18.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.18.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.18.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.18.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.18.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.18.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.19.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.19.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.19.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.19.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.19.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.19.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.19.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.20.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.20.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.20.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.20.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.20.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.20.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.20.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.21.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.21.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.21.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.21.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.21.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.21.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.21.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.22.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.22.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.22.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.22.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.22.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.22.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.22.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.23.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.23.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.23.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.23.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.23.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.23.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.23.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.24.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.24.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.24.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.24.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.24.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.24.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.24.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.25.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.25.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.25.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.25.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.25.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.25.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.25.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.26.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.26.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.26.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.26.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.26.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.26.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.26.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.27.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.27.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.27.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.27.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.27.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.27.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.27.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.28.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.28.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.28.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.28.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.28.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.28.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.28.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.29.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.29.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.29.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.29.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.29.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.29.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.29.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.30.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.30.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.30.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.30.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.30.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.30.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.30.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.31.input_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.31.attn.qkv_proj.MatMul.weight"<FLOAT16,[4096,6144]>,
        %"model.layers.31.attn.o_proj.MatMul.weight"<FLOAT16,[4096,4096]>,
        %"model.layers.31.post_attention_layernorm.weight"<FLOAT16,[4096]>,
        %"model.layers.31.mlp.gate_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.31.mlp.up_proj.MatMul.weight"<FLOAT16,[4096,14336]>,
        %"model.layers.31.mlp.down_proj.MatMul.weight"<FLOAT16,[14336,4096]>,
        %"model.layers.32.final_norm_layernorm.weight"<FLOAT16,[4096]>,
        %"lm_head.MatMul.weight"<FLOAT16,[4096,128256]>
    ),
) {
      0 |  # /model/constant_nodes/TensorProto.INT64/1D/1
           %"/model/constants/TensorProto.INT64/1D/1"<INT64,[]> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='/model/numpy_helper/TensorProto.INT64/1D/1')}
      1 |  # /model/attn_mask_reformat/attn_mask_subgraph/ReduceSum
           %"/model/attn_mask_reformat/attn_mask_subgraph/ReduceSum/output_0"<INT64,[batch_size,1]> ⬅️ ::ReduceSum(%"attention_mask", %"/model/constants/TensorProto.INT64/1D/1")
      2 |  # /model/attn_mask_reformat/attn_mask_subgraph/Sub
           %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/output_0"<INT64,[batch_size,1]> ⬅️ ::Sub(%"/model/attn_mask_reformat/attn_mask_subgraph/ReduceSum/output_0", %"/model/constants/TensorProto.INT64/1D/1")
      3 |  # /model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast
           %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0"<INT32,[batch_size,1]> ⬅️ ::Cast(%"/model/attn_mask_reformat/attn_mask_subgraph/Sub/output_0") {to=6}
      4 |  # /model/attn_mask_reformat/attn_mask_subgraph/Shape
           %"/model/attn_mask_reformat/attn_mask_subgraph/Shape/output_0"<INT64,[2]> ⬅️ ::Shape(%"attention_mask")
      5 |  # /model/constant_nodes/TensorProto.INT64/0D/1
           %"/model/constants/TensorProto.INT64/0D/1"<INT64,[]> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='/model/numpy_helper/TensorProto.INT64/0D/1')}
      6 |  # /model/attn_mask_reformat/attn_mask_subgraph/Gather
           %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/output_0"<INT64,[]> ⬅️ ::Gather(%"/model/attn_mask_reformat/attn_mask_subgraph/Shape/output_0", %"/model/constants/TensorProto.INT64/0D/1") {axis=0}
      7 |  # /model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast
           %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0"<INT32,?> ⬅️ ::Cast(%"/model/attn_mask_reformat/attn_mask_subgraph/Gather/output_0") {to=6}
      8 |  # /model/embed_tokens/Gather
           %"/model/embed_tokens/Gather/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::Gather(%"model.embed_tokens.weight", %"input_ids")
      9 |  # /model/layers.0/input_layernorm/LayerNorm
           %"/model/layers.0/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::SimplifiedLayerNormalization(%"/model/embed_tokens/Gather/output_0", %"model.layers.0.input_layernorm.weight") {epsilon=9.999999747378752e-06, axis=-1, stash_type=1}
     10 |  # /model/layers.0/attn/qkv_proj/MatMul
           %"/model/layers.0/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.0/input_layernorm/output_0", %"model.layers.0.attn.qkv_proj.MatMul.weight")
     11 |  # /model/layers.0/attn/GroupQueryAttention
           %"/model/layers.0/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.0.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.0.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.0/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.0.key", %"past_key_values.0.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     12 |  # /model/layers.0/attn/o_proj/MatMul
           %"/model/layers.0/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.0/attn/GroupQueryAttention/output_0", %"model.layers.0.attn.o_proj.MatMul.weight")
     13 |  # /model/layers.0/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.0/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.0/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/embed_tokens/Gather/output_0", %"/model/layers.0/attn/o_proj/MatMul/output_0", %"model.layers.0.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     14 |  # /model/layers.0/mlp/gate_proj/MatMul
           %"/model/layers.0/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.0/post_attention_layernorm/output_0", %"model.layers.0.mlp.gate_proj.MatMul.weight")
     15 |  # /model/layers.0/mlp/up_proj/MatMul
           %"/model/layers.0/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.0/post_attention_layernorm/output_0", %"model.layers.0.mlp.up_proj.MatMul.weight")
     16 |  # /model/layers.0/mlp/act_fn/Sigmoid
           %"/model/layers.0/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.0/mlp/gate_proj/MatMul/output_0")
     17 |  # /model/layers.0/mlp/act_fn/Mul
           %"/model/layers.0/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.0/mlp/gate_proj/MatMul/output_0", %"/model/layers.0/mlp/act_fn/Sigmoid/output_0")
     18 |  # /model/layers.0/mlp/Mul
           %"/model/layers.0/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.0/mlp/act_fn/Mul/output_0", %"/model/layers.0/mlp/up_proj/MatMul/output_0")
     19 |  # /model/layers.0/mlp/down_proj/MatMul
           %"/model/layers.0/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.0/mlp/Mul/output_0", %"model.layers.0.mlp.down_proj.MatMul.weight")
     20 |  # /model/layers.1/input_layernorm/SkipLayerNorm
           %"/model/layers.1/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.1/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.0/post_attention_layernorm/output_3", %"/model/layers.0/mlp/down_proj/MatMul/output_0", %"model.layers.1.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     21 |  # /model/layers.1/attn/qkv_proj/MatMul
           %"/model/layers.1/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.1/input_layernorm/output_0", %"model.layers.1.attn.qkv_proj.MatMul.weight")
     22 |  # /model/layers.1/attn/GroupQueryAttention
           %"/model/layers.1/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.1.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.1.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.1/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.1.key", %"past_key_values.1.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     23 |  # /model/layers.1/attn/o_proj/MatMul
           %"/model/layers.1/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.1/attn/GroupQueryAttention/output_0", %"model.layers.1.attn.o_proj.MatMul.weight")
     24 |  # /model/layers.1/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.1/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.1/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.1/input_layernorm/output_3", %"/model/layers.1/attn/o_proj/MatMul/output_0", %"model.layers.1.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     25 |  # /model/layers.1/mlp/gate_proj/MatMul
           %"/model/layers.1/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.1/post_attention_layernorm/output_0", %"model.layers.1.mlp.gate_proj.MatMul.weight")
     26 |  # /model/layers.1/mlp/up_proj/MatMul
           %"/model/layers.1/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.1/post_attention_layernorm/output_0", %"model.layers.1.mlp.up_proj.MatMul.weight")
     27 |  # /model/layers.1/mlp/act_fn/Sigmoid
           %"/model/layers.1/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.1/mlp/gate_proj/MatMul/output_0")
     28 |  # /model/layers.1/mlp/act_fn/Mul
           %"/model/layers.1/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.1/mlp/gate_proj/MatMul/output_0", %"/model/layers.1/mlp/act_fn/Sigmoid/output_0")
     29 |  # /model/layers.1/mlp/Mul
           %"/model/layers.1/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.1/mlp/act_fn/Mul/output_0", %"/model/layers.1/mlp/up_proj/MatMul/output_0")
     30 |  # /model/layers.1/mlp/down_proj/MatMul
           %"/model/layers.1/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.1/mlp/Mul/output_0", %"model.layers.1.mlp.down_proj.MatMul.weight")
     31 |  # /model/layers.2/input_layernorm/SkipLayerNorm
           %"/model/layers.2/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.2/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.1/post_attention_layernorm/output_3", %"/model/layers.1/mlp/down_proj/MatMul/output_0", %"model.layers.2.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     32 |  # /model/layers.2/attn/qkv_proj/MatMul
           %"/model/layers.2/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.2/input_layernorm/output_0", %"model.layers.2.attn.qkv_proj.MatMul.weight")
     33 |  # /model/layers.2/attn/GroupQueryAttention
           %"/model/layers.2/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.2.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.2.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.2/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.2.key", %"past_key_values.2.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     34 |  # /model/layers.2/attn/o_proj/MatMul
           %"/model/layers.2/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.2/attn/GroupQueryAttention/output_0", %"model.layers.2.attn.o_proj.MatMul.weight")
     35 |  # /model/layers.2/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.2/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.2/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.2/input_layernorm/output_3", %"/model/layers.2/attn/o_proj/MatMul/output_0", %"model.layers.2.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     36 |  # /model/layers.2/mlp/gate_proj/MatMul
           %"/model/layers.2/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.2/post_attention_layernorm/output_0", %"model.layers.2.mlp.gate_proj.MatMul.weight")
     37 |  # /model/layers.2/mlp/up_proj/MatMul
           %"/model/layers.2/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.2/post_attention_layernorm/output_0", %"model.layers.2.mlp.up_proj.MatMul.weight")
     38 |  # /model/layers.2/mlp/act_fn/Sigmoid
           %"/model/layers.2/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.2/mlp/gate_proj/MatMul/output_0")
     39 |  # /model/layers.2/mlp/act_fn/Mul
           %"/model/layers.2/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.2/mlp/gate_proj/MatMul/output_0", %"/model/layers.2/mlp/act_fn/Sigmoid/output_0")
     40 |  # /model/layers.2/mlp/Mul
           %"/model/layers.2/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.2/mlp/act_fn/Mul/output_0", %"/model/layers.2/mlp/up_proj/MatMul/output_0")
     41 |  # /model/layers.2/mlp/down_proj/MatMul
           %"/model/layers.2/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.2/mlp/Mul/output_0", %"model.layers.2.mlp.down_proj.MatMul.weight")
     42 |  # /model/layers.3/input_layernorm/SkipLayerNorm
           %"/model/layers.3/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.3/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.2/post_attention_layernorm/output_3", %"/model/layers.2/mlp/down_proj/MatMul/output_0", %"model.layers.3.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     43 |  # /model/layers.3/attn/qkv_proj/MatMul
           %"/model/layers.3/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.3/input_layernorm/output_0", %"model.layers.3.attn.qkv_proj.MatMul.weight")
     44 |  # /model/layers.3/attn/GroupQueryAttention
           %"/model/layers.3/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.3.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.3.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.3/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.3.key", %"past_key_values.3.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     45 |  # /model/layers.3/attn/o_proj/MatMul
           %"/model/layers.3/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.3/attn/GroupQueryAttention/output_0", %"model.layers.3.attn.o_proj.MatMul.weight")
     46 |  # /model/layers.3/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.3/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.3/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.3/input_layernorm/output_3", %"/model/layers.3/attn/o_proj/MatMul/output_0", %"model.layers.3.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     47 |  # /model/layers.3/mlp/gate_proj/MatMul
           %"/model/layers.3/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.3/post_attention_layernorm/output_0", %"model.layers.3.mlp.gate_proj.MatMul.weight")
     48 |  # /model/layers.3/mlp/up_proj/MatMul
           %"/model/layers.3/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.3/post_attention_layernorm/output_0", %"model.layers.3.mlp.up_proj.MatMul.weight")
     49 |  # /model/layers.3/mlp/act_fn/Sigmoid
           %"/model/layers.3/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.3/mlp/gate_proj/MatMul/output_0")
     50 |  # /model/layers.3/mlp/act_fn/Mul
           %"/model/layers.3/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.3/mlp/gate_proj/MatMul/output_0", %"/model/layers.3/mlp/act_fn/Sigmoid/output_0")
     51 |  # /model/layers.3/mlp/Mul
           %"/model/layers.3/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.3/mlp/act_fn/Mul/output_0", %"/model/layers.3/mlp/up_proj/MatMul/output_0")
     52 |  # /model/layers.3/mlp/down_proj/MatMul
           %"/model/layers.3/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.3/mlp/Mul/output_0", %"model.layers.3.mlp.down_proj.MatMul.weight")
     53 |  # /model/layers.4/input_layernorm/SkipLayerNorm
           %"/model/layers.4/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.4/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.3/post_attention_layernorm/output_3", %"/model/layers.3/mlp/down_proj/MatMul/output_0", %"model.layers.4.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     54 |  # /model/layers.4/attn/qkv_proj/MatMul
           %"/model/layers.4/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.4/input_layernorm/output_0", %"model.layers.4.attn.qkv_proj.MatMul.weight")
     55 |  # /model/layers.4/attn/GroupQueryAttention
           %"/model/layers.4/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.4.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.4.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.4/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.4.key", %"past_key_values.4.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     56 |  # /model/layers.4/attn/o_proj/MatMul
           %"/model/layers.4/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.4/attn/GroupQueryAttention/output_0", %"model.layers.4.attn.o_proj.MatMul.weight")
     57 |  # /model/layers.4/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.4/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.4/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.4/input_layernorm/output_3", %"/model/layers.4/attn/o_proj/MatMul/output_0", %"model.layers.4.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     58 |  # /model/layers.4/mlp/gate_proj/MatMul
           %"/model/layers.4/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.4/post_attention_layernorm/output_0", %"model.layers.4.mlp.gate_proj.MatMul.weight")
     59 |  # /model/layers.4/mlp/up_proj/MatMul
           %"/model/layers.4/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.4/post_attention_layernorm/output_0", %"model.layers.4.mlp.up_proj.MatMul.weight")
     60 |  # /model/layers.4/mlp/act_fn/Sigmoid
           %"/model/layers.4/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.4/mlp/gate_proj/MatMul/output_0")
     61 |  # /model/layers.4/mlp/act_fn/Mul
           %"/model/layers.4/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.4/mlp/gate_proj/MatMul/output_0", %"/model/layers.4/mlp/act_fn/Sigmoid/output_0")
     62 |  # /model/layers.4/mlp/Mul
           %"/model/layers.4/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.4/mlp/act_fn/Mul/output_0", %"/model/layers.4/mlp/up_proj/MatMul/output_0")
     63 |  # /model/layers.4/mlp/down_proj/MatMul
           %"/model/layers.4/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.4/mlp/Mul/output_0", %"model.layers.4.mlp.down_proj.MatMul.weight")
     64 |  # /model/layers.5/input_layernorm/SkipLayerNorm
           %"/model/layers.5/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.5/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.4/post_attention_layernorm/output_3", %"/model/layers.4/mlp/down_proj/MatMul/output_0", %"model.layers.5.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     65 |  # /model/layers.5/attn/qkv_proj/MatMul
           %"/model/layers.5/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.5/input_layernorm/output_0", %"model.layers.5.attn.qkv_proj.MatMul.weight")
     66 |  # /model/layers.5/attn/GroupQueryAttention
           %"/model/layers.5/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.5.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.5.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.5/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.5.key", %"past_key_values.5.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     67 |  # /model/layers.5/attn/o_proj/MatMul
           %"/model/layers.5/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.5/attn/GroupQueryAttention/output_0", %"model.layers.5.attn.o_proj.MatMul.weight")
     68 |  # /model/layers.5/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.5/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.5/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.5/input_layernorm/output_3", %"/model/layers.5/attn/o_proj/MatMul/output_0", %"model.layers.5.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     69 |  # /model/layers.5/mlp/gate_proj/MatMul
           %"/model/layers.5/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.5/post_attention_layernorm/output_0", %"model.layers.5.mlp.gate_proj.MatMul.weight")
     70 |  # /model/layers.5/mlp/up_proj/MatMul
           %"/model/layers.5/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.5/post_attention_layernorm/output_0", %"model.layers.5.mlp.up_proj.MatMul.weight")
     71 |  # /model/layers.5/mlp/act_fn/Sigmoid
           %"/model/layers.5/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.5/mlp/gate_proj/MatMul/output_0")
     72 |  # /model/layers.5/mlp/act_fn/Mul
           %"/model/layers.5/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.5/mlp/gate_proj/MatMul/output_0", %"/model/layers.5/mlp/act_fn/Sigmoid/output_0")
     73 |  # /model/layers.5/mlp/Mul
           %"/model/layers.5/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.5/mlp/act_fn/Mul/output_0", %"/model/layers.5/mlp/up_proj/MatMul/output_0")
     74 |  # /model/layers.5/mlp/down_proj/MatMul
           %"/model/layers.5/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.5/mlp/Mul/output_0", %"model.layers.5.mlp.down_proj.MatMul.weight")
     75 |  # /model/layers.6/input_layernorm/SkipLayerNorm
           %"/model/layers.6/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.6/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.5/post_attention_layernorm/output_3", %"/model/layers.5/mlp/down_proj/MatMul/output_0", %"model.layers.6.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     76 |  # /model/layers.6/attn/qkv_proj/MatMul
           %"/model/layers.6/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.6/input_layernorm/output_0", %"model.layers.6.attn.qkv_proj.MatMul.weight")
     77 |  # /model/layers.6/attn/GroupQueryAttention
           %"/model/layers.6/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.6.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.6.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.6/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.6.key", %"past_key_values.6.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     78 |  # /model/layers.6/attn/o_proj/MatMul
           %"/model/layers.6/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.6/attn/GroupQueryAttention/output_0", %"model.layers.6.attn.o_proj.MatMul.weight")
     79 |  # /model/layers.6/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.6/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.6/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.6/input_layernorm/output_3", %"/model/layers.6/attn/o_proj/MatMul/output_0", %"model.layers.6.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     80 |  # /model/layers.6/mlp/gate_proj/MatMul
           %"/model/layers.6/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.6/post_attention_layernorm/output_0", %"model.layers.6.mlp.gate_proj.MatMul.weight")
     81 |  # /model/layers.6/mlp/up_proj/MatMul
           %"/model/layers.6/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.6/post_attention_layernorm/output_0", %"model.layers.6.mlp.up_proj.MatMul.weight")
     82 |  # /model/layers.6/mlp/act_fn/Sigmoid
           %"/model/layers.6/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.6/mlp/gate_proj/MatMul/output_0")
     83 |  # /model/layers.6/mlp/act_fn/Mul
           %"/model/layers.6/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.6/mlp/gate_proj/MatMul/output_0", %"/model/layers.6/mlp/act_fn/Sigmoid/output_0")
     84 |  # /model/layers.6/mlp/Mul
           %"/model/layers.6/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.6/mlp/act_fn/Mul/output_0", %"/model/layers.6/mlp/up_proj/MatMul/output_0")
     85 |  # /model/layers.6/mlp/down_proj/MatMul
           %"/model/layers.6/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.6/mlp/Mul/output_0", %"model.layers.6.mlp.down_proj.MatMul.weight")
     86 |  # /model/layers.7/input_layernorm/SkipLayerNorm
           %"/model/layers.7/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.7/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.6/post_attention_layernorm/output_3", %"/model/layers.6/mlp/down_proj/MatMul/output_0", %"model.layers.7.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     87 |  # /model/layers.7/attn/qkv_proj/MatMul
           %"/model/layers.7/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.7/input_layernorm/output_0", %"model.layers.7.attn.qkv_proj.MatMul.weight")
     88 |  # /model/layers.7/attn/GroupQueryAttention
           %"/model/layers.7/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.7.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.7.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.7/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.7.key", %"past_key_values.7.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
     89 |  # /model/layers.7/attn/o_proj/MatMul
           %"/model/layers.7/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.7/attn/GroupQueryAttention/output_0", %"model.layers.7.attn.o_proj.MatMul.weight")
     90 |  # /model/layers.7/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.7/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.7/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.7/input_layernorm/output_3", %"/model/layers.7/attn/o_proj/MatMul/output_0", %"model.layers.7.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
     91 |  # /model/layers.7/mlp/gate_proj/MatMul
           %"/model/layers.7/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.7/post_attention_layernorm/output_0", %"model.layers.7.mlp.gate_proj.MatMul.weight")
     92 |  # /model/layers.7/mlp/up_proj/MatMul
           %"/model/layers.7/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.7/post_attention_layernorm/output_0", %"model.layers.7.mlp.up_proj.MatMul.weight")
     93 |  # /model/layers.7/mlp/act_fn/Sigmoid
           %"/model/layers.7/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.7/mlp/gate_proj/MatMul/output_0")
     94 |  # /model/layers.7/mlp/act_fn/Mul
           %"/model/layers.7/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.7/mlp/gate_proj/MatMul/output_0", %"/model/layers.7/mlp/act_fn/Sigmoid/output_0")
     95 |  # /model/layers.7/mlp/Mul
           %"/model/layers.7/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.7/mlp/act_fn/Mul/output_0", %"/model/layers.7/mlp/up_proj/MatMul/output_0")
     96 |  # /model/layers.7/mlp/down_proj/MatMul
           %"/model/layers.7/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.7/mlp/Mul/output_0", %"model.layers.7.mlp.down_proj.MatMul.weight")
     97 |  # /model/layers.8/input_layernorm/SkipLayerNorm
           %"/model/layers.8/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.8/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.7/post_attention_layernorm/output_3", %"/model/layers.7/mlp/down_proj/MatMul/output_0", %"model.layers.8.input_layernorm.weight") {epsilon=9.999999747378752e-06}
     98 |  # /model/layers.8/attn/qkv_proj/MatMul
           %"/model/layers.8/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.8/input_layernorm/output_0", %"model.layers.8.attn.qkv_proj.MatMul.weight")
     99 |  # /model/layers.8/attn/GroupQueryAttention
           %"/model/layers.8/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.8.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.8.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.8/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.8.key", %"past_key_values.8.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    100 |  # /model/layers.8/attn/o_proj/MatMul
           %"/model/layers.8/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.8/attn/GroupQueryAttention/output_0", %"model.layers.8.attn.o_proj.MatMul.weight")
    101 |  # /model/layers.8/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.8/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.8/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.8/input_layernorm/output_3", %"/model/layers.8/attn/o_proj/MatMul/output_0", %"model.layers.8.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    102 |  # /model/layers.8/mlp/gate_proj/MatMul
           %"/model/layers.8/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.8/post_attention_layernorm/output_0", %"model.layers.8.mlp.gate_proj.MatMul.weight")
    103 |  # /model/layers.8/mlp/up_proj/MatMul
           %"/model/layers.8/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.8/post_attention_layernorm/output_0", %"model.layers.8.mlp.up_proj.MatMul.weight")
    104 |  # /model/layers.8/mlp/act_fn/Sigmoid
           %"/model/layers.8/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.8/mlp/gate_proj/MatMul/output_0")
    105 |  # /model/layers.8/mlp/act_fn/Mul
           %"/model/layers.8/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.8/mlp/gate_proj/MatMul/output_0", %"/model/layers.8/mlp/act_fn/Sigmoid/output_0")
    106 |  # /model/layers.8/mlp/Mul
           %"/model/layers.8/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.8/mlp/act_fn/Mul/output_0", %"/model/layers.8/mlp/up_proj/MatMul/output_0")
    107 |  # /model/layers.8/mlp/down_proj/MatMul
           %"/model/layers.8/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.8/mlp/Mul/output_0", %"model.layers.8.mlp.down_proj.MatMul.weight")
    108 |  # /model/layers.9/input_layernorm/SkipLayerNorm
           %"/model/layers.9/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.9/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.8/post_attention_layernorm/output_3", %"/model/layers.8/mlp/down_proj/MatMul/output_0", %"model.layers.9.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    109 |  # /model/layers.9/attn/qkv_proj/MatMul
           %"/model/layers.9/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.9/input_layernorm/output_0", %"model.layers.9.attn.qkv_proj.MatMul.weight")
    110 |  # /model/layers.9/attn/GroupQueryAttention
           %"/model/layers.9/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.9.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.9.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.9/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.9.key", %"past_key_values.9.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    111 |  # /model/layers.9/attn/o_proj/MatMul
           %"/model/layers.9/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.9/attn/GroupQueryAttention/output_0", %"model.layers.9.attn.o_proj.MatMul.weight")
    112 |  # /model/layers.9/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.9/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.9/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.9/input_layernorm/output_3", %"/model/layers.9/attn/o_proj/MatMul/output_0", %"model.layers.9.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    113 |  # /model/layers.9/mlp/gate_proj/MatMul
           %"/model/layers.9/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.9/post_attention_layernorm/output_0", %"model.layers.9.mlp.gate_proj.MatMul.weight")
    114 |  # /model/layers.9/mlp/up_proj/MatMul
           %"/model/layers.9/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.9/post_attention_layernorm/output_0", %"model.layers.9.mlp.up_proj.MatMul.weight")
    115 |  # /model/layers.9/mlp/act_fn/Sigmoid
           %"/model/layers.9/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.9/mlp/gate_proj/MatMul/output_0")
    116 |  # /model/layers.9/mlp/act_fn/Mul
           %"/model/layers.9/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.9/mlp/gate_proj/MatMul/output_0", %"/model/layers.9/mlp/act_fn/Sigmoid/output_0")
    117 |  # /model/layers.9/mlp/Mul
           %"/model/layers.9/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.9/mlp/act_fn/Mul/output_0", %"/model/layers.9/mlp/up_proj/MatMul/output_0")
    118 |  # /model/layers.9/mlp/down_proj/MatMul
           %"/model/layers.9/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.9/mlp/Mul/output_0", %"model.layers.9.mlp.down_proj.MatMul.weight")
    119 |  # /model/layers.10/input_layernorm/SkipLayerNorm
           %"/model/layers.10/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.10/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.9/post_attention_layernorm/output_3", %"/model/layers.9/mlp/down_proj/MatMul/output_0", %"model.layers.10.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    120 |  # /model/layers.10/attn/qkv_proj/MatMul
           %"/model/layers.10/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.10/input_layernorm/output_0", %"model.layers.10.attn.qkv_proj.MatMul.weight")
    121 |  # /model/layers.10/attn/GroupQueryAttention
           %"/model/layers.10/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.10.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.10.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.10/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.10.key", %"past_key_values.10.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    122 |  # /model/layers.10/attn/o_proj/MatMul
           %"/model/layers.10/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.10/attn/GroupQueryAttention/output_0", %"model.layers.10.attn.o_proj.MatMul.weight")
    123 |  # /model/layers.10/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.10/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.10/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.10/input_layernorm/output_3", %"/model/layers.10/attn/o_proj/MatMul/output_0", %"model.layers.10.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    124 |  # /model/layers.10/mlp/gate_proj/MatMul
           %"/model/layers.10/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.10/post_attention_layernorm/output_0", %"model.layers.10.mlp.gate_proj.MatMul.weight")
    125 |  # /model/layers.10/mlp/up_proj/MatMul
           %"/model/layers.10/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.10/post_attention_layernorm/output_0", %"model.layers.10.mlp.up_proj.MatMul.weight")
    126 |  # /model/layers.10/mlp/act_fn/Sigmoid
           %"/model/layers.10/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.10/mlp/gate_proj/MatMul/output_0")
    127 |  # /model/layers.10/mlp/act_fn/Mul
           %"/model/layers.10/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.10/mlp/gate_proj/MatMul/output_0", %"/model/layers.10/mlp/act_fn/Sigmoid/output_0")
    128 |  # /model/layers.10/mlp/Mul
           %"/model/layers.10/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.10/mlp/act_fn/Mul/output_0", %"/model/layers.10/mlp/up_proj/MatMul/output_0")
    129 |  # /model/layers.10/mlp/down_proj/MatMul
           %"/model/layers.10/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.10/mlp/Mul/output_0", %"model.layers.10.mlp.down_proj.MatMul.weight")
    130 |  # /model/layers.11/input_layernorm/SkipLayerNorm
           %"/model/layers.11/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.11/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.10/post_attention_layernorm/output_3", %"/model/layers.10/mlp/down_proj/MatMul/output_0", %"model.layers.11.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    131 |  # /model/layers.11/attn/qkv_proj/MatMul
           %"/model/layers.11/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.11/input_layernorm/output_0", %"model.layers.11.attn.qkv_proj.MatMul.weight")
    132 |  # /model/layers.11/attn/GroupQueryAttention
           %"/model/layers.11/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.11.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.11.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.11/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.11.key", %"past_key_values.11.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    133 |  # /model/layers.11/attn/o_proj/MatMul
           %"/model/layers.11/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.11/attn/GroupQueryAttention/output_0", %"model.layers.11.attn.o_proj.MatMul.weight")
    134 |  # /model/layers.11/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.11/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.11/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.11/input_layernorm/output_3", %"/model/layers.11/attn/o_proj/MatMul/output_0", %"model.layers.11.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    135 |  # /model/layers.11/mlp/gate_proj/MatMul
           %"/model/layers.11/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.11/post_attention_layernorm/output_0", %"model.layers.11.mlp.gate_proj.MatMul.weight")
    136 |  # /model/layers.11/mlp/up_proj/MatMul
           %"/model/layers.11/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.11/post_attention_layernorm/output_0", %"model.layers.11.mlp.up_proj.MatMul.weight")
    137 |  # /model/layers.11/mlp/act_fn/Sigmoid
           %"/model/layers.11/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.11/mlp/gate_proj/MatMul/output_0")
    138 |  # /model/layers.11/mlp/act_fn/Mul
           %"/model/layers.11/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.11/mlp/gate_proj/MatMul/output_0", %"/model/layers.11/mlp/act_fn/Sigmoid/output_0")
    139 |  # /model/layers.11/mlp/Mul
           %"/model/layers.11/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.11/mlp/act_fn/Mul/output_0", %"/model/layers.11/mlp/up_proj/MatMul/output_0")
    140 |  # /model/layers.11/mlp/down_proj/MatMul
           %"/model/layers.11/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.11/mlp/Mul/output_0", %"model.layers.11.mlp.down_proj.MatMul.weight")
    141 |  # /model/layers.12/input_layernorm/SkipLayerNorm
           %"/model/layers.12/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.12/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.11/post_attention_layernorm/output_3", %"/model/layers.11/mlp/down_proj/MatMul/output_0", %"model.layers.12.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    142 |  # /model/layers.12/attn/qkv_proj/MatMul
           %"/model/layers.12/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.12/input_layernorm/output_0", %"model.layers.12.attn.qkv_proj.MatMul.weight")
    143 |  # /model/layers.12/attn/GroupQueryAttention
           %"/model/layers.12/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.12.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.12.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.12/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.12.key", %"past_key_values.12.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    144 |  # /model/layers.12/attn/o_proj/MatMul
           %"/model/layers.12/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.12/attn/GroupQueryAttention/output_0", %"model.layers.12.attn.o_proj.MatMul.weight")
    145 |  # /model/layers.12/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.12/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.12/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.12/input_layernorm/output_3", %"/model/layers.12/attn/o_proj/MatMul/output_0", %"model.layers.12.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    146 |  # /model/layers.12/mlp/gate_proj/MatMul
           %"/model/layers.12/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.12/post_attention_layernorm/output_0", %"model.layers.12.mlp.gate_proj.MatMul.weight")
    147 |  # /model/layers.12/mlp/up_proj/MatMul
           %"/model/layers.12/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.12/post_attention_layernorm/output_0", %"model.layers.12.mlp.up_proj.MatMul.weight")
    148 |  # /model/layers.12/mlp/act_fn/Sigmoid
           %"/model/layers.12/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.12/mlp/gate_proj/MatMul/output_0")
    149 |  # /model/layers.12/mlp/act_fn/Mul
           %"/model/layers.12/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.12/mlp/gate_proj/MatMul/output_0", %"/model/layers.12/mlp/act_fn/Sigmoid/output_0")
    150 |  # /model/layers.12/mlp/Mul
           %"/model/layers.12/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.12/mlp/act_fn/Mul/output_0", %"/model/layers.12/mlp/up_proj/MatMul/output_0")
    151 |  # /model/layers.12/mlp/down_proj/MatMul
           %"/model/layers.12/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.12/mlp/Mul/output_0", %"model.layers.12.mlp.down_proj.MatMul.weight")
    152 |  # /model/layers.13/input_layernorm/SkipLayerNorm
           %"/model/layers.13/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.13/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.12/post_attention_layernorm/output_3", %"/model/layers.12/mlp/down_proj/MatMul/output_0", %"model.layers.13.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    153 |  # /model/layers.13/attn/qkv_proj/MatMul
           %"/model/layers.13/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.13/input_layernorm/output_0", %"model.layers.13.attn.qkv_proj.MatMul.weight")
    154 |  # /model/layers.13/attn/GroupQueryAttention
           %"/model/layers.13/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.13.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.13.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.13/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.13.key", %"past_key_values.13.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    155 |  # /model/layers.13/attn/o_proj/MatMul
           %"/model/layers.13/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.13/attn/GroupQueryAttention/output_0", %"model.layers.13.attn.o_proj.MatMul.weight")
    156 |  # /model/layers.13/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.13/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.13/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.13/input_layernorm/output_3", %"/model/layers.13/attn/o_proj/MatMul/output_0", %"model.layers.13.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    157 |  # /model/layers.13/mlp/gate_proj/MatMul
           %"/model/layers.13/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.13/post_attention_layernorm/output_0", %"model.layers.13.mlp.gate_proj.MatMul.weight")
    158 |  # /model/layers.13/mlp/up_proj/MatMul
           %"/model/layers.13/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.13/post_attention_layernorm/output_0", %"model.layers.13.mlp.up_proj.MatMul.weight")
    159 |  # /model/layers.13/mlp/act_fn/Sigmoid
           %"/model/layers.13/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.13/mlp/gate_proj/MatMul/output_0")
    160 |  # /model/layers.13/mlp/act_fn/Mul
           %"/model/layers.13/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.13/mlp/gate_proj/MatMul/output_0", %"/model/layers.13/mlp/act_fn/Sigmoid/output_0")
    161 |  # /model/layers.13/mlp/Mul
           %"/model/layers.13/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.13/mlp/act_fn/Mul/output_0", %"/model/layers.13/mlp/up_proj/MatMul/output_0")
    162 |  # /model/layers.13/mlp/down_proj/MatMul
           %"/model/layers.13/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.13/mlp/Mul/output_0", %"model.layers.13.mlp.down_proj.MatMul.weight")
    163 |  # /model/layers.14/input_layernorm/SkipLayerNorm
           %"/model/layers.14/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.14/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.13/post_attention_layernorm/output_3", %"/model/layers.13/mlp/down_proj/MatMul/output_0", %"model.layers.14.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    164 |  # /model/layers.14/attn/qkv_proj/MatMul
           %"/model/layers.14/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.14/input_layernorm/output_0", %"model.layers.14.attn.qkv_proj.MatMul.weight")
    165 |  # /model/layers.14/attn/GroupQueryAttention
           %"/model/layers.14/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.14.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.14.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.14/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.14.key", %"past_key_values.14.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    166 |  # /model/layers.14/attn/o_proj/MatMul
           %"/model/layers.14/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.14/attn/GroupQueryAttention/output_0", %"model.layers.14.attn.o_proj.MatMul.weight")
    167 |  # /model/layers.14/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.14/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.14/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.14/input_layernorm/output_3", %"/model/layers.14/attn/o_proj/MatMul/output_0", %"model.layers.14.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    168 |  # /model/layers.14/mlp/gate_proj/MatMul
           %"/model/layers.14/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.14/post_attention_layernorm/output_0", %"model.layers.14.mlp.gate_proj.MatMul.weight")
    169 |  # /model/layers.14/mlp/up_proj/MatMul
           %"/model/layers.14/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.14/post_attention_layernorm/output_0", %"model.layers.14.mlp.up_proj.MatMul.weight")
    170 |  # /model/layers.14/mlp/act_fn/Sigmoid
           %"/model/layers.14/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.14/mlp/gate_proj/MatMul/output_0")
    171 |  # /model/layers.14/mlp/act_fn/Mul
           %"/model/layers.14/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.14/mlp/gate_proj/MatMul/output_0", %"/model/layers.14/mlp/act_fn/Sigmoid/output_0")
    172 |  # /model/layers.14/mlp/Mul
           %"/model/layers.14/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.14/mlp/act_fn/Mul/output_0", %"/model/layers.14/mlp/up_proj/MatMul/output_0")
    173 |  # /model/layers.14/mlp/down_proj/MatMul
           %"/model/layers.14/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.14/mlp/Mul/output_0", %"model.layers.14.mlp.down_proj.MatMul.weight")
    174 |  # /model/layers.15/input_layernorm/SkipLayerNorm
           %"/model/layers.15/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.15/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.14/post_attention_layernorm/output_3", %"/model/layers.14/mlp/down_proj/MatMul/output_0", %"model.layers.15.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    175 |  # /model/layers.15/attn/qkv_proj/MatMul
           %"/model/layers.15/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.15/input_layernorm/output_0", %"model.layers.15.attn.qkv_proj.MatMul.weight")
    176 |  # /model/layers.15/attn/GroupQueryAttention
           %"/model/layers.15/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.15.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.15.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.15/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.15.key", %"past_key_values.15.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    177 |  # /model/layers.15/attn/o_proj/MatMul
           %"/model/layers.15/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.15/attn/GroupQueryAttention/output_0", %"model.layers.15.attn.o_proj.MatMul.weight")
    178 |  # /model/layers.15/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.15/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.15/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.15/input_layernorm/output_3", %"/model/layers.15/attn/o_proj/MatMul/output_0", %"model.layers.15.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    179 |  # /model/layers.15/mlp/gate_proj/MatMul
           %"/model/layers.15/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.15/post_attention_layernorm/output_0", %"model.layers.15.mlp.gate_proj.MatMul.weight")
    180 |  # /model/layers.15/mlp/up_proj/MatMul
           %"/model/layers.15/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.15/post_attention_layernorm/output_0", %"model.layers.15.mlp.up_proj.MatMul.weight")
    181 |  # /model/layers.15/mlp/act_fn/Sigmoid
           %"/model/layers.15/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.15/mlp/gate_proj/MatMul/output_0")
    182 |  # /model/layers.15/mlp/act_fn/Mul
           %"/model/layers.15/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.15/mlp/gate_proj/MatMul/output_0", %"/model/layers.15/mlp/act_fn/Sigmoid/output_0")
    183 |  # /model/layers.15/mlp/Mul
           %"/model/layers.15/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.15/mlp/act_fn/Mul/output_0", %"/model/layers.15/mlp/up_proj/MatMul/output_0")
    184 |  # /model/layers.15/mlp/down_proj/MatMul
           %"/model/layers.15/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.15/mlp/Mul/output_0", %"model.layers.15.mlp.down_proj.MatMul.weight")
    185 |  # /model/layers.16/input_layernorm/SkipLayerNorm
           %"/model/layers.16/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.16/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.15/post_attention_layernorm/output_3", %"/model/layers.15/mlp/down_proj/MatMul/output_0", %"model.layers.16.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    186 |  # /model/layers.16/attn/qkv_proj/MatMul
           %"/model/layers.16/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.16/input_layernorm/output_0", %"model.layers.16.attn.qkv_proj.MatMul.weight")
    187 |  # /model/layers.16/attn/GroupQueryAttention
           %"/model/layers.16/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.16.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.16.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.16/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.16.key", %"past_key_values.16.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    188 |  # /model/layers.16/attn/o_proj/MatMul
           %"/model/layers.16/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.16/attn/GroupQueryAttention/output_0", %"model.layers.16.attn.o_proj.MatMul.weight")
    189 |  # /model/layers.16/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.16/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.16/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.16/input_layernorm/output_3", %"/model/layers.16/attn/o_proj/MatMul/output_0", %"model.layers.16.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    190 |  # /model/layers.16/mlp/gate_proj/MatMul
           %"/model/layers.16/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.16/post_attention_layernorm/output_0", %"model.layers.16.mlp.gate_proj.MatMul.weight")
    191 |  # /model/layers.16/mlp/up_proj/MatMul
           %"/model/layers.16/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.16/post_attention_layernorm/output_0", %"model.layers.16.mlp.up_proj.MatMul.weight")
    192 |  # /model/layers.16/mlp/act_fn/Sigmoid
           %"/model/layers.16/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.16/mlp/gate_proj/MatMul/output_0")
    193 |  # /model/layers.16/mlp/act_fn/Mul
           %"/model/layers.16/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.16/mlp/gate_proj/MatMul/output_0", %"/model/layers.16/mlp/act_fn/Sigmoid/output_0")
    194 |  # /model/layers.16/mlp/Mul
           %"/model/layers.16/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.16/mlp/act_fn/Mul/output_0", %"/model/layers.16/mlp/up_proj/MatMul/output_0")
    195 |  # /model/layers.16/mlp/down_proj/MatMul
           %"/model/layers.16/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.16/mlp/Mul/output_0", %"model.layers.16.mlp.down_proj.MatMul.weight")
    196 |  # /model/layers.17/input_layernorm/SkipLayerNorm
           %"/model/layers.17/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.17/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.16/post_attention_layernorm/output_3", %"/model/layers.16/mlp/down_proj/MatMul/output_0", %"model.layers.17.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    197 |  # /model/layers.17/attn/qkv_proj/MatMul
           %"/model/layers.17/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.17/input_layernorm/output_0", %"model.layers.17.attn.qkv_proj.MatMul.weight")
    198 |  # /model/layers.17/attn/GroupQueryAttention
           %"/model/layers.17/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.17.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.17.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.17/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.17.key", %"past_key_values.17.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    199 |  # /model/layers.17/attn/o_proj/MatMul
           %"/model/layers.17/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.17/attn/GroupQueryAttention/output_0", %"model.layers.17.attn.o_proj.MatMul.weight")
    200 |  # /model/layers.17/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.17/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.17/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.17/input_layernorm/output_3", %"/model/layers.17/attn/o_proj/MatMul/output_0", %"model.layers.17.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    201 |  # /model/layers.17/mlp/gate_proj/MatMul
           %"/model/layers.17/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.17/post_attention_layernorm/output_0", %"model.layers.17.mlp.gate_proj.MatMul.weight")
    202 |  # /model/layers.17/mlp/up_proj/MatMul
           %"/model/layers.17/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.17/post_attention_layernorm/output_0", %"model.layers.17.mlp.up_proj.MatMul.weight")
    203 |  # /model/layers.17/mlp/act_fn/Sigmoid
           %"/model/layers.17/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.17/mlp/gate_proj/MatMul/output_0")
    204 |  # /model/layers.17/mlp/act_fn/Mul
           %"/model/layers.17/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.17/mlp/gate_proj/MatMul/output_0", %"/model/layers.17/mlp/act_fn/Sigmoid/output_0")
    205 |  # /model/layers.17/mlp/Mul
           %"/model/layers.17/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.17/mlp/act_fn/Mul/output_0", %"/model/layers.17/mlp/up_proj/MatMul/output_0")
    206 |  # /model/layers.17/mlp/down_proj/MatMul
           %"/model/layers.17/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.17/mlp/Mul/output_0", %"model.layers.17.mlp.down_proj.MatMul.weight")
    207 |  # /model/layers.18/input_layernorm/SkipLayerNorm
           %"/model/layers.18/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.18/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.17/post_attention_layernorm/output_3", %"/model/layers.17/mlp/down_proj/MatMul/output_0", %"model.layers.18.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    208 |  # /model/layers.18/attn/qkv_proj/MatMul
           %"/model/layers.18/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.18/input_layernorm/output_0", %"model.layers.18.attn.qkv_proj.MatMul.weight")
    209 |  # /model/layers.18/attn/GroupQueryAttention
           %"/model/layers.18/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.18.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.18.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.18/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.18.key", %"past_key_values.18.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    210 |  # /model/layers.18/attn/o_proj/MatMul
           %"/model/layers.18/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.18/attn/GroupQueryAttention/output_0", %"model.layers.18.attn.o_proj.MatMul.weight")
    211 |  # /model/layers.18/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.18/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.18/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.18/input_layernorm/output_3", %"/model/layers.18/attn/o_proj/MatMul/output_0", %"model.layers.18.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    212 |  # /model/layers.18/mlp/gate_proj/MatMul
           %"/model/layers.18/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.18/post_attention_layernorm/output_0", %"model.layers.18.mlp.gate_proj.MatMul.weight")
    213 |  # /model/layers.18/mlp/up_proj/MatMul
           %"/model/layers.18/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.18/post_attention_layernorm/output_0", %"model.layers.18.mlp.up_proj.MatMul.weight")
    214 |  # /model/layers.18/mlp/act_fn/Sigmoid
           %"/model/layers.18/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.18/mlp/gate_proj/MatMul/output_0")
    215 |  # /model/layers.18/mlp/act_fn/Mul
           %"/model/layers.18/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.18/mlp/gate_proj/MatMul/output_0", %"/model/layers.18/mlp/act_fn/Sigmoid/output_0")
    216 |  # /model/layers.18/mlp/Mul
           %"/model/layers.18/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.18/mlp/act_fn/Mul/output_0", %"/model/layers.18/mlp/up_proj/MatMul/output_0")
    217 |  # /model/layers.18/mlp/down_proj/MatMul
           %"/model/layers.18/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.18/mlp/Mul/output_0", %"model.layers.18.mlp.down_proj.MatMul.weight")
    218 |  # /model/layers.19/input_layernorm/SkipLayerNorm
           %"/model/layers.19/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.19/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.18/post_attention_layernorm/output_3", %"/model/layers.18/mlp/down_proj/MatMul/output_0", %"model.layers.19.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    219 |  # /model/layers.19/attn/qkv_proj/MatMul
           %"/model/layers.19/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.19/input_layernorm/output_0", %"model.layers.19.attn.qkv_proj.MatMul.weight")
    220 |  # /model/layers.19/attn/GroupQueryAttention
           %"/model/layers.19/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.19.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.19.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.19/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.19.key", %"past_key_values.19.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    221 |  # /model/layers.19/attn/o_proj/MatMul
           %"/model/layers.19/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.19/attn/GroupQueryAttention/output_0", %"model.layers.19.attn.o_proj.MatMul.weight")
    222 |  # /model/layers.19/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.19/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.19/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.19/input_layernorm/output_3", %"/model/layers.19/attn/o_proj/MatMul/output_0", %"model.layers.19.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    223 |  # /model/layers.19/mlp/gate_proj/MatMul
           %"/model/layers.19/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.19/post_attention_layernorm/output_0", %"model.layers.19.mlp.gate_proj.MatMul.weight")
    224 |  # /model/layers.19/mlp/up_proj/MatMul
           %"/model/layers.19/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.19/post_attention_layernorm/output_0", %"model.layers.19.mlp.up_proj.MatMul.weight")
    225 |  # /model/layers.19/mlp/act_fn/Sigmoid
           %"/model/layers.19/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.19/mlp/gate_proj/MatMul/output_0")
    226 |  # /model/layers.19/mlp/act_fn/Mul
           %"/model/layers.19/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.19/mlp/gate_proj/MatMul/output_0", %"/model/layers.19/mlp/act_fn/Sigmoid/output_0")
    227 |  # /model/layers.19/mlp/Mul
           %"/model/layers.19/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.19/mlp/act_fn/Mul/output_0", %"/model/layers.19/mlp/up_proj/MatMul/output_0")
    228 |  # /model/layers.19/mlp/down_proj/MatMul
           %"/model/layers.19/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.19/mlp/Mul/output_0", %"model.layers.19.mlp.down_proj.MatMul.weight")
    229 |  # /model/layers.20/input_layernorm/SkipLayerNorm
           %"/model/layers.20/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.20/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.19/post_attention_layernorm/output_3", %"/model/layers.19/mlp/down_proj/MatMul/output_0", %"model.layers.20.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    230 |  # /model/layers.20/attn/qkv_proj/MatMul
           %"/model/layers.20/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.20/input_layernorm/output_0", %"model.layers.20.attn.qkv_proj.MatMul.weight")
    231 |  # /model/layers.20/attn/GroupQueryAttention
           %"/model/layers.20/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.20.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.20.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.20/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.20.key", %"past_key_values.20.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    232 |  # /model/layers.20/attn/o_proj/MatMul
           %"/model/layers.20/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.20/attn/GroupQueryAttention/output_0", %"model.layers.20.attn.o_proj.MatMul.weight")
    233 |  # /model/layers.20/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.20/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.20/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.20/input_layernorm/output_3", %"/model/layers.20/attn/o_proj/MatMul/output_0", %"model.layers.20.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    234 |  # /model/layers.20/mlp/gate_proj/MatMul
           %"/model/layers.20/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.20/post_attention_layernorm/output_0", %"model.layers.20.mlp.gate_proj.MatMul.weight")
    235 |  # /model/layers.20/mlp/up_proj/MatMul
           %"/model/layers.20/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.20/post_attention_layernorm/output_0", %"model.layers.20.mlp.up_proj.MatMul.weight")
    236 |  # /model/layers.20/mlp/act_fn/Sigmoid
           %"/model/layers.20/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.20/mlp/gate_proj/MatMul/output_0")
    237 |  # /model/layers.20/mlp/act_fn/Mul
           %"/model/layers.20/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.20/mlp/gate_proj/MatMul/output_0", %"/model/layers.20/mlp/act_fn/Sigmoid/output_0")
    238 |  # /model/layers.20/mlp/Mul
           %"/model/layers.20/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.20/mlp/act_fn/Mul/output_0", %"/model/layers.20/mlp/up_proj/MatMul/output_0")
    239 |  # /model/layers.20/mlp/down_proj/MatMul
           %"/model/layers.20/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.20/mlp/Mul/output_0", %"model.layers.20.mlp.down_proj.MatMul.weight")
    240 |  # /model/layers.21/input_layernorm/SkipLayerNorm
           %"/model/layers.21/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.21/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.20/post_attention_layernorm/output_3", %"/model/layers.20/mlp/down_proj/MatMul/output_0", %"model.layers.21.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    241 |  # /model/layers.21/attn/qkv_proj/MatMul
           %"/model/layers.21/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.21/input_layernorm/output_0", %"model.layers.21.attn.qkv_proj.MatMul.weight")
    242 |  # /model/layers.21/attn/GroupQueryAttention
           %"/model/layers.21/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.21.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.21.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.21/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.21.key", %"past_key_values.21.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    243 |  # /model/layers.21/attn/o_proj/MatMul
           %"/model/layers.21/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.21/attn/GroupQueryAttention/output_0", %"model.layers.21.attn.o_proj.MatMul.weight")
    244 |  # /model/layers.21/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.21/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.21/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.21/input_layernorm/output_3", %"/model/layers.21/attn/o_proj/MatMul/output_0", %"model.layers.21.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    245 |  # /model/layers.21/mlp/gate_proj/MatMul
           %"/model/layers.21/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.21/post_attention_layernorm/output_0", %"model.layers.21.mlp.gate_proj.MatMul.weight")
    246 |  # /model/layers.21/mlp/up_proj/MatMul
           %"/model/layers.21/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.21/post_attention_layernorm/output_0", %"model.layers.21.mlp.up_proj.MatMul.weight")
    247 |  # /model/layers.21/mlp/act_fn/Sigmoid
           %"/model/layers.21/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.21/mlp/gate_proj/MatMul/output_0")
    248 |  # /model/layers.21/mlp/act_fn/Mul
           %"/model/layers.21/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.21/mlp/gate_proj/MatMul/output_0", %"/model/layers.21/mlp/act_fn/Sigmoid/output_0")
    249 |  # /model/layers.21/mlp/Mul
           %"/model/layers.21/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.21/mlp/act_fn/Mul/output_0", %"/model/layers.21/mlp/up_proj/MatMul/output_0")
    250 |  # /model/layers.21/mlp/down_proj/MatMul
           %"/model/layers.21/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.21/mlp/Mul/output_0", %"model.layers.21.mlp.down_proj.MatMul.weight")
    251 |  # /model/layers.22/input_layernorm/SkipLayerNorm
           %"/model/layers.22/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.22/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.21/post_attention_layernorm/output_3", %"/model/layers.21/mlp/down_proj/MatMul/output_0", %"model.layers.22.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    252 |  # /model/layers.22/attn/qkv_proj/MatMul
           %"/model/layers.22/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.22/input_layernorm/output_0", %"model.layers.22.attn.qkv_proj.MatMul.weight")
    253 |  # /model/layers.22/attn/GroupQueryAttention
           %"/model/layers.22/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.22.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.22.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.22/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.22.key", %"past_key_values.22.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    254 |  # /model/layers.22/attn/o_proj/MatMul
           %"/model/layers.22/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.22/attn/GroupQueryAttention/output_0", %"model.layers.22.attn.o_proj.MatMul.weight")
    255 |  # /model/layers.22/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.22/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.22/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.22/input_layernorm/output_3", %"/model/layers.22/attn/o_proj/MatMul/output_0", %"model.layers.22.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    256 |  # /model/layers.22/mlp/gate_proj/MatMul
           %"/model/layers.22/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.22/post_attention_layernorm/output_0", %"model.layers.22.mlp.gate_proj.MatMul.weight")
    257 |  # /model/layers.22/mlp/up_proj/MatMul
           %"/model/layers.22/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.22/post_attention_layernorm/output_0", %"model.layers.22.mlp.up_proj.MatMul.weight")
    258 |  # /model/layers.22/mlp/act_fn/Sigmoid
           %"/model/layers.22/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.22/mlp/gate_proj/MatMul/output_0")
    259 |  # /model/layers.22/mlp/act_fn/Mul
           %"/model/layers.22/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.22/mlp/gate_proj/MatMul/output_0", %"/model/layers.22/mlp/act_fn/Sigmoid/output_0")
    260 |  # /model/layers.22/mlp/Mul
           %"/model/layers.22/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.22/mlp/act_fn/Mul/output_0", %"/model/layers.22/mlp/up_proj/MatMul/output_0")
    261 |  # /model/layers.22/mlp/down_proj/MatMul
           %"/model/layers.22/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.22/mlp/Mul/output_0", %"model.layers.22.mlp.down_proj.MatMul.weight")
    262 |  # /model/layers.23/input_layernorm/SkipLayerNorm
           %"/model/layers.23/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.23/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.22/post_attention_layernorm/output_3", %"/model/layers.22/mlp/down_proj/MatMul/output_0", %"model.layers.23.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    263 |  # /model/layers.23/attn/qkv_proj/MatMul
           %"/model/layers.23/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.23/input_layernorm/output_0", %"model.layers.23.attn.qkv_proj.MatMul.weight")
    264 |  # /model/layers.23/attn/GroupQueryAttention
           %"/model/layers.23/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.23.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.23.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.23/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.23.key", %"past_key_values.23.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    265 |  # /model/layers.23/attn/o_proj/MatMul
           %"/model/layers.23/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.23/attn/GroupQueryAttention/output_0", %"model.layers.23.attn.o_proj.MatMul.weight")
    266 |  # /model/layers.23/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.23/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.23/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.23/input_layernorm/output_3", %"/model/layers.23/attn/o_proj/MatMul/output_0", %"model.layers.23.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    267 |  # /model/layers.23/mlp/gate_proj/MatMul
           %"/model/layers.23/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.23/post_attention_layernorm/output_0", %"model.layers.23.mlp.gate_proj.MatMul.weight")
    268 |  # /model/layers.23/mlp/up_proj/MatMul
           %"/model/layers.23/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.23/post_attention_layernorm/output_0", %"model.layers.23.mlp.up_proj.MatMul.weight")
    269 |  # /model/layers.23/mlp/act_fn/Sigmoid
           %"/model/layers.23/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.23/mlp/gate_proj/MatMul/output_0")
    270 |  # /model/layers.23/mlp/act_fn/Mul
           %"/model/layers.23/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.23/mlp/gate_proj/MatMul/output_0", %"/model/layers.23/mlp/act_fn/Sigmoid/output_0")
    271 |  # /model/layers.23/mlp/Mul
           %"/model/layers.23/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.23/mlp/act_fn/Mul/output_0", %"/model/layers.23/mlp/up_proj/MatMul/output_0")
    272 |  # /model/layers.23/mlp/down_proj/MatMul
           %"/model/layers.23/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.23/mlp/Mul/output_0", %"model.layers.23.mlp.down_proj.MatMul.weight")
    273 |  # /model/layers.24/input_layernorm/SkipLayerNorm
           %"/model/layers.24/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.24/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.23/post_attention_layernorm/output_3", %"/model/layers.23/mlp/down_proj/MatMul/output_0", %"model.layers.24.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    274 |  # /model/layers.24/attn/qkv_proj/MatMul
           %"/model/layers.24/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.24/input_layernorm/output_0", %"model.layers.24.attn.qkv_proj.MatMul.weight")
    275 |  # /model/layers.24/attn/GroupQueryAttention
           %"/model/layers.24/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.24.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.24.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.24/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.24.key", %"past_key_values.24.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    276 |  # /model/layers.24/attn/o_proj/MatMul
           %"/model/layers.24/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.24/attn/GroupQueryAttention/output_0", %"model.layers.24.attn.o_proj.MatMul.weight")
    277 |  # /model/layers.24/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.24/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.24/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.24/input_layernorm/output_3", %"/model/layers.24/attn/o_proj/MatMul/output_0", %"model.layers.24.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    278 |  # /model/layers.24/mlp/gate_proj/MatMul
           %"/model/layers.24/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.24/post_attention_layernorm/output_0", %"model.layers.24.mlp.gate_proj.MatMul.weight")
    279 |  # /model/layers.24/mlp/up_proj/MatMul
           %"/model/layers.24/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.24/post_attention_layernorm/output_0", %"model.layers.24.mlp.up_proj.MatMul.weight")
    280 |  # /model/layers.24/mlp/act_fn/Sigmoid
           %"/model/layers.24/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.24/mlp/gate_proj/MatMul/output_0")
    281 |  # /model/layers.24/mlp/act_fn/Mul
           %"/model/layers.24/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.24/mlp/gate_proj/MatMul/output_0", %"/model/layers.24/mlp/act_fn/Sigmoid/output_0")
    282 |  # /model/layers.24/mlp/Mul
           %"/model/layers.24/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.24/mlp/act_fn/Mul/output_0", %"/model/layers.24/mlp/up_proj/MatMul/output_0")
    283 |  # /model/layers.24/mlp/down_proj/MatMul
           %"/model/layers.24/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.24/mlp/Mul/output_0", %"model.layers.24.mlp.down_proj.MatMul.weight")
    284 |  # /model/layers.25/input_layernorm/SkipLayerNorm
           %"/model/layers.25/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.25/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.24/post_attention_layernorm/output_3", %"/model/layers.24/mlp/down_proj/MatMul/output_0", %"model.layers.25.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    285 |  # /model/layers.25/attn/qkv_proj/MatMul
           %"/model/layers.25/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.25/input_layernorm/output_0", %"model.layers.25.attn.qkv_proj.MatMul.weight")
    286 |  # /model/layers.25/attn/GroupQueryAttention
           %"/model/layers.25/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.25.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.25.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.25/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.25.key", %"past_key_values.25.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    287 |  # /model/layers.25/attn/o_proj/MatMul
           %"/model/layers.25/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.25/attn/GroupQueryAttention/output_0", %"model.layers.25.attn.o_proj.MatMul.weight")
    288 |  # /model/layers.25/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.25/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.25/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.25/input_layernorm/output_3", %"/model/layers.25/attn/o_proj/MatMul/output_0", %"model.layers.25.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    289 |  # /model/layers.25/mlp/gate_proj/MatMul
           %"/model/layers.25/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.25/post_attention_layernorm/output_0", %"model.layers.25.mlp.gate_proj.MatMul.weight")
    290 |  # /model/layers.25/mlp/up_proj/MatMul
           %"/model/layers.25/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.25/post_attention_layernorm/output_0", %"model.layers.25.mlp.up_proj.MatMul.weight")
    291 |  # /model/layers.25/mlp/act_fn/Sigmoid
           %"/model/layers.25/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.25/mlp/gate_proj/MatMul/output_0")
    292 |  # /model/layers.25/mlp/act_fn/Mul
           %"/model/layers.25/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.25/mlp/gate_proj/MatMul/output_0", %"/model/layers.25/mlp/act_fn/Sigmoid/output_0")
    293 |  # /model/layers.25/mlp/Mul
           %"/model/layers.25/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.25/mlp/act_fn/Mul/output_0", %"/model/layers.25/mlp/up_proj/MatMul/output_0")
    294 |  # /model/layers.25/mlp/down_proj/MatMul
           %"/model/layers.25/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.25/mlp/Mul/output_0", %"model.layers.25.mlp.down_proj.MatMul.weight")
    295 |  # /model/layers.26/input_layernorm/SkipLayerNorm
           %"/model/layers.26/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.26/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.25/post_attention_layernorm/output_3", %"/model/layers.25/mlp/down_proj/MatMul/output_0", %"model.layers.26.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    296 |  # /model/layers.26/attn/qkv_proj/MatMul
           %"/model/layers.26/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.26/input_layernorm/output_0", %"model.layers.26.attn.qkv_proj.MatMul.weight")
    297 |  # /model/layers.26/attn/GroupQueryAttention
           %"/model/layers.26/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.26.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.26.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.26/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.26.key", %"past_key_values.26.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    298 |  # /model/layers.26/attn/o_proj/MatMul
           %"/model/layers.26/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.26/attn/GroupQueryAttention/output_0", %"model.layers.26.attn.o_proj.MatMul.weight")
    299 |  # /model/layers.26/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.26/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.26/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.26/input_layernorm/output_3", %"/model/layers.26/attn/o_proj/MatMul/output_0", %"model.layers.26.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    300 |  # /model/layers.26/mlp/gate_proj/MatMul
           %"/model/layers.26/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.26/post_attention_layernorm/output_0", %"model.layers.26.mlp.gate_proj.MatMul.weight")
    301 |  # /model/layers.26/mlp/up_proj/MatMul
           %"/model/layers.26/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.26/post_attention_layernorm/output_0", %"model.layers.26.mlp.up_proj.MatMul.weight")
    302 |  # /model/layers.26/mlp/act_fn/Sigmoid
           %"/model/layers.26/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.26/mlp/gate_proj/MatMul/output_0")
    303 |  # /model/layers.26/mlp/act_fn/Mul
           %"/model/layers.26/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.26/mlp/gate_proj/MatMul/output_0", %"/model/layers.26/mlp/act_fn/Sigmoid/output_0")
    304 |  # /model/layers.26/mlp/Mul
           %"/model/layers.26/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.26/mlp/act_fn/Mul/output_0", %"/model/layers.26/mlp/up_proj/MatMul/output_0")
    305 |  # /model/layers.26/mlp/down_proj/MatMul
           %"/model/layers.26/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.26/mlp/Mul/output_0", %"model.layers.26.mlp.down_proj.MatMul.weight")
    306 |  # /model/layers.27/input_layernorm/SkipLayerNorm
           %"/model/layers.27/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.27/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.26/post_attention_layernorm/output_3", %"/model/layers.26/mlp/down_proj/MatMul/output_0", %"model.layers.27.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    307 |  # /model/layers.27/attn/qkv_proj/MatMul
           %"/model/layers.27/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.27/input_layernorm/output_0", %"model.layers.27.attn.qkv_proj.MatMul.weight")
    308 |  # /model/layers.27/attn/GroupQueryAttention
           %"/model/layers.27/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.27.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.27.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.27/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.27.key", %"past_key_values.27.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    309 |  # /model/layers.27/attn/o_proj/MatMul
           %"/model/layers.27/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.27/attn/GroupQueryAttention/output_0", %"model.layers.27.attn.o_proj.MatMul.weight")
    310 |  # /model/layers.27/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.27/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.27/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.27/input_layernorm/output_3", %"/model/layers.27/attn/o_proj/MatMul/output_0", %"model.layers.27.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    311 |  # /model/layers.27/mlp/gate_proj/MatMul
           %"/model/layers.27/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.27/post_attention_layernorm/output_0", %"model.layers.27.mlp.gate_proj.MatMul.weight")
    312 |  # /model/layers.27/mlp/up_proj/MatMul
           %"/model/layers.27/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.27/post_attention_layernorm/output_0", %"model.layers.27.mlp.up_proj.MatMul.weight")
    313 |  # /model/layers.27/mlp/act_fn/Sigmoid
           %"/model/layers.27/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.27/mlp/gate_proj/MatMul/output_0")
    314 |  # /model/layers.27/mlp/act_fn/Mul
           %"/model/layers.27/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.27/mlp/gate_proj/MatMul/output_0", %"/model/layers.27/mlp/act_fn/Sigmoid/output_0")
    315 |  # /model/layers.27/mlp/Mul
           %"/model/layers.27/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.27/mlp/act_fn/Mul/output_0", %"/model/layers.27/mlp/up_proj/MatMul/output_0")
    316 |  # /model/layers.27/mlp/down_proj/MatMul
           %"/model/layers.27/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.27/mlp/Mul/output_0", %"model.layers.27.mlp.down_proj.MatMul.weight")
    317 |  # /model/layers.28/input_layernorm/SkipLayerNorm
           %"/model/layers.28/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.28/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.27/post_attention_layernorm/output_3", %"/model/layers.27/mlp/down_proj/MatMul/output_0", %"model.layers.28.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    318 |  # /model/layers.28/attn/qkv_proj/MatMul
           %"/model/layers.28/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.28/input_layernorm/output_0", %"model.layers.28.attn.qkv_proj.MatMul.weight")
    319 |  # /model/layers.28/attn/GroupQueryAttention
           %"/model/layers.28/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.28.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.28.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.28/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.28.key", %"past_key_values.28.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    320 |  # /model/layers.28/attn/o_proj/MatMul
           %"/model/layers.28/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.28/attn/GroupQueryAttention/output_0", %"model.layers.28.attn.o_proj.MatMul.weight")
    321 |  # /model/layers.28/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.28/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.28/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.28/input_layernorm/output_3", %"/model/layers.28/attn/o_proj/MatMul/output_0", %"model.layers.28.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    322 |  # /model/layers.28/mlp/gate_proj/MatMul
           %"/model/layers.28/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.28/post_attention_layernorm/output_0", %"model.layers.28.mlp.gate_proj.MatMul.weight")
    323 |  # /model/layers.28/mlp/up_proj/MatMul
           %"/model/layers.28/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.28/post_attention_layernorm/output_0", %"model.layers.28.mlp.up_proj.MatMul.weight")
    324 |  # /model/layers.28/mlp/act_fn/Sigmoid
           %"/model/layers.28/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.28/mlp/gate_proj/MatMul/output_0")
    325 |  # /model/layers.28/mlp/act_fn/Mul
           %"/model/layers.28/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.28/mlp/gate_proj/MatMul/output_0", %"/model/layers.28/mlp/act_fn/Sigmoid/output_0")
    326 |  # /model/layers.28/mlp/Mul
           %"/model/layers.28/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.28/mlp/act_fn/Mul/output_0", %"/model/layers.28/mlp/up_proj/MatMul/output_0")
    327 |  # /model/layers.28/mlp/down_proj/MatMul
           %"/model/layers.28/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.28/mlp/Mul/output_0", %"model.layers.28.mlp.down_proj.MatMul.weight")
    328 |  # /model/layers.29/input_layernorm/SkipLayerNorm
           %"/model/layers.29/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.29/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.28/post_attention_layernorm/output_3", %"/model/layers.28/mlp/down_proj/MatMul/output_0", %"model.layers.29.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    329 |  # /model/layers.29/attn/qkv_proj/MatMul
           %"/model/layers.29/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.29/input_layernorm/output_0", %"model.layers.29.attn.qkv_proj.MatMul.weight")
    330 |  # /model/layers.29/attn/GroupQueryAttention
           %"/model/layers.29/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.29.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.29.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.29/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.29.key", %"past_key_values.29.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    331 |  # /model/layers.29/attn/o_proj/MatMul
           %"/model/layers.29/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.29/attn/GroupQueryAttention/output_0", %"model.layers.29.attn.o_proj.MatMul.weight")
    332 |  # /model/layers.29/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.29/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.29/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.29/input_layernorm/output_3", %"/model/layers.29/attn/o_proj/MatMul/output_0", %"model.layers.29.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    333 |  # /model/layers.29/mlp/gate_proj/MatMul
           %"/model/layers.29/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.29/post_attention_layernorm/output_0", %"model.layers.29.mlp.gate_proj.MatMul.weight")
    334 |  # /model/layers.29/mlp/up_proj/MatMul
           %"/model/layers.29/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.29/post_attention_layernorm/output_0", %"model.layers.29.mlp.up_proj.MatMul.weight")
    335 |  # /model/layers.29/mlp/act_fn/Sigmoid
           %"/model/layers.29/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.29/mlp/gate_proj/MatMul/output_0")
    336 |  # /model/layers.29/mlp/act_fn/Mul
           %"/model/layers.29/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.29/mlp/gate_proj/MatMul/output_0", %"/model/layers.29/mlp/act_fn/Sigmoid/output_0")
    337 |  # /model/layers.29/mlp/Mul
           %"/model/layers.29/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.29/mlp/act_fn/Mul/output_0", %"/model/layers.29/mlp/up_proj/MatMul/output_0")
    338 |  # /model/layers.29/mlp/down_proj/MatMul
           %"/model/layers.29/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.29/mlp/Mul/output_0", %"model.layers.29.mlp.down_proj.MatMul.weight")
    339 |  # /model/layers.30/input_layernorm/SkipLayerNorm
           %"/model/layers.30/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.30/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.29/post_attention_layernorm/output_3", %"/model/layers.29/mlp/down_proj/MatMul/output_0", %"model.layers.30.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    340 |  # /model/layers.30/attn/qkv_proj/MatMul
           %"/model/layers.30/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.30/input_layernorm/output_0", %"model.layers.30.attn.qkv_proj.MatMul.weight")
    341 |  # /model/layers.30/attn/GroupQueryAttention
           %"/model/layers.30/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.30.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.30.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.30/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.30.key", %"past_key_values.30.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    342 |  # /model/layers.30/attn/o_proj/MatMul
           %"/model/layers.30/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.30/attn/GroupQueryAttention/output_0", %"model.layers.30.attn.o_proj.MatMul.weight")
    343 |  # /model/layers.30/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.30/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.30/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.30/input_layernorm/output_3", %"/model/layers.30/attn/o_proj/MatMul/output_0", %"model.layers.30.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    344 |  # /model/layers.30/mlp/gate_proj/MatMul
           %"/model/layers.30/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.30/post_attention_layernorm/output_0", %"model.layers.30.mlp.gate_proj.MatMul.weight")
    345 |  # /model/layers.30/mlp/up_proj/MatMul
           %"/model/layers.30/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.30/post_attention_layernorm/output_0", %"model.layers.30.mlp.up_proj.MatMul.weight")
    346 |  # /model/layers.30/mlp/act_fn/Sigmoid
           %"/model/layers.30/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.30/mlp/gate_proj/MatMul/output_0")
    347 |  # /model/layers.30/mlp/act_fn/Mul
           %"/model/layers.30/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.30/mlp/gate_proj/MatMul/output_0", %"/model/layers.30/mlp/act_fn/Sigmoid/output_0")
    348 |  # /model/layers.30/mlp/Mul
           %"/model/layers.30/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.30/mlp/act_fn/Mul/output_0", %"/model/layers.30/mlp/up_proj/MatMul/output_0")
    349 |  # /model/layers.30/mlp/down_proj/MatMul
           %"/model/layers.30/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.30/mlp/Mul/output_0", %"model.layers.30.mlp.down_proj.MatMul.weight")
    350 |  # /model/layers.31/input_layernorm/SkipLayerNorm
           %"/model/layers.31/input_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.31/input_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.30/post_attention_layernorm/output_3", %"/model/layers.30/mlp/down_proj/MatMul/output_0", %"model.layers.31.input_layernorm.weight") {epsilon=9.999999747378752e-06}
    351 |  # /model/layers.31/attn/qkv_proj/MatMul
           %"/model/layers.31/attn/qkv_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,6144]> ⬅️ ::MatMul(%"/model/layers.31/input_layernorm/output_0", %"model.layers.31.attn.qkv_proj.MatMul.weight")
    352 |  # /model/layers.31/attn/GroupQueryAttention
           %"/model/layers.31/attn/GroupQueryAttention/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %"present.31.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.31.value"<FLOAT16,[batch_size,8,total_sequence_length,128]> ⬅️ com.microsoft::GroupQueryAttention(%"/model/layers.31/attn/qkv_proj/MatMul/output_0", None, None, %"past_key_values.31.key", %"past_key_values.31.value", %"/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast/output_0", %"/model/attn_mask_reformat/attn_mask_subgraph/Gather/Cast/output_0", %"cos_cache", %"sin_cache") {num_heads=32, kv_num_heads=8, scale=0.0883883461356163, do_rotary=1, rotary_interleaved=0}
    353 |  # /model/layers.31/attn/o_proj/MatMul
           %"/model/layers.31/attn/o_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.31/attn/GroupQueryAttention/output_0", %"model.layers.31.attn.o_proj.MatMul.weight")
    354 |  # /model/layers.31/post_attention_layernorm/SkipLayerNorm
           %"/model/layers.31/post_attention_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]>, %""<?,?>, %""<?,?>, %"/model/layers.31/post_attention_layernorm/output_3"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.31/input_layernorm/output_3", %"/model/layers.31/attn/o_proj/MatMul/output_0", %"model.layers.31.post_attention_layernorm.weight") {epsilon=9.999999747378752e-06}
    355 |  # /model/layers.31/mlp/gate_proj/MatMul
           %"/model/layers.31/mlp/gate_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.31/post_attention_layernorm/output_0", %"model.layers.31.mlp.gate_proj.MatMul.weight")
    356 |  # /model/layers.31/mlp/up_proj/MatMul
           %"/model/layers.31/mlp/up_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::MatMul(%"/model/layers.31/post_attention_layernorm/output_0", %"model.layers.31.mlp.up_proj.MatMul.weight")
    357 |  # /model/layers.31/mlp/act_fn/Sigmoid
           %"/model/layers.31/mlp/act_fn/Sigmoid/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Sigmoid(%"/model/layers.31/mlp/gate_proj/MatMul/output_0")
    358 |  # /model/layers.31/mlp/act_fn/Mul
           %"/model/layers.31/mlp/act_fn/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.31/mlp/gate_proj/MatMul/output_0", %"/model/layers.31/mlp/act_fn/Sigmoid/output_0")
    359 |  # /model/layers.31/mlp/Mul
           %"/model/layers.31/mlp/Mul/output_0"<FLOAT16,[batch_size,sequence_length,14336]> ⬅️ ::Mul(%"/model/layers.31/mlp/act_fn/Mul/output_0", %"/model/layers.31/mlp/up_proj/MatMul/output_0")
    360 |  # /model/layers.31/mlp/down_proj/MatMul
           %"/model/layers.31/mlp/down_proj/MatMul/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ ::MatMul(%"/model/layers.31/mlp/Mul/output_0", %"model.layers.31.mlp.down_proj.MatMul.weight")
    361 |  # /model/layers.32/final_norm_layernorm/SkipLayerNorm
           %"/model/layers.32/final_norm_layernorm/output_0"<FLOAT16,[batch_size,sequence_length,4096]> ⬅️ com.microsoft::SkipSimplifiedLayerNormalization(%"/model/layers.31/post_attention_layernorm/output_3", %"/model/layers.31/mlp/down_proj/MatMul/output_0", %"model.layers.32.final_norm_layernorm.weight") {epsilon=9.999999747378752e-06}
    362 |  # /lm_head/MatMul
           %"logits"<FLOAT16,[batch_size,sequence_length,128256]> ⬅️ ::MatMul(%"/model/layers.32/final_norm_layernorm/output_0", %"lm_head.MatMul.weight")
    return %"logits"<FLOAT16,[batch_size,sequence_length,128256]>, %"present.0.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.0.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.1.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.1.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.2.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.2.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.3.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.3.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.4.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.4.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.5.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.5.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.6.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.6.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.7.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.7.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.8.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.8.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.9.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.9.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.10.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.10.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.11.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.11.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.12.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.12.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.13.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.13.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.14.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.14.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.15.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.15.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.16.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.16.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.17.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.17.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.18.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.18.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.19.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.19.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.20.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.20.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.21.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.21.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.22.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.22.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.23.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.23.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.24.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.24.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.25.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.25.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.26.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.26.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.27.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.27.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.28.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.28.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.29.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.29.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.30.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.30.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.31.key"<FLOAT16,[batch_size,8,total_sequence_length,128]>, %"present.31.value"<FLOAT16,[batch_size,8,total_sequence_length,128]>
}