graph(
    name=main_graph,
    inputs=(
        %"l_input_ids_"<INT64,[s0,s1]>,
        %"l_attention_mask_"<INT64,[s0,s1]>,
        %"l_position_ids_"<INT64,[s0,s1]>,
        %"key_states"<FLOAT,[s0,32,0,128]>,
        %"value_states"<FLOAT,[s0,32,0,128]>,
        %"key_states_1"<FLOAT,[s0,32,0,128]>,
        %"value_states_1"<FLOAT,[s0,32,0,128]>,
        %"key_states_2"<FLOAT,[s0,32,0,128]>,
        %"value_states_2"<FLOAT,[s0,32,0,128]>,
        %"key_states_3"<FLOAT,[s0,32,0,128]>,
        %"value_states_3"<FLOAT,[s0,32,0,128]>,
        %"key_states_4"<FLOAT,[s0,32,0,128]>,
        %"value_states_4"<FLOAT,[s0,32,0,128]>,
        %"key_states_5"<FLOAT,[s0,32,0,128]>,
        %"value_states_5"<FLOAT,[s0,32,0,128]>,
        %"key_states_6"<FLOAT,[s0,32,0,128]>,
        %"value_states_6"<FLOAT,[s0,32,0,128]>,
        %"key_states_7"<FLOAT,[s0,32,0,128]>,
        %"value_states_7"<FLOAT,[s0,32,0,128]>,
        %"key_states_8"<FLOAT,[s0,32,0,128]>,
        %"value_states_8"<FLOAT,[s0,32,0,128]>,
        %"key_states_9"<FLOAT,[s0,32,0,128]>,
        %"value_states_9"<FLOAT,[s0,32,0,128]>,
        %"key_states_10"<FLOAT,[s0,32,0,128]>,
        %"value_states_10"<FLOAT,[s0,32,0,128]>,
        %"key_states_11"<FLOAT,[s0,32,0,128]>,
        %"value_states_11"<FLOAT,[s0,32,0,128]>,
        %"key_states_12"<FLOAT,[s0,32,0,128]>,
        %"value_states_12"<FLOAT,[s0,32,0,128]>,
        %"key_states_13"<FLOAT,[s0,32,0,128]>,
        %"value_states_13"<FLOAT,[s0,32,0,128]>,
        %"key_states_14"<FLOAT,[s0,32,0,128]>,
        %"value_states_14"<FLOAT,[s0,32,0,128]>,
        %"key_states_15"<FLOAT,[s0,32,0,128]>,
        %"value_states_15"<FLOAT,[s0,32,0,128]>,
        %"key_states_16"<FLOAT,[s0,32,0,128]>,
        %"value_states_16"<FLOAT,[s0,32,0,128]>,
        %"key_states_17"<FLOAT,[s0,32,0,128]>,
        %"value_states_17"<FLOAT,[s0,32,0,128]>,
        %"key_states_18"<FLOAT,[s0,32,0,128]>,
        %"value_states_18"<FLOAT,[s0,32,0,128]>,
        %"key_states_19"<FLOAT,[s0,32,0,128]>,
        %"value_states_19"<FLOAT,[s0,32,0,128]>,
        %"key_states_20"<FLOAT,[s0,32,0,128]>,
        %"value_states_20"<FLOAT,[s0,32,0,128]>,
        %"key_states_21"<FLOAT,[s0,32,0,128]>,
        %"value_states_21"<FLOAT,[s0,32,0,128]>,
        %"key_states_22"<FLOAT,[s0,32,0,128]>,
        %"value_states_22"<FLOAT,[s0,32,0,128]>,
        %"key_states_23"<FLOAT,[s0,32,0,128]>,
        %"value_states_23"<FLOAT,[s0,32,0,128]>,
        %"key_states_24"<FLOAT,[s0,32,0,128]>,
        %"value_states_24"<FLOAT,[s0,32,0,128]>,
        %"key_states_25"<FLOAT,[s0,32,0,128]>,
        %"value_states_25"<FLOAT,[s0,32,0,128]>,
        %"key_states_26"<FLOAT,[s0,32,0,128]>,
        %"value_states_26"<FLOAT,[s0,32,0,128]>,
        %"key_states_27"<FLOAT,[s0,32,0,128]>,
        %"value_states_27"<FLOAT,[s0,32,0,128]>,
        %"key_states_28"<FLOAT,[s0,32,0,128]>,
        %"value_states_28"<FLOAT,[s0,32,0,128]>,
        %"key_states_29"<FLOAT,[s0,32,0,128]>,
        %"value_states_29"<FLOAT,[s0,32,0,128]>,
        %"key_states_30"<FLOAT,[s0,32,0,128]>,
        %"value_states_30"<FLOAT,[s0,32,0,128]>,
        %"key_states_31"<FLOAT,[s0,32,0,128]>,
        %"value_states_31"<FLOAT,[s0,32,0,128]>
    ),
    outputs=(
        %"lm_head_1"<FLOAT,[s0,s1,32000]>,
        %"model_1_2"<FLOAT,[s0,32,s1,128]>,
        %"model_1_3"<FLOAT,[s0,32,s1,128]>,
        %"model_1_4"<FLOAT,[s0,32,s1,128]>,
        %"model_1_5"<FLOAT,[s0,32,s1,128]>,
        %"model_1_6"<FLOAT,[s0,32,s1,128]>,
        %"model_1_7"<FLOAT,[s0,32,s1,128]>,
        %"model_1_8"<FLOAT,[s0,32,s1,128]>,
        %"model_1_9"<FLOAT,[s0,32,s1,128]>,
        %"model_1_10"<FLOAT,[s0,32,s1,128]>,
        %"model_1_11"<FLOAT,[s0,32,s1,128]>,
        %"model_1_12"<FLOAT,[s0,32,s1,128]>,
        %"model_1_13"<FLOAT,[s0,32,s1,128]>,
        %"model_1_14"<FLOAT,[s0,32,s1,128]>,
        %"model_1_15"<FLOAT,[s0,32,s1,128]>,
        %"model_1_16"<FLOAT,[s0,32,s1,128]>,
        %"model_1_17"<FLOAT,[s0,32,s1,128]>,
        %"model_1_18"<FLOAT,[s0,32,s1,128]>,
        %"model_1_19"<FLOAT,[s0,32,s1,128]>,
        %"model_1_20"<FLOAT,[s0,32,s1,128]>,
        %"model_1_21"<FLOAT,[s0,32,s1,128]>,
        %"model_1_22"<FLOAT,[s0,32,s1,128]>,
        %"model_1_23"<FLOAT,[s0,32,s1,128]>,
        %"model_1_24"<FLOAT,[s0,32,s1,128]>,
        %"model_1_25"<FLOAT,[s0,32,s1,128]>,
        %"model_1_26"<FLOAT,[s0,32,s1,128]>,
        %"model_1_27"<FLOAT,[s0,32,s1,128]>,
        %"model_1_28"<FLOAT,[s0,32,s1,128]>,
        %"model_1_29"<FLOAT,[s0,32,s1,128]>,
        %"model_1_30"<FLOAT,[s0,32,s1,128]>,
        %"model_1_31"<FLOAT,[s0,32,s1,128]>,
        %"model_1_32"<FLOAT,[s0,32,s1,128]>,
        %"model_1_33"<FLOAT,[s0,32,s1,128]>,
        %"model_1_34"<FLOAT,[s0,32,s1,128]>,
        %"model_1_35"<FLOAT,[s0,32,s1,128]>,
        %"model_1_36"<FLOAT,[s0,32,s1,128]>,
        %"model_1_37"<FLOAT,[s0,32,s1,128]>,
        %"model_1_38"<FLOAT,[s0,32,s1,128]>,
        %"model_1_39"<FLOAT,[s0,32,s1,128]>,
        %"model_1_40"<FLOAT,[s0,32,s1,128]>,
        %"model_1_41"<FLOAT,[s0,32,s1,128]>,
        %"model_1_42"<FLOAT,[s0,32,s1,128]>,
        %"model_1_43"<FLOAT,[s0,32,s1,128]>,
        %"model_1_44"<FLOAT,[s0,32,s1,128]>,
        %"model_1_45"<FLOAT,[s0,32,s1,128]>,
        %"model_1_46"<FLOAT,[s0,32,s1,128]>,
        %"model_1_47"<FLOAT,[s0,32,s1,128]>,
        %"model_1_48"<FLOAT,[s0,32,s1,128]>,
        %"model_1_49"<FLOAT,[s0,32,s1,128]>,
        %"model_1_50"<FLOAT,[s0,32,s1,128]>,
        %"model_1_51"<FLOAT,[s0,32,s1,128]>,
        %"model_1_52"<FLOAT,[s0,32,s1,128]>,
        %"model_1_53"<FLOAT,[s0,32,s1,128]>,
        %"model_1_54"<FLOAT,[s0,32,s1,128]>,
        %"model_1_55"<FLOAT,[s0,32,s1,128]>,
        %"model_1_56"<FLOAT,[s0,32,s1,128]>,
        %"model_1_57"<FLOAT,[s0,32,s1,128]>,
        %"model_1_58"<FLOAT,[s0,32,s1,128]>,
        %"model_1_59"<FLOAT,[s0,32,s1,128]>,
        %"model_1_60"<FLOAT,[s0,32,s1,128]>,
        %"model_1_61"<FLOAT,[s0,32,s1,128]>,
        %"model_1_62"<FLOAT,[s0,32,s1,128]>,
        %"model_1_63"<FLOAT,[s0,32,s1,128]>,
        %"model_1_64"<FLOAT,[s0,32,s1,128]>,
        %"model_1_65"<FLOAT,[s0,32,s1,128]>
    ),
    initializers=(
        %"model.embed_tokens.weight"<FLOAT,[32000,4096]>,
        %"model.layers.0.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.0.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.0.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.0.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.0.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.0.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.0.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.0.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.0.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.0.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.1.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.1.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.1.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.1.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.1.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.1.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.1.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.1.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.1.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.1.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.2.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.2.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.2.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.2.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.2.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.2.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.2.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.2.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.2.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.2.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.3.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.3.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.3.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.3.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.3.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.3.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.3.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.3.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.3.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.3.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.4.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.4.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.4.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.4.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.4.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.4.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.4.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.4.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.4.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.4.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.5.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.5.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.5.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.5.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.5.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.5.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.5.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.5.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.5.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.5.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.6.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.6.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.6.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.6.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.6.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.6.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.6.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.6.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.6.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.6.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.7.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.7.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.7.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.7.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.7.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.7.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.7.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.7.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.7.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.7.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.8.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.8.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.8.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.8.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.8.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.8.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.8.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.8.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.8.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.8.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.9.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.9.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.9.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.9.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.9.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.9.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.9.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.9.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.9.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.9.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.10.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.10.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.10.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.10.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.10.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.10.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.10.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.10.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.10.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.10.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.11.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.11.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.11.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.11.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.11.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.11.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.11.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.11.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.11.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.11.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.12.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.12.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.12.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.12.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.12.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.12.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.12.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.12.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.12.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.12.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.13.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.13.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.13.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.13.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.13.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.13.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.13.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.13.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.13.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.13.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.14.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.14.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.14.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.14.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.14.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.14.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.14.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.14.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.14.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.14.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.15.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.15.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.15.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.15.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.15.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.15.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.15.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.15.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.15.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.15.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.16.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.16.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.16.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.16.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.16.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.16.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.16.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.16.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.16.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.16.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.17.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.17.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.17.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.17.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.17.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.17.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.17.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.17.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.17.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.17.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.18.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.18.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.18.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.18.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.18.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.18.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.18.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.18.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.18.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.18.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.19.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.19.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.19.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.19.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.19.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.19.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.19.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.19.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.19.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.19.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.20.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.20.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.20.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.20.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.20.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.20.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.20.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.20.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.20.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.20.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.21.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.21.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.21.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.21.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.21.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.21.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.21.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.21.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.21.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.21.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.22.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.22.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.22.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.22.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.22.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.22.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.22.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.22.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.22.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.22.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.23.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.23.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.23.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.23.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.23.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.23.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.23.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.23.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.23.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.23.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.24.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.24.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.24.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.24.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.24.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.24.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.24.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.24.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.24.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.24.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.25.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.25.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.25.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.25.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.25.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.25.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.25.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.25.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.25.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.25.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.26.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.26.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.26.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.26.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.26.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.26.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.26.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.26.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.26.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.26.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.27.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.27.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.27.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.27.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.27.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.27.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.27.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.27.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.27.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.27.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.28.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.28.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.28.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.28.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.28.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.28.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.28.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.28.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.28.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.28.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.29.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.29.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.29.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.29.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.29.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.29.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.29.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.29.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.29.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.29.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.30.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.30.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.30.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.30.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.30.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.30.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.30.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.30.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.30.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.30.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.layers.31.input_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.31.self_attn.q_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.31.self_attn.k_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.31.self_attn.v_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.31.self_attn.rotary_emb.inv_freq"<FLOAT,[64]>,
        %"model.layers.31.self_attn.o_proj.weight"<FLOAT,[4096,4096]>,
        %"model.layers.31.post_attention_layernorm.weight"<FLOAT,[4096]>,
        %"model.layers.31.mlp.gate_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.31.mlp.up_proj.weight"<FLOAT,[11008,4096]>,
        %"model.layers.31.mlp.down_proj.weight"<FLOAT,[4096,11008]>,
        %"model.norm.weight"<FLOAT,[4096]>,
        %"lm_head.weight"<FLOAT,[32000,4096]>
    ),
) {
       0 |  # n0__3
            %"model_embed_tokens_1__1"<?,?> ⬅️ ::Gather(%"model.embed_tokens.weight", %"l_input_ids_")
       1 |  # n0__4
            %"shape__4"<?,?> ⬅️ ::Shape(%"l_input_ids_")
       2 |  # n1__4
            %"dim__4"<?,?> ⬅️ ::Constant() {value_int=1}
       3 |  # n2__4
            %"tmp__4"<?,?> ⬅️ ::Constant() {value_ints=[1]}
       4 |  # n3__4
            %"start__4"<?,?> ⬅️ ::Reshape(%"dim__4", %"tmp__4")
       5 |  # n4__4
            %"dim_0__4"<?,?> ⬅️ ::Constant() {value_int=1}
       6 |  # n5__4
            %"int64_1__4"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
       7 |  # n6__4
            %"tmp_1__4"<?,?> ⬅️ ::Add(%"dim_0__4", %"int64_1__4")
       8 |  # n7__4
            %"tmp_2__4"<?,?> ⬅️ ::Constant() {value_ints=[1]}
       9 |  # n8__4
            %"end__4"<?,?> ⬅️ ::Reshape(%"tmp_1__4", %"tmp_2__4")
      10 |  # n9__4
            %"model_1"<INT64,[1]> ⬅️ ::Slice(%"shape__4", %"start__4", %"end__4")
      11 |  # Constant_98__1
            %"_val_70__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      12 |  # n0__5
            %"alpha__5"<?,?> ⬅️ ::Constant() {value_float=1.0}
      13 |  # n1__5
            %"alpha_0__5"<?,?> ⬅️ ::CastLike(%"alpha__5", %"model_1")
      14 |  # n2__5
            %"other_1__5"<?,?> ⬅️ ::Mul(%"model_1", %"alpha_0__5")
      15 |  # n3__5
            %"add__1"<?,?> ⬅️ ::Add(%"_val_70__1", %"other_1__5")
      16 |  # Constant_100__1
            %"_val_72__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='')}
      17 |  # CastLike_101__1
            %"_val_73__1"<?,?> ⬅️ ::CastLike(%"_val_72__1", %"add__1")
      18 |  # Constant_102__1
            %"_val_74__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      19 |  # Range_103__1
            %"arange__1"<?,?> ⬅️ ::Range(%"_val_74__1", %"add__1", %"_val_73__1")
      20 |  # n0__6
            %"shape__6"<?,?> ⬅️ ::Shape(%"l_attention_mask_")
      21 |  # n1__6
            %"dim__6"<?,?> ⬅️ ::Constant() {value_int=1}
      22 |  # n2__6
            %"tmp__6"<?,?> ⬅️ ::Constant() {value_ints=[1]}
      23 |  # n3__6
            %"start__6"<?,?> ⬅️ ::Reshape(%"dim__6", %"tmp__6")
      24 |  # n4__6
            %"dim_0__6"<?,?> ⬅️ ::Constant() {value_int=1}
      25 |  # n5__6
            %"int64_1__6"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
      26 |  # n6__6
            %"tmp_1__6"<?,?> ⬅️ ::Add(%"dim_0__6", %"int64_1__6")
      27 |  # n7__6
            %"tmp_2__6"<?,?> ⬅️ ::Constant() {value_ints=[1]}
      28 |  # n8__6
            %"end__6"<?,?> ⬅️ ::Reshape(%"tmp_1__6", %"tmp_2__6")
      29 |  # n9__6
            %"sym_size_int_1__1"<?,?> ⬅️ ::Slice(%"shape__6", %"start__6", %"end__6")
      30 |  # Constant_105__1
            %"_val_77__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
      31 |  # Reshape_106__1
            %"_val_78__1"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_77__1") {allowzero=0}
      32 |  # Constant_107__1
            %"_val_79__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
      33 |  # Reshape_108__1
            %"_val_80__1"<?,?> ⬅️ ::Reshape(%"sym_size_int_1__1", %"_val_79__1") {allowzero=0}
      34 |  # Concat_109__1
            %"_val_81__1"<?,?> ⬅️ ::Concat(%"_val_78__1", %"_val_80__1") {axis=0}
      35 |  # Constant_110__1
            %"_val_82__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='')}
      36 |  # n0__7
            %"size_0__7"<?,?> ⬅️ ::Cast(%"_val_81__1") {to=7}
      37 |  # n1__7
            %"fill_value_1__7"<?,?> ⬅️ ::Cast(%"_val_82__1") {to=1}
      38 |  # n2__7
            %"full__1"<?,?> ⬅️ ::Expand(%"fill_value_1__7", %"size_0__7")
      39 |  # n0__8
            %"diagonal__8"<?,?> ⬅️ ::Constant() {value_int=1}
      40 |  # n1__8
            %"triu__1"<?,?> ⬅️ ::Trilu(%"full__1", %"diagonal__8") {upper=1}
      41 |  # Constant_113__1
            %"_val_85__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='')}
      42 |  # CastLike_114__1
            %"_val_86__1"<?,?> ⬅️ ::CastLike(%"_val_85__1", %"sym_size_int_1__1")
      43 |  # Constant_115__1
            %"_val_87__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='')}
      44 |  # CastLike_116__1
            %"_val_88__1"<?,?> ⬅️ ::CastLike(%"_val_87__1", %"sym_size_int_1__1")
      45 |  # Range_117__1
            %"arange_1__1"<?,?> ⬅️ ::Range(%"_val_86__1", %"sym_size_int_1__1", %"_val_88__1")
      46 |  # Constant_118__1
            %"_val_90__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='')}
      47 |  # n0__9
            %"size_0__9"<?,?> ⬅️ ::Cast(%"_val_90__1") {to=7}
      48 |  # n1__9
            %"view__1"<?,?> ⬅️ ::Reshape(%"arange__1", %"size_0__9")
      49 |  # n0__10
            %"gt__1"<?,?> ⬅️ ::Greater(%"arange_1__1", %"view__1")
      50 |  # Cast_121__1
            %"convert_element_type_default__1"<?,?> ⬅️ ::Cast(%"gt__1") {to=1}
      51 |  # n0__11
            %"mul__1"<?,?> ⬅️ ::Mul(%"triu__1", %"convert_element_type_default__1")
      52 |  # n0__12
            %"shape__12"<?,?> ⬅️ ::Shape(%"l_input_ids_")
      53 |  # n1__12
            %"dim__12"<?,?> ⬅️ ::Constant() {value_int=0}
      54 |  # n2__12
            %"tmp__12"<?,?> ⬅️ ::Constant() {value_ints=[1]}
      55 |  # n3__12
            %"start__12"<?,?> ⬅️ ::Reshape(%"dim__12", %"tmp__12")
      56 |  # n4__12
            %"dim_0__12"<?,?> ⬅️ ::Constant() {value_int=0}
      57 |  # n5__12
            %"int64_1__12"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
      58 |  # n6__12
            %"tmp_1__12"<?,?> ⬅️ ::Add(%"dim_0__12", %"int64_1__12")
      59 |  # n7__12
            %"tmp_2__12"<?,?> ⬅️ ::Constant() {value_ints=[1]}
      60 |  # n8__12
            %"end__12"<?,?> ⬅️ ::Reshape(%"tmp_1__12", %"tmp_2__12")
      61 |  # n9__12
            %"model_1_1"<INT64,[1]> ⬅️ ::Slice(%"shape__12", %"start__12", %"end__12")
      62 |  # n0__13
            %"dim__13"<?,?> ⬅️ ::Constant() {value_int=0}
      63 |  # n1__13
            %"dim_0__13"<?,?> ⬅️ ::Cast(%"dim__13") {to=7}
      64 |  # n2__13
            %"unsqueeze_2__1"<?,?> ⬅️ ::Unsqueeze(%"mul__1", %"dim_0__13")
      65 |  # n0__14
            %"dim__14"<?,?> ⬅️ ::Constant() {value_int=1}
      66 |  # n1__14
            %"dim_0__14"<?,?> ⬅️ ::Cast(%"dim__14") {to=7}
      67 |  # n2__14
            %"unsqueeze_3__1"<?,?> ⬅️ ::Unsqueeze(%"unsqueeze_2__1", %"dim_0__14")
      68 |  # Constant_126__1
            %"_val_98__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      69 |  # Cast_127__1
            %"_val_99__1"<?,?> ⬅️ ::Cast(%"_val_98__1") {to=7}
      70 |  # Constant_128__1
            %"_val_100__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      71 |  # Reshape_129__1
            %"_val_101__1"<?,?> ⬅️ ::Reshape(%"_val_99__1", %"_val_100__1") {allowzero=0}
      72 |  # Constant_130__1
            %"_val_102__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      73 |  # Cast_131__1
            %"_val_103__1"<?,?> ⬅️ ::Cast(%"_val_102__1") {to=7}
      74 |  # Constant_132__1
            %"_val_104__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      75 |  # Reshape_133__1
            %"_val_105__1"<?,?> ⬅️ ::Reshape(%"_val_103__1", %"_val_104__1") {allowzero=0}
      76 |  # Constant_134__1
            %"_val_106__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      77 |  # Cast_135__1
            %"_val_107__1"<?,?> ⬅️ ::Cast(%"_val_106__1") {to=7}
      78 |  # Constant_136__1
            %"_val_108__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      79 |  # Reshape_137__1
            %"_val_109__1"<?,?> ⬅️ ::Reshape(%"_val_107__1", %"_val_108__1") {allowzero=0}
      80 |  # Constant_138__1
            %"_val_110__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      81 |  # Cast_139__1
            %"_val_111__1"<?,?> ⬅️ ::Cast(%"_val_110__1") {to=7}
      82 |  # Constant_140__1
            %"_val_112__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      83 |  # Reshape_141__1
            %"_val_113__1"<?,?> ⬅️ ::Reshape(%"_val_111__1", %"_val_112__1") {allowzero=0}
      84 |  # Slice_142__1
            %"slice_3__1"<?,?> ⬅️ ::Slice(%"unsqueeze_3__1", %"_val_101__1", %"_val_105__1", %"_val_109__1", %"_val_113__1")
      85 |  # Constant_143__1
            %"_val_115__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      86 |  # Cast_144__1
            %"_val_116__1"<?,?> ⬅️ ::Cast(%"_val_115__1") {to=7}
      87 |  # Constant_145__1
            %"_val_117__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      88 |  # Reshape_146__1
            %"_val_118__1"<?,?> ⬅️ ::Reshape(%"_val_116__1", %"_val_117__1") {allowzero=0}
      89 |  # Constant_147__1
            %"_val_119__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      90 |  # Cast_148__1
            %"_val_120__1"<?,?> ⬅️ ::Cast(%"_val_119__1") {to=7}
      91 |  # Constant_149__1
            %"_val_121__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      92 |  # Reshape_150__1
            %"_val_122__1"<?,?> ⬅️ ::Reshape(%"_val_120__1", %"_val_121__1") {allowzero=0}
      93 |  # Constant_151__1
            %"_val_123__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      94 |  # Cast_152__1
            %"_val_124__1"<?,?> ⬅️ ::Cast(%"_val_123__1") {to=7}
      95 |  # Constant_153__1
            %"_val_125__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      96 |  # Reshape_154__1
            %"_val_126__1"<?,?> ⬅️ ::Reshape(%"_val_124__1", %"_val_125__1") {allowzero=0}
      97 |  # Constant_155__1
            %"_val_127__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
      98 |  # Cast_156__1
            %"_val_128__1"<?,?> ⬅️ ::Cast(%"_val_127__1") {to=7}
      99 |  # Constant_157__1
            %"_val_129__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     100 |  # Reshape_158__1
            %"_val_130__1"<?,?> ⬅️ ::Reshape(%"_val_128__1", %"_val_129__1") {allowzero=0}
     101 |  # Slice_159__1
            %"slice_4__1"<?,?> ⬅️ ::Slice(%"slice_3__1", %"_val_118__1", %"_val_122__1", %"_val_126__1", %"_val_130__1")
     102 |  # Constant_160__1
            %"_val_132__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     103 |  # Reshape_161__1
            %"_val_133__1"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_132__1") {allowzero=0}
     104 |  # Constant_162__1
            %"_val_134__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     105 |  # Constant_163__1
            %"_val_135__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     106 |  # Constant_164__1
            %"_val_136__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     107 |  # Concat_165__1
            %"_val_137__1"<?,?> ⬅️ ::Concat(%"_val_133__1", %"_val_134__1", %"_val_135__1", %"_val_136__1") {axis=0}
     108 |  # n0__15
            %"size_0__15"<?,?> ⬅️ ::Cast(%"_val_137__1") {to=7}
     109 |  # n1__15
            %"size_1__15"<?,?> ⬅️ ::Abs(%"size_0__15")
     110 |  # n2__15
            %"expand_1__1"<?,?> ⬅️ ::Expand(%"slice_4__1", %"size_1__15")
     111 |  # n0__16
            %"clone__1"<?,?> ⬅️ ::Identity(%"expand_1__1")
     112 |  # Constant_168__1
            %"_val_140__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     113 |  # Cast_169__1
            %"_val_141__1"<?,?> ⬅️ ::Cast(%"_val_140__1") {to=7}
     114 |  # Constant_170__1
            %"_val_142__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     115 |  # Reshape_171__1
            %"_val_143__1"<?,?> ⬅️ ::Reshape(%"_val_141__1", %"_val_142__1") {allowzero=0}
     116 |  # Constant_172__1
            %"_val_144__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     117 |  # Cast_173__1
            %"_val_145__1"<?,?> ⬅️ ::Cast(%"_val_144__1") {to=7}
     118 |  # Constant_174__1
            %"_val_146__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     119 |  # Reshape_175__1
            %"_val_147__1"<?,?> ⬅️ ::Reshape(%"_val_145__1", %"_val_146__1") {allowzero=0}
     120 |  # Constant_176__1
            %"_val_148__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     121 |  # Cast_177__1
            %"_val_149__1"<?,?> ⬅️ ::Cast(%"_val_148__1") {to=7}
     122 |  # Constant_178__1
            %"_val_150__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     123 |  # Reshape_179__1
            %"_val_151__1"<?,?> ⬅️ ::Reshape(%"_val_149__1", %"_val_150__1") {allowzero=0}
     124 |  # Constant_180__1
            %"_val_152__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     125 |  # Cast_181__1
            %"_val_153__1"<?,?> ⬅️ ::Cast(%"_val_152__1") {to=7}
     126 |  # Constant_182__1
            %"_val_154__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     127 |  # Reshape_183__1
            %"_val_155__1"<?,?> ⬅️ ::Reshape(%"_val_153__1", %"_val_154__1") {allowzero=0}
     128 |  # Slice_184__1
            %"slice_5__1"<?,?> ⬅️ ::Slice(%"clone__1", %"_val_143__1", %"_val_147__1", %"_val_151__1", %"_val_155__1")
     129 |  # Constant_185__1
            %"_val_157__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     130 |  # Cast_186__1
            %"_val_158__1"<?,?> ⬅️ ::Cast(%"_val_157__1") {to=7}
     131 |  # Constant_187__1
            %"_val_159__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     132 |  # Reshape_188__1
            %"_val_160__1"<?,?> ⬅️ ::Reshape(%"_val_158__1", %"_val_159__1") {allowzero=0}
     133 |  # Constant_189__1
            %"_val_161__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     134 |  # Cast_190__1
            %"_val_162__1"<?,?> ⬅️ ::Cast(%"_val_161__1") {to=7}
     135 |  # Constant_191__1
            %"_val_163__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     136 |  # Reshape_192__1
            %"_val_164__1"<?,?> ⬅️ ::Reshape(%"_val_162__1", %"_val_163__1") {allowzero=0}
     137 |  # Constant_193__1
            %"_val_165__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     138 |  # Cast_194__1
            %"_val_166__1"<?,?> ⬅️ ::Cast(%"_val_165__1") {to=7}
     139 |  # Constant_195__1
            %"_val_167__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     140 |  # Reshape_196__1
            %"_val_168__1"<?,?> ⬅️ ::Reshape(%"_val_166__1", %"_val_167__1") {allowzero=0}
     141 |  # Constant_197__1
            %"_val_169__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     142 |  # Cast_198__1
            %"_val_170__1"<?,?> ⬅️ ::Cast(%"_val_169__1") {to=7}
     143 |  # Constant_199__1
            %"_val_171__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     144 |  # Reshape_200__1
            %"_val_172__1"<?,?> ⬅️ ::Reshape(%"_val_170__1", %"_val_171__1") {allowzero=0}
     145 |  # Slice_201__1
            %"slice_6__1"<?,?> ⬅️ ::Slice(%"slice_5__1", %"_val_160__1", %"_val_164__1", %"_val_168__1", %"_val_172__1")
     146 |  # Constant_202__1
            %"_val_174__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     147 |  # Cast_203__1
            %"_val_175__1"<?,?> ⬅️ ::Cast(%"_val_174__1") {to=7}
     148 |  # Constant_204__1
            %"_val_176__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     149 |  # Reshape_205__1
            %"_val_177__1"<?,?> ⬅️ ::Reshape(%"_val_175__1", %"_val_176__1") {allowzero=0}
     150 |  # Constant_206__1
            %"_val_178__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     151 |  # Cast_207__1
            %"_val_179__1"<?,?> ⬅️ ::Cast(%"_val_178__1") {to=7}
     152 |  # Constant_208__1
            %"_val_180__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     153 |  # Reshape_209__1
            %"_val_181__1"<?,?> ⬅️ ::Reshape(%"_val_179__1", %"_val_180__1") {allowzero=0}
     154 |  # Constant_210__1
            %"_val_182__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     155 |  # Cast_211__1
            %"_val_183__1"<?,?> ⬅️ ::Cast(%"_val_182__1") {to=7}
     156 |  # Constant_212__1
            %"_val_184__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     157 |  # Reshape_213__1
            %"_val_185__1"<?,?> ⬅️ ::Reshape(%"_val_183__1", %"_val_184__1") {allowzero=0}
     158 |  # Constant_214__1
            %"_val_186__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     159 |  # Cast_215__1
            %"_val_187__1"<?,?> ⬅️ ::Cast(%"_val_186__1") {to=7}
     160 |  # Constant_216__1
            %"_val_188__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     161 |  # Reshape_217__1
            %"_val_189__1"<?,?> ⬅️ ::Reshape(%"_val_187__1", %"_val_188__1") {allowzero=0}
     162 |  # Slice_218__1
            %"slice_7__1"<?,?> ⬅️ ::Slice(%"slice_6__1", %"_val_177__1", %"_val_181__1", %"_val_185__1", %"_val_189__1")
     163 |  # Constant_219__1
            %"_val_191__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     164 |  # Cast_220__1
            %"_val_192__1"<?,?> ⬅️ ::Cast(%"_val_191__1") {to=7}
     165 |  # Constant_221__1
            %"_val_193__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     166 |  # Reshape_222__1
            %"_val_194__1"<?,?> ⬅️ ::Reshape(%"_val_192__1", %"_val_193__1") {allowzero=0}
     167 |  # Constant_223__1
            %"_val_195__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     168 |  # Cast_224__1
            %"_val_196__1"<?,?> ⬅️ ::Cast(%"_val_195__1") {to=7}
     169 |  # Constant_225__1
            %"_val_197__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     170 |  # Reshape_226__1
            %"_val_198__1"<?,?> ⬅️ ::Reshape(%"_val_196__1", %"_val_197__1") {allowzero=0}
     171 |  # Constant_227__1
            %"_val_199__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     172 |  # Cast_228__1
            %"_val_200__1"<?,?> ⬅️ ::Cast(%"_val_199__1") {to=7}
     173 |  # Constant_229__1
            %"_val_201__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     174 |  # Reshape_230__1
            %"_val_202__1"<?,?> ⬅️ ::Reshape(%"_val_200__1", %"_val_201__1") {allowzero=0}
     175 |  # Constant_231__1
            %"_val_203__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     176 |  # Cast_232__1
            %"_val_204__1"<?,?> ⬅️ ::Cast(%"_val_203__1") {to=7}
     177 |  # Constant_233__1
            %"_val_205__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     178 |  # Reshape_234__1
            %"_val_206__1"<?,?> ⬅️ ::Reshape(%"_val_204__1", %"_val_205__1") {allowzero=0}
     179 |  # Slice_235__1
            %"slice_8__1"<?,?> ⬅️ ::Slice(%"l_attention_mask_", %"_val_194__1", %"_val_198__1", %"_val_202__1", %"_val_206__1")
     180 |  # n0__17
            %"dim__17"<?,?> ⬅️ ::Constant() {value_int=1}
     181 |  # n1__17
            %"dim_0__17"<?,?> ⬅️ ::Cast(%"dim__17") {to=7}
     182 |  # n2__17
            %"unsqueeze_4__1"<?,?> ⬅️ ::Unsqueeze(%"slice_8__1", %"dim_0__17")
     183 |  # n0__18
            %"dim__18"<?,?> ⬅️ ::Constant() {value_int=2}
     184 |  # n1__18
            %"dim_0__18"<?,?> ⬅️ ::Cast(%"dim__18") {to=7}
     185 |  # n2__18
            %"unsqueeze_5__1"<?,?> ⬅️ ::Unsqueeze(%"unsqueeze_4__1", %"dim_0__18")
     186 |  # Constant_238__1
            %"_val_210__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     187 |  # Cast_239__1
            %"_val_211__1"<?,?> ⬅️ ::Cast(%"_val_210__1") {to=7}
     188 |  # Constant_240__1
            %"_val_212__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     189 |  # Reshape_241__1
            %"_val_213__1"<?,?> ⬅️ ::Reshape(%"_val_211__1", %"_val_212__1") {allowzero=0}
     190 |  # Constant_242__1
            %"_val_214__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     191 |  # Cast_243__1
            %"_val_215__1"<?,?> ⬅️ ::Cast(%"_val_214__1") {to=7}
     192 |  # Constant_244__1
            %"_val_216__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     193 |  # Reshape_245__1
            %"_val_217__1"<?,?> ⬅️ ::Reshape(%"_val_215__1", %"_val_216__1") {allowzero=0}
     194 |  # Constant_246__1
            %"_val_218__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     195 |  # Cast_247__1
            %"_val_219__1"<?,?> ⬅️ ::Cast(%"_val_218__1") {to=7}
     196 |  # Constant_248__1
            %"_val_220__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     197 |  # Reshape_249__1
            %"_val_221__1"<?,?> ⬅️ ::Reshape(%"_val_219__1", %"_val_220__1") {allowzero=0}
     198 |  # Constant_250__1
            %"_val_222__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     199 |  # Cast_251__1
            %"_val_223__1"<?,?> ⬅️ ::Cast(%"_val_222__1") {to=7}
     200 |  # Constant_252__1
            %"_val_224__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     201 |  # Reshape_253__1
            %"_val_225__1"<?,?> ⬅️ ::Reshape(%"_val_223__1", %"_val_224__1") {allowzero=0}
     202 |  # Slice_254__1
            %"slice_9__1"<?,?> ⬅️ ::Slice(%"unsqueeze_5__1", %"_val_213__1", %"_val_217__1", %"_val_221__1", %"_val_225__1")
     203 |  # Cast_255__1
            %"convert_element_type_default_1__1"<?,?> ⬅️ ::Cast(%"slice_9__1") {to=1}
     204 |  # n0__19
            %"alpha__19"<?,?> ⬅️ ::Constant() {value_float=1.0}
     205 |  # n1__19
            %"alpha_0__19"<?,?> ⬅️ ::CastLike(%"alpha__19", %"convert_element_type_default_1__1")
     206 |  # n2__19
            %"other_1__19"<?,?> ⬅️ ::Mul(%"convert_element_type_default_1__1", %"alpha_0__19")
     207 |  # n3__19
            %"add_1__1"<?,?> ⬅️ ::Add(%"slice_7__1", %"other_1__19")
     208 |  # Constant_257__1
            %"_val_229__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     209 |  # Cast_258__1
            %"scalar_tensor_default__1"<?,?> ⬅️ ::Cast(%"_val_229__1") {to=1}
     210 |  # n0__20
            %"eq__1"<?,?> ⬅️ ::Equal(%"add_1__1", %"scalar_tensor_default__1")
     211 |  # Constant_260__1
            %"_val_232__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     212 |  # Cast_261__1
            %"_val_233__1"<?,?> ⬅️ ::Cast(%"_val_232__1") {to=7}
     213 |  # Constant_262__1
            %"_val_234__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     214 |  # Reshape_263__1
            %"_val_235__1"<?,?> ⬅️ ::Reshape(%"_val_233__1", %"_val_234__1") {allowzero=0}
     215 |  # Constant_264__1
            %"_val_236__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     216 |  # Cast_265__1
            %"_val_237__1"<?,?> ⬅️ ::Cast(%"_val_236__1") {to=7}
     217 |  # Constant_266__1
            %"_val_238__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     218 |  # Reshape_267__1
            %"_val_239__1"<?,?> ⬅️ ::Reshape(%"_val_237__1", %"_val_238__1") {allowzero=0}
     219 |  # Constant_268__1
            %"_val_240__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     220 |  # Cast_269__1
            %"_val_241__1"<?,?> ⬅️ ::Cast(%"_val_240__1") {to=7}
     221 |  # Constant_270__1
            %"_val_242__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     222 |  # Reshape_271__1
            %"_val_243__1"<?,?> ⬅️ ::Reshape(%"_val_241__1", %"_val_242__1") {allowzero=0}
     223 |  # Constant_272__1
            %"_val_244__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     224 |  # Cast_273__1
            %"_val_245__1"<?,?> ⬅️ ::Cast(%"_val_244__1") {to=7}
     225 |  # Constant_274__1
            %"_val_246__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     226 |  # Reshape_275__1
            %"_val_247__1"<?,?> ⬅️ ::Reshape(%"_val_245__1", %"_val_246__1") {allowzero=0}
     227 |  # Slice_276__1
            %"slice_10__1"<?,?> ⬅️ ::Slice(%"clone__1", %"_val_235__1", %"_val_239__1", %"_val_243__1", %"_val_247__1")
     228 |  # Constant_277__1
            %"_val_249__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     229 |  # Cast_278__1
            %"_val_250__1"<?,?> ⬅️ ::Cast(%"_val_249__1") {to=7}
     230 |  # Constant_279__1
            %"_val_251__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     231 |  # Reshape_280__1
            %"_val_252__1"<?,?> ⬅️ ::Reshape(%"_val_250__1", %"_val_251__1") {allowzero=0}
     232 |  # Constant_281__1
            %"_val_253__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     233 |  # Cast_282__1
            %"_val_254__1"<?,?> ⬅️ ::Cast(%"_val_253__1") {to=7}
     234 |  # Constant_283__1
            %"_val_255__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     235 |  # Reshape_284__1
            %"_val_256__1"<?,?> ⬅️ ::Reshape(%"_val_254__1", %"_val_255__1") {allowzero=0}
     236 |  # Constant_285__1
            %"_val_257__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     237 |  # Cast_286__1
            %"_val_258__1"<?,?> ⬅️ ::Cast(%"_val_257__1") {to=7}
     238 |  # Constant_287__1
            %"_val_259__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     239 |  # Reshape_288__1
            %"_val_260__1"<?,?> ⬅️ ::Reshape(%"_val_258__1", %"_val_259__1") {allowzero=0}
     240 |  # Constant_289__1
            %"_val_261__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     241 |  # Cast_290__1
            %"_val_262__1"<?,?> ⬅️ ::Cast(%"_val_261__1") {to=7}
     242 |  # Constant_291__1
            %"_val_263__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     243 |  # Reshape_292__1
            %"_val_264__1"<?,?> ⬅️ ::Reshape(%"_val_262__1", %"_val_263__1") {allowzero=0}
     244 |  # Slice_293__1
            %"slice_11__1"<?,?> ⬅️ ::Slice(%"slice_10__1", %"_val_252__1", %"_val_256__1", %"_val_260__1", %"_val_264__1")
     245 |  # Constant_294__1
            %"_val_266__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     246 |  # Cast_295__1
            %"_val_267__1"<?,?> ⬅️ ::Cast(%"_val_266__1") {to=7}
     247 |  # Constant_296__1
            %"_val_268__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     248 |  # Reshape_297__1
            %"_val_269__1"<?,?> ⬅️ ::Reshape(%"_val_267__1", %"_val_268__1") {allowzero=0}
     249 |  # Constant_298__1
            %"_val_270__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     250 |  # Cast_299__1
            %"_val_271__1"<?,?> ⬅️ ::Cast(%"_val_270__1") {to=7}
     251 |  # Constant_300__1
            %"_val_272__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     252 |  # Reshape_301__1
            %"_val_273__1"<?,?> ⬅️ ::Reshape(%"_val_271__1", %"_val_272__1") {allowzero=0}
     253 |  # Constant_302__1
            %"_val_274__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     254 |  # Cast_303__1
            %"_val_275__1"<?,?> ⬅️ ::Cast(%"_val_274__1") {to=7}
     255 |  # Constant_304__1
            %"_val_276__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     256 |  # Reshape_305__1
            %"_val_277__1"<?,?> ⬅️ ::Reshape(%"_val_275__1", %"_val_276__1") {allowzero=0}
     257 |  # Constant_306__1
            %"_val_278__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     258 |  # Cast_307__1
            %"_val_279__1"<?,?> ⬅️ ::Cast(%"_val_278__1") {to=7}
     259 |  # Constant_308__1
            %"_val_280__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     260 |  # Reshape_309__1
            %"_val_281__1"<?,?> ⬅️ ::Reshape(%"_val_279__1", %"_val_280__1") {allowzero=0}
     261 |  # Slice_310__1
            %"slice_12__1"<?,?> ⬅️ ::Slice(%"slice_11__1", %"_val_269__1", %"_val_273__1", %"_val_277__1", %"_val_281__1")
     262 |  # Constant_311__1
            %"_val_283__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='')}
     263 |  # CastLike_312__1
            %"_val_284__1"<?,?> ⬅️ ::CastLike(%"_val_283__1", %"slice_12__1")
     264 |  # Where_313__1
            %"masked_fill__1"<?,?> ⬅️ ::Where(%"eq__1", %"_val_284__1", %"slice_12__1")
     265 |  # Constant_314__1
            %"_val_286__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     266 |  # Cast_315__1
            %"_val_287__1"<?,?> ⬅️ ::Cast(%"_val_286__1") {to=7}
     267 |  # Constant_316__1
            %"_val_288__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     268 |  # Reshape_317__1
            %"_val_289__1"<?,?> ⬅️ ::Reshape(%"_val_287__1", %"_val_288__1") {allowzero=0}
     269 |  # Constant_318__1
            %"_val_290__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     270 |  # Cast_319__1
            %"_val_291__1"<?,?> ⬅️ ::Cast(%"_val_290__1") {to=7}
     271 |  # Constant_320__1
            %"_val_292__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     272 |  # Reshape_321__1
            %"_val_293__1"<?,?> ⬅️ ::Reshape(%"_val_291__1", %"_val_292__1") {allowzero=0}
     273 |  # Constant_322__1
            %"_val_294__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     274 |  # Cast_323__1
            %"_val_295__1"<?,?> ⬅️ ::Cast(%"_val_294__1") {to=7}
     275 |  # Constant_324__1
            %"_val_296__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     276 |  # Reshape_325__1
            %"_val_297__1"<?,?> ⬅️ ::Reshape(%"_val_295__1", %"_val_296__1") {allowzero=0}
     277 |  # Constant_326__1
            %"_val_298__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     278 |  # Cast_327__1
            %"_val_299__1"<?,?> ⬅️ ::Cast(%"_val_298__1") {to=7}
     279 |  # Constant_328__1
            %"_val_300__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     280 |  # Reshape_329__1
            %"_val_301__1"<?,?> ⬅️ ::Reshape(%"_val_299__1", %"_val_300__1") {allowzero=0}
     281 |  # Slice_330__1
            %"slice_13__1"<?,?> ⬅️ ::Slice(%"clone__1", %"_val_289__1", %"_val_293__1", %"_val_297__1", %"_val_301__1")
     282 |  # Constant_331__1
            %"_val_303__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     283 |  # Cast_332__1
            %"_val_304__1"<?,?> ⬅️ ::Cast(%"_val_303__1") {to=7}
     284 |  # Constant_333__1
            %"_val_305__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     285 |  # Reshape_334__1
            %"_val_306__1"<?,?> ⬅️ ::Reshape(%"_val_304__1", %"_val_305__1") {allowzero=0}
     286 |  # Constant_335__1
            %"_val_307__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     287 |  # Cast_336__1
            %"_val_308__1"<?,?> ⬅️ ::Cast(%"_val_307__1") {to=7}
     288 |  # Constant_337__1
            %"_val_309__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     289 |  # Reshape_338__1
            %"_val_310__1"<?,?> ⬅️ ::Reshape(%"_val_308__1", %"_val_309__1") {allowzero=0}
     290 |  # Constant_339__1
            %"_val_311__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     291 |  # Cast_340__1
            %"_val_312__1"<?,?> ⬅️ ::Cast(%"_val_311__1") {to=7}
     292 |  # Constant_341__1
            %"_val_313__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     293 |  # Reshape_342__1
            %"_val_314__1"<?,?> ⬅️ ::Reshape(%"_val_312__1", %"_val_313__1") {allowzero=0}
     294 |  # Constant_343__1
            %"_val_315__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     295 |  # Cast_344__1
            %"_val_316__1"<?,?> ⬅️ ::Cast(%"_val_315__1") {to=7}
     296 |  # Constant_345__1
            %"_val_317__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     297 |  # Reshape_346__1
            %"_val_318__1"<?,?> ⬅️ ::Reshape(%"_val_316__1", %"_val_317__1") {allowzero=0}
     298 |  # Slice_347__1
            %"slice_14__1"<?,?> ⬅️ ::Slice(%"slice_13__1", %"_val_306__1", %"_val_310__1", %"_val_314__1", %"_val_318__1")
     299 |  # Constant_348__1
            %"_val_320__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     300 |  # Cast_349__1
            %"_val_321__1"<?,?> ⬅️ ::Cast(%"_val_320__1") {to=7}
     301 |  # Constant_350__1
            %"_val_322__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     302 |  # Reshape_351__1
            %"_val_323__1"<?,?> ⬅️ ::Reshape(%"_val_321__1", %"_val_322__1") {allowzero=0}
     303 |  # Constant_352__1
            %"_val_324__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     304 |  # Cast_353__1
            %"_val_325__1"<?,?> ⬅️ ::Cast(%"_val_324__1") {to=7}
     305 |  # Constant_354__1
            %"_val_326__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     306 |  # Reshape_355__1
            %"_val_327__1"<?,?> ⬅️ ::Reshape(%"_val_325__1", %"_val_326__1") {allowzero=0}
     307 |  # Constant_356__1
            %"_val_328__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     308 |  # Cast_357__1
            %"_val_329__1"<?,?> ⬅️ ::Cast(%"_val_328__1") {to=7}
     309 |  # Constant_358__1
            %"_val_330__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     310 |  # Reshape_359__1
            %"_val_331__1"<?,?> ⬅️ ::Reshape(%"_val_329__1", %"_val_330__1") {allowzero=0}
     311 |  # Constant_360__1
            %"_val_332__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     312 |  # Cast_361__1
            %"_val_333__1"<?,?> ⬅️ ::Cast(%"_val_332__1") {to=7}
     313 |  # Constant_362__1
            %"_val_334__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     314 |  # Reshape_363__1
            %"_val_335__1"<?,?> ⬅️ ::Reshape(%"_val_333__1", %"_val_334__1") {allowzero=0}
     315 |  # Slice_364__1
            %"slice_15__1"<?,?> ⬅️ ::Slice(%"slice_14__1", %"_val_323__1", %"_val_327__1", %"_val_331__1", %"_val_335__1")
     316 |  # n0__21
            %"copy__1"<?,?> ⬅️ ::CastLike(%"masked_fill__1", %"slice_15__1")
     317 |  # Constant_366__1
            %"_val_338__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     318 |  # Cast_367__1
            %"_val_339__1"<?,?> ⬅️ ::Cast(%"_val_338__1") {to=7}
     319 |  # Constant_368__1
            %"_val_340__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     320 |  # Reshape_369__1
            %"_val_341__1"<?,?> ⬅️ ::Reshape(%"_val_339__1", %"_val_340__1") {allowzero=0}
     321 |  # Constant_370__1
            %"_val_342__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     322 |  # Cast_371__1
            %"_val_343__1"<?,?> ⬅️ ::Cast(%"_val_342__1") {to=7}
     323 |  # Constant_372__1
            %"_val_344__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     324 |  # Reshape_373__1
            %"_val_345__1"<?,?> ⬅️ ::Reshape(%"_val_343__1", %"_val_344__1") {allowzero=0}
     325 |  # Constant_374__1
            %"_val_346__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     326 |  # Cast_375__1
            %"_val_347__1"<?,?> ⬅️ ::Cast(%"_val_346__1") {to=7}
     327 |  # Constant_376__1
            %"_val_348__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     328 |  # Reshape_377__1
            %"_val_349__1"<?,?> ⬅️ ::Reshape(%"_val_347__1", %"_val_348__1") {allowzero=0}
     329 |  # Constant_378__1
            %"_val_350__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     330 |  # Cast_379__1
            %"_val_351__1"<?,?> ⬅️ ::Cast(%"_val_350__1") {to=7}
     331 |  # Constant_380__1
            %"_val_352__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     332 |  # Reshape_381__1
            %"_val_353__1"<?,?> ⬅️ ::Reshape(%"_val_351__1", %"_val_352__1") {allowzero=0}
     333 |  # Slice_382__1
            %"slice_16__1"<?,?> ⬅️ ::Slice(%"clone__1", %"_val_341__1", %"_val_345__1", %"_val_349__1", %"_val_353__1")
     334 |  # Constant_383__1
            %"_val_355__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     335 |  # Cast_384__1
            %"_val_356__1"<?,?> ⬅️ ::Cast(%"_val_355__1") {to=7}
     336 |  # Constant_385__1
            %"_val_357__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     337 |  # Reshape_386__1
            %"_val_358__1"<?,?> ⬅️ ::Reshape(%"_val_356__1", %"_val_357__1") {allowzero=0}
     338 |  # Constant_387__1
            %"_val_359__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     339 |  # Cast_388__1
            %"_val_360__1"<?,?> ⬅️ ::Cast(%"_val_359__1") {to=7}
     340 |  # Constant_389__1
            %"_val_361__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     341 |  # Reshape_390__1
            %"_val_362__1"<?,?> ⬅️ ::Reshape(%"_val_360__1", %"_val_361__1") {allowzero=0}
     342 |  # Constant_391__1
            %"_val_363__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     343 |  # Cast_392__1
            %"_val_364__1"<?,?> ⬅️ ::Cast(%"_val_363__1") {to=7}
     344 |  # Constant_393__1
            %"_val_365__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     345 |  # Reshape_394__1
            %"_val_366__1"<?,?> ⬅️ ::Reshape(%"_val_364__1", %"_val_365__1") {allowzero=0}
     346 |  # Constant_395__1
            %"_val_367__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     347 |  # Cast_396__1
            %"_val_368__1"<?,?> ⬅️ ::Cast(%"_val_367__1") {to=7}
     348 |  # Constant_397__1
            %"_val_369__1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     349 |  # Reshape_398__1
            %"_val_370__1"<?,?> ⬅️ ::Reshape(%"_val_368__1", %"_val_369__1") {allowzero=0}
     350 |  # Slice_399__1
            %"slice_17__1"<?,?> ⬅️ ::Slice(%"slice_16__1", %"_val_358__1", %"_val_362__1", %"_val_366__1", %"_val_370__1")
     351 |  # Constant_400__1
            %"_val_372__1"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     352 |  # Shape_401__1
            %"_val_373__1"<?,?> ⬅️ ::Shape(%"slice_17__1") {start=0}
     353 |  # Constant_402__1
            %"_val_374__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     354 |  # Gather_403__1
            %"_val_375__1"<?,?> ⬅️ ::Gather(%"_val_373__1", %"_val_374__1") {axis=0}
     355 |  # Constant_404__1
            %"_val_376__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     356 |  # Constant_405__1
            %"_val_377__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     357 |  # Range_406__1
            %"_val_378__1"<?,?> ⬅️ ::Range(%"_val_376__1", %"_val_375__1", %"_val_377__1")
     358 |  # Constant_407__1
            %"_val_379__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     359 |  # Unsqueeze_408__1
            %"_val_380__1"<?,?> ⬅️ ::Unsqueeze(%"_val_379__1", %"_val_372__1")
     360 |  # Constant_409__1
            %"_val_381__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     361 |  # Unsqueeze_410__1
            %"_val_382__1"<?,?> ⬅️ ::Unsqueeze(%"_val_381__1", %"_val_372__1")
     362 |  # Constant_411__1
            %"_val_383__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     363 |  # Unsqueeze_412__1
            %"_val_384__1"<?,?> ⬅️ ::Unsqueeze(%"_val_383__1", %"_val_372__1")
     364 |  # Slice_413__1
            %"_val_385__1"<?,?> ⬅️ ::Slice(%"_val_378__1", %"_val_380__1", %"_val_382__1", %"_val_372__1", %"_val_384__1")
     365 |  # Constant_414__1
            %"_val_386__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     366 |  # Unsqueeze_415__1
            %"_val_387__1"<?,?> ⬅️ ::Unsqueeze(%"_val_385__1", %"_val_386__1")
     367 |  # Transpose_416__1
            %"_val_388__1"<?,?> ⬅️ ::Transpose(%"copy__1") {perm=[2, 1, 0, 3]}
     368 |  # Transpose_417__1
            %"_val_389__1"<?,?> ⬅️ ::Transpose(%"slice_17__1") {perm=[2, 1, 0, 3]}
     369 |  # ScatterND_418__1
            %"_val_390__1"<?,?> ⬅️ ::ScatterND(%"_val_389__1", %"_val_387__1", %"_val_388__1") {reduction=none}
     370 |  # Transpose_419__1
            %"slice_scatter__1"<?,?> ⬅️ ::Transpose(%"_val_390__1") {perm=[2, 1, 0, 3]}
     371 |  # Constant_420__1
            %"_val_392__1"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     372 |  # Shape_421__1
            %"_val_393__1"<?,?> ⬅️ ::Shape(%"slice_16__1") {start=0}
     373 |  # Constant_422__1
            %"_val_394__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     374 |  # Gather_423__1
            %"_val_395__1"<?,?> ⬅️ ::Gather(%"_val_393__1", %"_val_394__1") {axis=0}
     375 |  # Constant_424__1
            %"_val_396__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     376 |  # Constant_425__1
            %"_val_397__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     377 |  # Range_426__1
            %"_val_398__1"<?,?> ⬅️ ::Range(%"_val_396__1", %"_val_395__1", %"_val_397__1")
     378 |  # Constant_427__1
            %"_val_399__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     379 |  # Unsqueeze_428__1
            %"_val_400__1"<?,?> ⬅️ ::Unsqueeze(%"_val_399__1", %"_val_392__1")
     380 |  # Constant_429__1
            %"_val_401__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     381 |  # Unsqueeze_430__1
            %"_val_402__1"<?,?> ⬅️ ::Unsqueeze(%"_val_401__1", %"_val_392__1")
     382 |  # Constant_431__1
            %"_val_403__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     383 |  # Unsqueeze_432__1
            %"_val_404__1"<?,?> ⬅️ ::Unsqueeze(%"_val_403__1", %"_val_392__1")
     384 |  # Slice_433__1
            %"_val_405__1"<?,?> ⬅️ ::Slice(%"_val_398__1", %"_val_400__1", %"_val_402__1", %"_val_392__1", %"_val_404__1")
     385 |  # Constant_434__1
            %"_val_406__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     386 |  # Unsqueeze_435__1
            %"_val_407__1"<?,?> ⬅️ ::Unsqueeze(%"_val_405__1", %"_val_406__1")
     387 |  # Transpose_436__1
            %"_val_408__1"<?,?> ⬅️ ::Transpose(%"slice_scatter__1") {perm=[1, 0, 2, 3]}
     388 |  # Transpose_437__1
            %"_val_409__1"<?,?> ⬅️ ::Transpose(%"slice_16__1") {perm=[1, 0, 2, 3]}
     389 |  # ScatterND_438__1
            %"_val_410__1"<?,?> ⬅️ ::ScatterND(%"_val_409__1", %"_val_407__1", %"_val_408__1") {reduction=none}
     390 |  # Transpose_439__1
            %"slice_scatter_1__1"<?,?> ⬅️ ::Transpose(%"_val_410__1") {perm=[1, 0, 2, 3]}
     391 |  # Constant_440__1
            %"_val_412__1"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     392 |  # Shape_441__1
            %"_val_413__1"<?,?> ⬅️ ::Shape(%"clone__1") {start=0}
     393 |  # Constant_442__1
            %"_val_414__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     394 |  # Gather_443__1
            %"_val_415__1"<?,?> ⬅️ ::Gather(%"_val_413__1", %"_val_414__1") {axis=0}
     395 |  # Constant_444__1
            %"_val_416__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     396 |  # Constant_445__1
            %"_val_417__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     397 |  # Range_446__1
            %"_val_418__1"<?,?> ⬅️ ::Range(%"_val_416__1", %"_val_415__1", %"_val_417__1")
     398 |  # Constant_447__1
            %"_val_419__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     399 |  # Unsqueeze_448__1
            %"_val_420__1"<?,?> ⬅️ ::Unsqueeze(%"_val_419__1", %"_val_412__1")
     400 |  # Constant_449__1
            %"_val_421__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     401 |  # Unsqueeze_450__1
            %"_val_422__1"<?,?> ⬅️ ::Unsqueeze(%"_val_421__1", %"_val_412__1")
     402 |  # Constant_451__1
            %"_val_423__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     403 |  # Unsqueeze_452__1
            %"_val_424__1"<?,?> ⬅️ ::Unsqueeze(%"_val_423__1", %"_val_412__1")
     404 |  # Slice_453__1
            %"_val_425__1"<?,?> ⬅️ ::Slice(%"_val_418__1", %"_val_420__1", %"_val_422__1", %"_val_412__1", %"_val_424__1")
     405 |  # Constant_454__1
            %"_val_426__1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     406 |  # Unsqueeze_455__1
            %"_val_427__1"<?,?> ⬅️ ::Unsqueeze(%"_val_425__1", %"_val_426__1")
     407 |  # ScatterND_456__1
            %"slice_scatter_2__1"<?,?> ⬅️ ::ScatterND(%"clone__1", %"_val_427__1", %"slice_scatter_1__1") {reduction=none}
     408 |  # n0__23
            %"model_layers_0_input_layernorm_1__22"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_embed_tokens_1__1", %"model.layers.0.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
     409 |  # Transpose_5__25
            %"t__25"<?,?> ⬅️ ::Transpose(%"model.layers.0.self_attn.q_proj.weight") {perm=[1, 0]}
     410 |  # n0__26
            %"mul_3__25"<?,?> ⬅️ ::Mul(%"model_1_1", %"model_1")
     411 |  # Constant_7__25
            %"_val_6__25"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     412 |  # Reshape_8__25
            %"_val_7__25"<?,?> ⬅️ ::Reshape(%"mul_3__25", %"_val_6__25") {allowzero=0}
     413 |  # Constant_9__25
            %"_val_8__25"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     414 |  # Concat_10__25
            %"_val_9__25"<?,?> ⬅️ ::Concat(%"_val_7__25", %"_val_8__25") {axis=0}
     415 |  # n0__27
            %"size_0__27"<?,?> ⬅️ ::Cast(%"_val_9__25") {to=7}
     416 |  # n1__27
            %"view_1__25"<?,?> ⬅️ ::Reshape(%"model_layers_0_input_layernorm_1__22", %"size_0__27")
     417 |  # n0__28
            %"mm__25"<?,?> ⬅️ ::MatMul(%"view_1__25", %"t__25")
     418 |  # Constant_13__25
            %"_val_12__25"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     419 |  # Reshape_14__25
            %"_val_13__25"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_12__25") {allowzero=0}
     420 |  # Constant_15__25
            %"_val_14__25"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     421 |  # Reshape_16__25
            %"_val_15__25"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_14__25") {allowzero=0}
     422 |  # Constant_17__25
            %"_val_16__25"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     423 |  # Concat_18__25
            %"_val_17__25"<?,?> ⬅️ ::Concat(%"_val_13__25", %"_val_15__25", %"_val_16__25") {axis=0}
     424 |  # n0__29
            %"size_0__29"<?,?> ⬅️ ::Cast(%"_val_17__25") {to=7}
     425 |  # n1__29
            %"model_layers_0_self_attn_q_proj_1__24"<?,?> ⬅️ ::Reshape(%"mm__25", %"size_0__29")
     426 |  # Transpose_5__30
            %"t_1__30"<?,?> ⬅️ ::Transpose(%"model.layers.0.self_attn.k_proj.weight") {perm=[1, 0]}
     427 |  # n0__31
            %"mul_4__30"<?,?> ⬅️ ::Mul(%"model_1_1", %"model_1")
     428 |  # Constant_7__30
            %"_val_6__30"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     429 |  # Reshape_8__30
            %"_val_7__30"<?,?> ⬅️ ::Reshape(%"mul_4__30", %"_val_6__30") {allowzero=0}
     430 |  # Constant_9__30
            %"_val_8__30"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     431 |  # Concat_10__30
            %"_val_9__30"<?,?> ⬅️ ::Concat(%"_val_7__30", %"_val_8__30") {axis=0}
     432 |  # n0__32
            %"size_0__32"<?,?> ⬅️ ::Cast(%"_val_9__30") {to=7}
     433 |  # n1__32
            %"view_3__30"<?,?> ⬅️ ::Reshape(%"model_layers_0_input_layernorm_1__22", %"size_0__32")
     434 |  # n0__33
            %"mm_1__30"<?,?> ⬅️ ::MatMul(%"view_3__30", %"t_1__30")
     435 |  # Constant_13__30
            %"_val_12__30"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     436 |  # Reshape_14__30
            %"_val_13__30"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_12__30") {allowzero=0}
     437 |  # Constant_15__30
            %"_val_14__30"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     438 |  # Reshape_16__30
            %"_val_15__30"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_14__30") {allowzero=0}
     439 |  # Constant_17__30
            %"_val_16__30"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     440 |  # Concat_18__30
            %"_val_17__30"<?,?> ⬅️ ::Concat(%"_val_13__30", %"_val_15__30", %"_val_16__30") {axis=0}
     441 |  # n0__34
            %"size_0__34"<?,?> ⬅️ ::Cast(%"_val_17__30") {to=7}
     442 |  # n1__34
            %"model_layers_0_self_attn_k_proj_1__24"<?,?> ⬅️ ::Reshape(%"mm_1__30", %"size_0__34")
     443 |  # Transpose_5__35
            %"t_2__35"<?,?> ⬅️ ::Transpose(%"model.layers.0.self_attn.v_proj.weight") {perm=[1, 0]}
     444 |  # n0__36
            %"mul_5__35"<?,?> ⬅️ ::Mul(%"model_1_1", %"model_1")
     445 |  # Constant_7__35
            %"_val_6__35"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     446 |  # Reshape_8__35
            %"_val_7__35"<?,?> ⬅️ ::Reshape(%"mul_5__35", %"_val_6__35") {allowzero=0}
     447 |  # Constant_9__35
            %"_val_8__35"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     448 |  # Concat_10__35
            %"_val_9__35"<?,?> ⬅️ ::Concat(%"_val_7__35", %"_val_8__35") {axis=0}
     449 |  # n0__37
            %"size_0__37"<?,?> ⬅️ ::Cast(%"_val_9__35") {to=7}
     450 |  # n1__37
            %"view_5__35"<?,?> ⬅️ ::Reshape(%"model_layers_0_input_layernorm_1__22", %"size_0__37")
     451 |  # n0__38
            %"mm_2__35"<?,?> ⬅️ ::MatMul(%"view_5__35", %"t_2__35")
     452 |  # Constant_13__35
            %"_val_12__35"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     453 |  # Reshape_14__35
            %"_val_13__35"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_12__35") {allowzero=0}
     454 |  # Constant_15__35
            %"_val_14__35"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     455 |  # Reshape_16__35
            %"_val_15__35"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_14__35") {allowzero=0}
     456 |  # Constant_17__35
            %"_val_16__35"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     457 |  # Concat_18__35
            %"_val_17__35"<?,?> ⬅️ ::Concat(%"_val_13__35", %"_val_15__35", %"_val_16__35") {axis=0}
     458 |  # n0__39
            %"size_0__39"<?,?> ⬅️ ::Cast(%"_val_17__35") {to=7}
     459 |  # n1__39
            %"model_layers_0_self_attn_v_proj_1__24"<?,?> ⬅️ ::Reshape(%"mm_2__35", %"size_0__39")
     460 |  # Constant_85__24
            %"_val_13__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     461 |  # Reshape_86__24
            %"_val_14__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_13__24") {allowzero=0}
     462 |  # Constant_87__24
            %"_val_15__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     463 |  # Reshape_88__24
            %"_val_16__24"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_15__24") {allowzero=0}
     464 |  # Constant_89__24
            %"_val_17__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     465 |  # Constant_90__24
            %"_val_18__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     466 |  # Concat_91__24
            %"_val_19__24"<?,?> ⬅️ ::Concat(%"_val_14__24", %"_val_16__24", %"_val_17__24", %"_val_18__24") {axis=0}
     467 |  # n0__40
            %"size_0__40"<?,?> ⬅️ ::Cast(%"_val_19__24") {to=7}
     468 |  # n1__40
            %"view_7__24"<?,?> ⬅️ ::Reshape(%"model_layers_0_self_attn_q_proj_1__24", %"size_0__40")
     469 |  # Transpose_93__24
            %"transpose__24"<?,?> ⬅️ ::Transpose(%"view_7__24") {perm=[0, 2, 1, 3]}
     470 |  # Constant_94__24
            %"_val_22__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     471 |  # Reshape_95__24
            %"_val_23__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_22__24") {allowzero=0}
     472 |  # Constant_96__24
            %"_val_24__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     473 |  # Reshape_97__24
            %"_val_25__24"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_24__24") {allowzero=0}
     474 |  # Constant_98__24
            %"_val_26__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     475 |  # Constant_99__24
            %"_val_27__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     476 |  # Concat_100__24
            %"_val_28__24"<?,?> ⬅️ ::Concat(%"_val_23__24", %"_val_25__24", %"_val_26__24", %"_val_27__24") {axis=0}
     477 |  # n0__41
            %"size_0__41"<?,?> ⬅️ ::Cast(%"_val_28__24") {to=7}
     478 |  # n1__41
            %"view_8__24"<?,?> ⬅️ ::Reshape(%"model_layers_0_self_attn_k_proj_1__24", %"size_0__41")
     479 |  # Transpose_102__24
            %"transpose_1__24"<?,?> ⬅️ ::Transpose(%"view_8__24") {perm=[0, 2, 1, 3]}
     480 |  # Constant_103__24
            %"_val_31__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     481 |  # Reshape_104__24
            %"_val_32__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_31__24") {allowzero=0}
     482 |  # Constant_105__24
            %"_val_33__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     483 |  # Reshape_106__24
            %"_val_34__24"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_33__24") {allowzero=0}
     484 |  # Constant_107__24
            %"_val_35__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     485 |  # Constant_108__24
            %"_val_36__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     486 |  # Concat_109__24
            %"_val_37__24"<?,?> ⬅️ ::Concat(%"_val_32__24", %"_val_34__24", %"_val_35__24", %"_val_36__24") {axis=0}
     487 |  # n0__42
            %"size_0__42"<?,?> ⬅️ ::Cast(%"_val_37__24") {to=7}
     488 |  # n1__42
            %"view_9__24"<?,?> ⬅️ ::Reshape(%"model_layers_0_self_attn_v_proj_1__24", %"size_0__42")
     489 |  # Transpose_111__24
            %"transpose_2__24"<?,?> ⬅️ ::Transpose(%"view_9__24") {perm=[0, 2, 1, 3]}
     490 |  # n0__44
            %"dim__44"<?,?> ⬅️ ::Constant() {value_int=0}
     491 |  # n1__44
            %"dim_0__44"<?,?> ⬅️ ::Cast(%"dim__44") {to=7}
     492 |  # n2__44
            %"unsqueeze_6__43"<?,?> ⬅️ ::Unsqueeze(%"model.layers.0.self_attn.rotary_emb.inv_freq", %"dim_0__44")
     493 |  # Constant_31__43
            %"_val_3__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     494 |  # Cast_32__43
            %"_val_4__43"<?,?> ⬅️ ::Cast(%"_val_3__43") {to=7}
     495 |  # Constant_33__43
            %"_val_5__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     496 |  # Reshape_34__43
            %"_val_6__43"<?,?> ⬅️ ::Reshape(%"_val_4__43", %"_val_5__43") {allowzero=0}
     497 |  # Constant_35__43
            %"_val_7__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     498 |  # Cast_36__43
            %"_val_8__43"<?,?> ⬅️ ::Cast(%"_val_7__43") {to=7}
     499 |  # Constant_37__43
            %"_val_9__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     500 |  # Reshape_38__43
            %"_val_10__43"<?,?> ⬅️ ::Reshape(%"_val_8__43", %"_val_9__43") {allowzero=0}
     501 |  # Constant_39__43
            %"_val_11__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     502 |  # Cast_40__43
            %"_val_12__43"<?,?> ⬅️ ::Cast(%"_val_11__43") {to=7}
     503 |  # Constant_41__43
            %"_val_13__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     504 |  # Reshape_42__43
            %"_val_14__43"<?,?> ⬅️ ::Reshape(%"_val_12__43", %"_val_13__43") {allowzero=0}
     505 |  # Constant_43__43
            %"_val_15__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     506 |  # Cast_44__43
            %"_val_16__43"<?,?> ⬅️ ::Cast(%"_val_15__43") {to=7}
     507 |  # Constant_45__43
            %"_val_17__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     508 |  # Reshape_46__43
            %"_val_18__43"<?,?> ⬅️ ::Reshape(%"_val_16__43", %"_val_17__43") {allowzero=0}
     509 |  # Slice_47__43
            %"slice_21__43"<?,?> ⬅️ ::Slice(%"unsqueeze_6__43", %"_val_6__43", %"_val_10__43", %"_val_14__43", %"_val_18__43")
     510 |  # n0__45
            %"dim__45"<?,?> ⬅️ ::Constant() {value_int=2}
     511 |  # n1__45
            %"dim_0__45"<?,?> ⬅️ ::Cast(%"dim__45") {to=7}
     512 |  # n2__45
            %"unsqueeze_7__43"<?,?> ⬅️ ::Unsqueeze(%"slice_21__43", %"dim_0__45")
     513 |  # n0__46
            %"shape__46"<?,?> ⬅️ ::Shape(%"l_position_ids_")
     514 |  # n1__46
            %"dim__46"<?,?> ⬅️ ::Constant() {value_int=0}
     515 |  # n2__46
            %"tmp__46"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     516 |  # n3__46
            %"start__46"<?,?> ⬅️ ::Reshape(%"dim__46", %"tmp__46")
     517 |  # n4__46
            %"dim_0__46"<?,?> ⬅️ ::Constant() {value_int=0}
     518 |  # n5__46
            %"int64_1__46"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     519 |  # n6__46
            %"tmp_1__46"<?,?> ⬅️ ::Add(%"dim_0__46", %"int64_1__46")
     520 |  # n7__46
            %"tmp_2__46"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     521 |  # n8__46
            %"end__46"<?,?> ⬅️ ::Reshape(%"tmp_1__46", %"tmp_2__46")
     522 |  # n9__46
            %"model_layers_0_1__1"<?,?> ⬅️ ::Slice(%"shape__46", %"start__46", %"end__46")
     523 |  # Constant_50__43
            %"_val_22__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     524 |  # Reshape_51__43
            %"_val_23__43"<?,?> ⬅️ ::Reshape(%"model_layers_0_1__1", %"_val_22__43") {allowzero=0}
     525 |  # Constant_52__43
            %"_val_24__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     526 |  # Constant_53__43
            %"_val_25__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     527 |  # Concat_54__43
            %"_val_26__43"<?,?> ⬅️ ::Concat(%"_val_23__43", %"_val_24__43", %"_val_25__43") {axis=0}
     528 |  # n0__47
            %"size_0__47"<?,?> ⬅️ ::Cast(%"_val_26__43") {to=7}
     529 |  # n1__47
            %"size_1__47"<?,?> ⬅️ ::Abs(%"size_0__47")
     530 |  # n2__47
            %"expand_2__43"<?,?> ⬅️ ::Expand(%"unsqueeze_7__43", %"size_1__47")
     531 |  # Constant_56__43
            %"_val_28__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     532 |  # Cast_57__43
            %"_val_29__43"<?,?> ⬅️ ::Cast(%"_val_28__43") {to=7}
     533 |  # Constant_58__43
            %"_val_30__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     534 |  # Reshape_59__43
            %"_val_31__43"<?,?> ⬅️ ::Reshape(%"_val_29__43", %"_val_30__43") {allowzero=0}
     535 |  # Constant_60__43
            %"_val_32__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     536 |  # Cast_61__43
            %"_val_33__43"<?,?> ⬅️ ::Cast(%"_val_32__43") {to=7}
     537 |  # Constant_62__43
            %"_val_34__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     538 |  # Reshape_63__43
            %"_val_35__43"<?,?> ⬅️ ::Reshape(%"_val_33__43", %"_val_34__43") {allowzero=0}
     539 |  # Constant_64__43
            %"_val_36__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     540 |  # Cast_65__43
            %"_val_37__43"<?,?> ⬅️ ::Cast(%"_val_36__43") {to=7}
     541 |  # Constant_66__43
            %"_val_38__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     542 |  # Reshape_67__43
            %"_val_39__43"<?,?> ⬅️ ::Reshape(%"_val_37__43", %"_val_38__43") {allowzero=0}
     543 |  # Constant_68__43
            %"_val_40__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     544 |  # Cast_69__43
            %"_val_41__43"<?,?> ⬅️ ::Cast(%"_val_40__43") {to=7}
     545 |  # Constant_70__43
            %"_val_42__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     546 |  # Reshape_71__43
            %"_val_43__43"<?,?> ⬅️ ::Reshape(%"_val_41__43", %"_val_42__43") {allowzero=0}
     547 |  # Slice_72__43
            %"slice_22__43"<?,?> ⬅️ ::Slice(%"l_position_ids_", %"_val_31__43", %"_val_35__43", %"_val_39__43", %"_val_43__43")
     548 |  # n0__48
            %"dim__48"<?,?> ⬅️ ::Constant() {value_int=1}
     549 |  # n1__48
            %"dim_0__48"<?,?> ⬅️ ::Cast(%"dim__48") {to=7}
     550 |  # n2__48
            %"unsqueeze_8__43"<?,?> ⬅️ ::Unsqueeze(%"slice_22__43", %"dim_0__48")
     551 |  # Constant_74__43
            %"_val_46__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     552 |  # Cast_75__43
            %"_val_47__43"<?,?> ⬅️ ::Cast(%"_val_46__43") {to=7}
     553 |  # Constant_76__43
            %"_val_48__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     554 |  # Reshape_77__43
            %"_val_49__43"<?,?> ⬅️ ::Reshape(%"_val_47__43", %"_val_48__43") {allowzero=0}
     555 |  # Constant_78__43
            %"_val_50__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     556 |  # Cast_79__43
            %"_val_51__43"<?,?> ⬅️ ::Cast(%"_val_50__43") {to=7}
     557 |  # Constant_80__43
            %"_val_52__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     558 |  # Reshape_81__43
            %"_val_53__43"<?,?> ⬅️ ::Reshape(%"_val_51__43", %"_val_52__43") {allowzero=0}
     559 |  # Constant_82__43
            %"_val_54__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     560 |  # Cast_83__43
            %"_val_55__43"<?,?> ⬅️ ::Cast(%"_val_54__43") {to=7}
     561 |  # Constant_84__43
            %"_val_56__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     562 |  # Reshape_85__43
            %"_val_57__43"<?,?> ⬅️ ::Reshape(%"_val_55__43", %"_val_56__43") {allowzero=0}
     563 |  # Constant_86__43
            %"_val_58__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     564 |  # Cast_87__43
            %"_val_59__43"<?,?> ⬅️ ::Cast(%"_val_58__43") {to=7}
     565 |  # Constant_88__43
            %"_val_60__43"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     566 |  # Reshape_89__43
            %"_val_61__43"<?,?> ⬅️ ::Reshape(%"_val_59__43", %"_val_60__43") {allowzero=0}
     567 |  # Slice_90__43
            %"slice_23__43"<?,?> ⬅️ ::Slice(%"unsqueeze_8__43", %"_val_49__43", %"_val_53__43", %"_val_57__43", %"_val_61__43")
     568 |  # Cast_91__43
            %"_to_copy__43"<?,?> ⬅️ ::Cast(%"slice_23__43") {to=1}
     569 |  # Constant_92__43
            %"_val_64__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     570 |  # Reshape_93__43
            %"_val_65__43"<?,?> ⬅️ ::Reshape(%"model_layers_0_1__1", %"_val_64__43") {allowzero=0}
     571 |  # Constant_94__43
            %"_val_66__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     572 |  # Constant_95__43
            %"_val_67__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     573 |  # Concat_96__43
            %"_val_68__43"<?,?> ⬅️ ::Concat(%"_val_65__43", %"_val_66__43", %"_val_67__43") {axis=0}
     574 |  # n0__49
            %"size_0__49"<?,?> ⬅️ ::Cast(%"_val_68__43") {to=7}
     575 |  # n1__49
            %"size_1__49"<?,?> ⬅️ ::Abs(%"size_0__49")
     576 |  # n2__49
            %"expand_3__43"<?,?> ⬅️ ::Expand(%"expand_2__43", %"size_1__49")
     577 |  # Constant_98__43
            %"_val_70__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     578 |  # Reshape_99__43
            %"_val_71__43"<?,?> ⬅️ ::Reshape(%"model_layers_0_1__1", %"_val_70__43") {allowzero=0}
     579 |  # Constant_100__43
            %"_val_72__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     580 |  # Constant_101__43
            %"_val_73__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     581 |  # Concat_102__43
            %"_val_74__43"<?,?> ⬅️ ::Concat(%"_val_71__43", %"_val_72__43", %"_val_73__43") {axis=0}
     582 |  # n0__50
            %"size_0__50"<?,?> ⬅️ ::Cast(%"_val_74__43") {to=7}
     583 |  # n1__50
            %"view_10__43"<?,?> ⬅️ ::Reshape(%"expand_3__43", %"size_0__50")
     584 |  # n0__51
            %"shape__51"<?,?> ⬅️ ::Shape(%"slice_23__43")
     585 |  # n1__51
            %"dim__51"<?,?> ⬅️ ::Constant() {value_int=2}
     586 |  # n2__51
            %"tmp__51"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     587 |  # n3__51
            %"start__51"<?,?> ⬅️ ::Reshape(%"dim__51", %"tmp__51")
     588 |  # n4__51
            %"dim_0__51"<?,?> ⬅️ ::Constant() {value_int=2}
     589 |  # n5__51
            %"int64_1__51"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     590 |  # n6__51
            %"tmp_1__51"<?,?> ⬅️ ::Add(%"dim_0__51", %"int64_1__51")
     591 |  # n7__51
            %"tmp_2__51"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     592 |  # n8__51
            %"end__51"<?,?> ⬅️ ::Reshape(%"tmp_1__51", %"tmp_2__51")
     593 |  # n9__51
            %"sym_size_int_4__43"<?,?> ⬅️ ::Slice(%"shape__51", %"start__51", %"end__51")
     594 |  # Constant_105__43
            %"_val_77__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     595 |  # Reshape_106__43
            %"_val_78__43"<?,?> ⬅️ ::Reshape(%"model_layers_0_1__1", %"_val_77__43") {allowzero=0}
     596 |  # Constant_107__43
            %"_val_79__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     597 |  # Reshape_108__43
            %"_val_80__43"<?,?> ⬅️ ::Reshape(%"sym_size_int_4__43", %"_val_79__43") {allowzero=0}
     598 |  # Constant_109__43
            %"_val_81__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     599 |  # Concat_110__43
            %"_val_82__43"<?,?> ⬅️ ::Concat(%"_val_78__43", %"_val_81__43", %"_val_80__43") {axis=0}
     600 |  # n0__52
            %"size_0__52"<?,?> ⬅️ ::Cast(%"_val_82__43") {to=7}
     601 |  # n1__52
            %"size_1__52"<?,?> ⬅️ ::Abs(%"size_0__52")
     602 |  # n2__52
            %"expand_4__43"<?,?> ⬅️ ::Expand(%"_to_copy__43", %"size_1__52")
     603 |  # Constant_112__43
            %"_val_84__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     604 |  # Reshape_113__43
            %"_val_85__43"<?,?> ⬅️ ::Reshape(%"model_layers_0_1__1", %"_val_84__43") {allowzero=0}
     605 |  # Constant_114__43
            %"_val_86__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     606 |  # Reshape_115__43
            %"_val_87__43"<?,?> ⬅️ ::Reshape(%"sym_size_int_4__43", %"_val_86__43") {allowzero=0}
     607 |  # Constant_116__43
            %"_val_88__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     608 |  # Concat_117__43
            %"_val_89__43"<?,?> ⬅️ ::Concat(%"_val_85__43", %"_val_88__43", %"_val_87__43") {axis=0}
     609 |  # n0__53
            %"size_0__53"<?,?> ⬅️ ::Cast(%"_val_89__43") {to=7}
     610 |  # n1__53
            %"view_11__43"<?,?> ⬅️ ::Reshape(%"expand_4__43", %"size_0__53")
     611 |  # n0__54
            %"bmm__43"<?,?> ⬅️ ::MatMul(%"view_10__43", %"view_11__43")
     612 |  # Constant_120__43
            %"_val_92__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     613 |  # Reshape_121__43
            %"_val_93__43"<?,?> ⬅️ ::Reshape(%"model_layers_0_1__1", %"_val_92__43") {allowzero=0}
     614 |  # Constant_122__43
            %"_val_94__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     615 |  # Reshape_123__43
            %"_val_95__43"<?,?> ⬅️ ::Reshape(%"sym_size_int_4__43", %"_val_94__43") {allowzero=0}
     616 |  # Constant_124__43
            %"_val_96__43"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     617 |  # Concat_125__43
            %"_val_97__43"<?,?> ⬅️ ::Concat(%"_val_93__43", %"_val_96__43", %"_val_95__43") {axis=0}
     618 |  # n0__55
            %"size_0__55"<?,?> ⬅️ ::Cast(%"_val_97__43") {to=7}
     619 |  # n1__55
            %"view_12__43"<?,?> ⬅️ ::Reshape(%"bmm__43", %"size_0__55")
     620 |  # Transpose_127__43
            %"transpose_3__43"<?,?> ⬅️ ::Transpose(%"view_12__43") {perm=[0, 2, 1]}
     621 |  # SequenceConstruct_128__43
            %"100__43"<?,?> ⬅️ ::SequenceConstruct(%"transpose_3__43", %"transpose_3__43")
     622 |  # n0__56
            %"cat__43"<?,?> ⬅️ ::ConcatFromSequence(%"100__43") {axis=-1}
     623 |  # n0__57
            %"model_layers_0_self_attn_rotary_emb_1_1__24"<?,?> ⬅️ ::Cos(%"cat__43")
     624 |  # n0__58
            %"model_layers_0_self_attn_rotary_emb_1_2__24"<?,?> ⬅️ ::Sin(%"cat__43")
     625 |  # n0__59
            %"dim__59"<?,?> ⬅️ ::Constant() {value_int=1}
     626 |  # n1__59
            %"dim_0__59"<?,?> ⬅️ ::Cast(%"dim__59") {to=7}
     627 |  # n2__59
            %"unsqueeze_9__24"<?,?> ⬅️ ::Unsqueeze(%"model_layers_0_self_attn_rotary_emb_1_1__24", %"dim_0__59")
     628 |  # n0__60
            %"dim__60"<?,?> ⬅️ ::Constant() {value_int=1}
     629 |  # n1__60
            %"dim_0__60"<?,?> ⬅️ ::Cast(%"dim__60") {to=7}
     630 |  # n2__60
            %"unsqueeze_10__24"<?,?> ⬅️ ::Unsqueeze(%"model_layers_0_self_attn_rotary_emb_1_2__24", %"dim_0__60")
     631 |  # n0__61
            %"mul_6__24"<?,?> ⬅️ ::Mul(%"transpose__24", %"unsqueeze_9__24")
     632 |  # Constant_116__24
            %"_val_47__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     633 |  # Cast_117__24
            %"_val_48__24"<?,?> ⬅️ ::Cast(%"_val_47__24") {to=7}
     634 |  # Constant_118__24
            %"_val_49__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     635 |  # Reshape_119__24
            %"_val_50__24"<?,?> ⬅️ ::Reshape(%"_val_48__24", %"_val_49__24") {allowzero=0}
     636 |  # Constant_120__24
            %"_val_51__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     637 |  # Cast_121__24
            %"_val_52__24"<?,?> ⬅️ ::Cast(%"_val_51__24") {to=7}
     638 |  # Constant_122__24
            %"_val_53__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     639 |  # Reshape_123__24
            %"_val_54__24"<?,?> ⬅️ ::Reshape(%"_val_52__24", %"_val_53__24") {allowzero=0}
     640 |  # Constant_124__24
            %"_val_55__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     641 |  # Cast_125__24
            %"_val_56__24"<?,?> ⬅️ ::Cast(%"_val_55__24") {to=7}
     642 |  # Constant_126__24
            %"_val_57__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     643 |  # Reshape_127__24
            %"_val_58__24"<?,?> ⬅️ ::Reshape(%"_val_56__24", %"_val_57__24") {allowzero=0}
     644 |  # Constant_128__24
            %"_val_59__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     645 |  # Cast_129__24
            %"_val_60__24"<?,?> ⬅️ ::Cast(%"_val_59__24") {to=7}
     646 |  # Constant_130__24
            %"_val_61__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     647 |  # Reshape_131__24
            %"_val_62__24"<?,?> ⬅️ ::Reshape(%"_val_60__24", %"_val_61__24") {allowzero=0}
     648 |  # Slice_132__24
            %"slice_24__24"<?,?> ⬅️ ::Slice(%"transpose__24", %"_val_50__24", %"_val_54__24", %"_val_58__24", %"_val_62__24")
     649 |  # Constant_133__24
            %"_val_64__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     650 |  # Cast_134__24
            %"_val_65__24"<?,?> ⬅️ ::Cast(%"_val_64__24") {to=7}
     651 |  # Constant_135__24
            %"_val_66__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     652 |  # Reshape_136__24
            %"_val_67__24"<?,?> ⬅️ ::Reshape(%"_val_65__24", %"_val_66__24") {allowzero=0}
     653 |  # Constant_137__24
            %"_val_68__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     654 |  # Cast_138__24
            %"_val_69__24"<?,?> ⬅️ ::Cast(%"_val_68__24") {to=7}
     655 |  # Constant_139__24
            %"_val_70__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     656 |  # Reshape_140__24
            %"_val_71__24"<?,?> ⬅️ ::Reshape(%"_val_69__24", %"_val_70__24") {allowzero=0}
     657 |  # Constant_141__24
            %"_val_72__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     658 |  # Cast_142__24
            %"_val_73__24"<?,?> ⬅️ ::Cast(%"_val_72__24") {to=7}
     659 |  # Constant_143__24
            %"_val_74__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     660 |  # Reshape_144__24
            %"_val_75__24"<?,?> ⬅️ ::Reshape(%"_val_73__24", %"_val_74__24") {allowzero=0}
     661 |  # Constant_145__24
            %"_val_76__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     662 |  # Cast_146__24
            %"_val_77__24"<?,?> ⬅️ ::Cast(%"_val_76__24") {to=7}
     663 |  # Constant_147__24
            %"_val_78__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     664 |  # Reshape_148__24
            %"_val_79__24"<?,?> ⬅️ ::Reshape(%"_val_77__24", %"_val_78__24") {allowzero=0}
     665 |  # Slice_149__24
            %"slice_25__24"<?,?> ⬅️ ::Slice(%"transpose__24", %"_val_67__24", %"_val_71__24", %"_val_75__24", %"_val_79__24")
     666 |  # n0__62
            %"neg__24"<?,?> ⬅️ ::Neg(%"slice_25__24")
     667 |  # SequenceConstruct_151__24
            %"82__24"<?,?> ⬅️ ::SequenceConstruct(%"neg__24", %"slice_24__24")
     668 |  # n0__63
            %"cat_1__24"<?,?> ⬅️ ::ConcatFromSequence(%"82__24") {axis=-1}
     669 |  # n0__64
            %"mul_7__24"<?,?> ⬅️ ::Mul(%"cat_1__24", %"unsqueeze_10__24")
     670 |  # n0__65
            %"alpha__65"<?,?> ⬅️ ::Constant() {value_float=1.0}
     671 |  # n1__65
            %"alpha_0__65"<?,?> ⬅️ ::CastLike(%"alpha__65", %"mul_7__24")
     672 |  # n2__65
            %"other_1__65"<?,?> ⬅️ ::Mul(%"mul_7__24", %"alpha_0__65")
     673 |  # n3__65
            %"add_3__24"<?,?> ⬅️ ::Add(%"mul_6__24", %"other_1__65")
     674 |  # n0__66
            %"mul_8__24"<?,?> ⬅️ ::Mul(%"transpose_1__24", %"unsqueeze_9__24")
     675 |  # Constant_156__24
            %"_val_87__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     676 |  # Cast_157__24
            %"_val_88__24"<?,?> ⬅️ ::Cast(%"_val_87__24") {to=7}
     677 |  # Constant_158__24
            %"_val_89__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     678 |  # Reshape_159__24
            %"_val_90__24"<?,?> ⬅️ ::Reshape(%"_val_88__24", %"_val_89__24") {allowzero=0}
     679 |  # Constant_160__24
            %"_val_91__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     680 |  # Cast_161__24
            %"_val_92__24"<?,?> ⬅️ ::Cast(%"_val_91__24") {to=7}
     681 |  # Constant_162__24
            %"_val_93__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     682 |  # Reshape_163__24
            %"_val_94__24"<?,?> ⬅️ ::Reshape(%"_val_92__24", %"_val_93__24") {allowzero=0}
     683 |  # Constant_164__24
            %"_val_95__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     684 |  # Cast_165__24
            %"_val_96__24"<?,?> ⬅️ ::Cast(%"_val_95__24") {to=7}
     685 |  # Constant_166__24
            %"_val_97__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     686 |  # Reshape_167__24
            %"_val_98__24"<?,?> ⬅️ ::Reshape(%"_val_96__24", %"_val_97__24") {allowzero=0}
     687 |  # Constant_168__24
            %"_val_99__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     688 |  # Cast_169__24
            %"_val_100__24"<?,?> ⬅️ ::Cast(%"_val_99__24") {to=7}
     689 |  # Constant_170__24
            %"_val_101__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     690 |  # Reshape_171__24
            %"_val_102__24"<?,?> ⬅️ ::Reshape(%"_val_100__24", %"_val_101__24") {allowzero=0}
     691 |  # Slice_172__24
            %"slice_26__24"<?,?> ⬅️ ::Slice(%"transpose_1__24", %"_val_90__24", %"_val_94__24", %"_val_98__24", %"_val_102__24")
     692 |  # Constant_173__24
            %"_val_104__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     693 |  # Cast_174__24
            %"_val_105__24"<?,?> ⬅️ ::Cast(%"_val_104__24") {to=7}
     694 |  # Constant_175__24
            %"_val_106__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     695 |  # Reshape_176__24
            %"_val_107__24"<?,?> ⬅️ ::Reshape(%"_val_105__24", %"_val_106__24") {allowzero=0}
     696 |  # Constant_177__24
            %"_val_108__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     697 |  # Cast_178__24
            %"_val_109__24"<?,?> ⬅️ ::Cast(%"_val_108__24") {to=7}
     698 |  # Constant_179__24
            %"_val_110__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     699 |  # Reshape_180__24
            %"_val_111__24"<?,?> ⬅️ ::Reshape(%"_val_109__24", %"_val_110__24") {allowzero=0}
     700 |  # Constant_181__24
            %"_val_112__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     701 |  # Cast_182__24
            %"_val_113__24"<?,?> ⬅️ ::Cast(%"_val_112__24") {to=7}
     702 |  # Constant_183__24
            %"_val_114__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     703 |  # Reshape_184__24
            %"_val_115__24"<?,?> ⬅️ ::Reshape(%"_val_113__24", %"_val_114__24") {allowzero=0}
     704 |  # Constant_185__24
            %"_val_116__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     705 |  # Cast_186__24
            %"_val_117__24"<?,?> ⬅️ ::Cast(%"_val_116__24") {to=7}
     706 |  # Constant_187__24
            %"_val_118__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     707 |  # Reshape_188__24
            %"_val_119__24"<?,?> ⬅️ ::Reshape(%"_val_117__24", %"_val_118__24") {allowzero=0}
     708 |  # Slice_189__24
            %"slice_27__24"<?,?> ⬅️ ::Slice(%"transpose_1__24", %"_val_107__24", %"_val_111__24", %"_val_115__24", %"_val_119__24")
     709 |  # n0__67
            %"neg_1__24"<?,?> ⬅️ ::Neg(%"slice_27__24")
     710 |  # SequenceConstruct_191__24
            %"122__24"<?,?> ⬅️ ::SequenceConstruct(%"neg_1__24", %"slice_26__24")
     711 |  # n0__68
            %"cat_2__24"<?,?> ⬅️ ::ConcatFromSequence(%"122__24") {axis=-1}
     712 |  # n0__69
            %"mul_9__24"<?,?> ⬅️ ::Mul(%"cat_2__24", %"unsqueeze_10__24")
     713 |  # n0__70
            %"alpha__70"<?,?> ⬅️ ::Constant() {value_float=1.0}
     714 |  # n1__70
            %"alpha_0__70"<?,?> ⬅️ ::CastLike(%"alpha__70", %"mul_9__24")
     715 |  # n2__70
            %"other_1__70"<?,?> ⬅️ ::Mul(%"mul_9__24", %"alpha_0__70")
     716 |  # n3__70
            %"add_4__24"<?,?> ⬅️ ::Add(%"mul_8__24", %"other_1__70")
     717 |  # SequenceConstruct_195__24
            %"126__24"<?,?> ⬅️ ::SequenceConstruct(%"key_states", %"add_4__24")
     718 |  # n0__71
            %"model_1_2"<FLOAT,[s0,32,s1,128]> ⬅️ ::ConcatFromSequence(%"126__24") {axis=-2}
     719 |  # SequenceConstruct_197__24
            %"128__24"<?,?> ⬅️ ::SequenceConstruct(%"value_states", %"transpose_2__24")
     720 |  # n0__72
            %"model_1_3"<FLOAT,[s0,32,s1,128]> ⬅️ ::ConcatFromSequence(%"128__24") {axis=-2}
     721 |  # Transpose_199__24
            %"transpose_4__24"<?,?> ⬅️ ::Transpose(%"model_1_2") {perm=[0, 1, 3, 2]}
     722 |  # n0__73
            %"shape__73"<?,?> ⬅️ ::Shape(%"model_layers_0_self_attn_q_proj_1__24")
     723 |  # n1__73
            %"dim__73"<?,?> ⬅️ ::Constant() {value_int=1}
     724 |  # n2__73
            %"tmp__73"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     725 |  # n3__73
            %"start__73"<?,?> ⬅️ ::Reshape(%"dim__73", %"tmp__73")
     726 |  # n4__73
            %"dim_0__73"<?,?> ⬅️ ::Constant() {value_int=1}
     727 |  # n5__73
            %"int64_1__73"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     728 |  # n6__73
            %"tmp_1__73"<?,?> ⬅️ ::Add(%"dim_0__73", %"int64_1__73")
     729 |  # n7__73
            %"tmp_2__73"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     730 |  # n8__73
            %"end__73"<?,?> ⬅️ ::Reshape(%"tmp_1__73", %"tmp_2__73")
     731 |  # n9__73
            %"sym_size_int_5__24"<?,?> ⬅️ ::Slice(%"shape__73", %"start__73", %"end__73")
     732 |  # Constant_201__24
            %"_val_132__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     733 |  # Reshape_202__24
            %"_val_133__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_132__24") {allowzero=0}
     734 |  # Constant_203__24
            %"_val_134__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     735 |  # Reshape_204__24
            %"_val_135__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_5__24", %"_val_134__24") {allowzero=0}
     736 |  # Constant_205__24
            %"_val_136__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     737 |  # Constant_206__24
            %"_val_137__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     738 |  # Concat_207__24
            %"_val_138__24"<?,?> ⬅️ ::Concat(%"_val_133__24", %"_val_136__24", %"_val_135__24", %"_val_137__24") {axis=0}
     739 |  # n0__74
            %"size_0__74"<?,?> ⬅️ ::Cast(%"_val_138__24") {to=7}
     740 |  # n1__74
            %"size_1__74"<?,?> ⬅️ ::Abs(%"size_0__74")
     741 |  # n2__74
            %"expand_5__24"<?,?> ⬅️ ::Expand(%"add_3__24", %"size_1__74")
     742 |  # n0__75
            %"clone_1__24"<?,?> ⬅️ ::Identity(%"expand_5__24")
     743 |  # Constant_210__24
            %"_val_141__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     744 |  # n0__76
            %"mul_10__24"<?,?> ⬅️ ::Mul(%"model_1_1", %"_val_141__24")
     745 |  # Constant_212__24
            %"_val_143__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     746 |  # Reshape_213__24
            %"_val_144__24"<?,?> ⬅️ ::Reshape(%"mul_10__24", %"_val_143__24") {allowzero=0}
     747 |  # Constant_214__24
            %"_val_145__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     748 |  # Reshape_215__24
            %"_val_146__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_5__24", %"_val_145__24") {allowzero=0}
     749 |  # Constant_216__24
            %"_val_147__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     750 |  # Concat_217__24
            %"_val_148__24"<?,?> ⬅️ ::Concat(%"_val_144__24", %"_val_146__24", %"_val_147__24") {axis=0}
     751 |  # n0__77
            %"size_0__77"<?,?> ⬅️ ::Cast(%"_val_148__24") {to=7}
     752 |  # n1__77
            %"view_13__24"<?,?> ⬅️ ::Reshape(%"clone_1__24", %"size_0__77")
     753 |  # n0__78
            %"shape__78"<?,?> ⬅️ ::Shape(%"model_1_2")
     754 |  # n1__78
            %"dim__78"<?,?> ⬅️ ::Constant() {value_int=2}
     755 |  # n2__78
            %"tmp__78"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     756 |  # n3__78
            %"start__78"<?,?> ⬅️ ::Reshape(%"dim__78", %"tmp__78")
     757 |  # n4__78
            %"dim_0__78"<?,?> ⬅️ ::Constant() {value_int=2}
     758 |  # n5__78
            %"int64_1__78"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     759 |  # n6__78
            %"tmp_1__78"<?,?> ⬅️ ::Add(%"dim_0__78", %"int64_1__78")
     760 |  # n7__78
            %"tmp_2__78"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     761 |  # n8__78
            %"end__78"<?,?> ⬅️ ::Reshape(%"tmp_1__78", %"tmp_2__78")
     762 |  # n9__78
            %"sym_size_int_6__24"<?,?> ⬅️ ::Slice(%"shape__78", %"start__78", %"end__78")
     763 |  # Constant_220__24
            %"_val_151__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     764 |  # Reshape_221__24
            %"_val_152__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_151__24") {allowzero=0}
     765 |  # Constant_222__24
            %"_val_153__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     766 |  # Reshape_223__24
            %"_val_154__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_6__24", %"_val_153__24") {allowzero=0}
     767 |  # Constant_224__24
            %"_val_155__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     768 |  # Constant_225__24
            %"_val_156__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     769 |  # Concat_226__24
            %"_val_157__24"<?,?> ⬅️ ::Concat(%"_val_152__24", %"_val_155__24", %"_val_156__24", %"_val_154__24") {axis=0}
     770 |  # n0__79
            %"size_0__79"<?,?> ⬅️ ::Cast(%"_val_157__24") {to=7}
     771 |  # n1__79
            %"size_1__79"<?,?> ⬅️ ::Abs(%"size_0__79")
     772 |  # n2__79
            %"expand_6__24"<?,?> ⬅️ ::Expand(%"transpose_4__24", %"size_1__79")
     773 |  # Constant_228__24
            %"_val_159__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     774 |  # Reshape_229__24
            %"_val_160__24"<?,?> ⬅️ ::Reshape(%"mul_10__24", %"_val_159__24") {allowzero=0}
     775 |  # Constant_230__24
            %"_val_161__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     776 |  # Reshape_231__24
            %"_val_162__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_6__24", %"_val_161__24") {allowzero=0}
     777 |  # Constant_232__24
            %"_val_163__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     778 |  # Concat_233__24
            %"_val_164__24"<?,?> ⬅️ ::Concat(%"_val_160__24", %"_val_163__24", %"_val_162__24") {axis=0}
     779 |  # n0__80
            %"size_0__80"<?,?> ⬅️ ::Cast(%"_val_164__24") {to=7}
     780 |  # n1__80
            %"view_14__24"<?,?> ⬅️ ::Reshape(%"expand_6__24", %"size_0__80")
     781 |  # n0__81
            %"bmm_1__24"<?,?> ⬅️ ::MatMul(%"view_13__24", %"view_14__24")
     782 |  # Constant_236__24
            %"_val_167__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     783 |  # Reshape_237__24
            %"_val_168__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_167__24") {allowzero=0}
     784 |  # Constant_238__24
            %"_val_169__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     785 |  # Reshape_239__24
            %"_val_170__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_5__24", %"_val_169__24") {allowzero=0}
     786 |  # Constant_240__24
            %"_val_171__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     787 |  # Reshape_241__24
            %"_val_172__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_6__24", %"_val_171__24") {allowzero=0}
     788 |  # Constant_242__24
            %"_val_173__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     789 |  # Concat_243__24
            %"_val_174__24"<?,?> ⬅️ ::Concat(%"_val_168__24", %"_val_173__24", %"_val_170__24", %"_val_172__24") {axis=0}
     790 |  # n0__82
            %"size_0__82"<?,?> ⬅️ ::Cast(%"_val_174__24") {to=7}
     791 |  # n1__82
            %"view_15__24"<?,?> ⬅️ ::Reshape(%"bmm_1__24", %"size_0__82")
     792 |  # Constant_245__24
            %"_val_176__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='')}
     793 |  # n0__83
            %"div__24"<?,?> ⬅️ ::Div(%"view_15__24", %"_val_176__24")
     794 |  # Constant_247__24
            %"_val_178__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     795 |  # Cast_248__24
            %"_val_179__24"<?,?> ⬅️ ::Cast(%"_val_178__24") {to=7}
     796 |  # Constant_249__24
            %"_val_180__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     797 |  # Reshape_250__24
            %"_val_181__24"<?,?> ⬅️ ::Reshape(%"_val_179__24", %"_val_180__24") {allowzero=0}
     798 |  # Constant_251__24
            %"_val_182__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     799 |  # Cast_252__24
            %"_val_183__24"<?,?> ⬅️ ::Cast(%"_val_182__24") {to=7}
     800 |  # Constant_253__24
            %"_val_184__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     801 |  # Reshape_254__24
            %"_val_185__24"<?,?> ⬅️ ::Reshape(%"_val_183__24", %"_val_184__24") {allowzero=0}
     802 |  # Constant_255__24
            %"_val_186__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     803 |  # Cast_256__24
            %"_val_187__24"<?,?> ⬅️ ::Cast(%"_val_186__24") {to=7}
     804 |  # Constant_257__24
            %"_val_188__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     805 |  # Reshape_258__24
            %"_val_189__24"<?,?> ⬅️ ::Reshape(%"_val_187__24", %"_val_188__24") {allowzero=0}
     806 |  # Constant_259__24
            %"_val_190__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     807 |  # Cast_260__24
            %"_val_191__24"<?,?> ⬅️ ::Cast(%"_val_190__24") {to=7}
     808 |  # Constant_261__24
            %"_val_192__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     809 |  # Reshape_262__24
            %"_val_193__24"<?,?> ⬅️ ::Reshape(%"_val_191__24", %"_val_192__24") {allowzero=0}
     810 |  # Slice_263__24
            %"slice_31__24"<?,?> ⬅️ ::Slice(%"slice_scatter_2__1", %"_val_181__24", %"_val_185__24", %"_val_189__24", %"_val_193__24")
     811 |  # Constant_264__24
            %"_val_195__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     812 |  # Cast_265__24
            %"_val_196__24"<?,?> ⬅️ ::Cast(%"_val_195__24") {to=7}
     813 |  # Constant_266__24
            %"_val_197__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     814 |  # Reshape_267__24
            %"_val_198__24"<?,?> ⬅️ ::Reshape(%"_val_196__24", %"_val_197__24") {allowzero=0}
     815 |  # Constant_268__24
            %"_val_199__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     816 |  # Cast_269__24
            %"_val_200__24"<?,?> ⬅️ ::Cast(%"_val_199__24") {to=7}
     817 |  # Constant_270__24
            %"_val_201__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     818 |  # Reshape_271__24
            %"_val_202__24"<?,?> ⬅️ ::Reshape(%"_val_200__24", %"_val_201__24") {allowzero=0}
     819 |  # Constant_272__24
            %"_val_203__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     820 |  # Cast_273__24
            %"_val_204__24"<?,?> ⬅️ ::Cast(%"_val_203__24") {to=7}
     821 |  # Constant_274__24
            %"_val_205__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     822 |  # Reshape_275__24
            %"_val_206__24"<?,?> ⬅️ ::Reshape(%"_val_204__24", %"_val_205__24") {allowzero=0}
     823 |  # Constant_276__24
            %"_val_207__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     824 |  # Cast_277__24
            %"_val_208__24"<?,?> ⬅️ ::Cast(%"_val_207__24") {to=7}
     825 |  # Constant_278__24
            %"_val_209__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     826 |  # Reshape_279__24
            %"_val_210__24"<?,?> ⬅️ ::Reshape(%"_val_208__24", %"_val_209__24") {allowzero=0}
     827 |  # Slice_280__24
            %"slice_32__24"<?,?> ⬅️ ::Slice(%"slice_31__24", %"_val_198__24", %"_val_202__24", %"_val_206__24", %"_val_210__24")
     828 |  # Constant_281__24
            %"_val_212__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     829 |  # Cast_282__24
            %"_val_213__24"<?,?> ⬅️ ::Cast(%"_val_212__24") {to=7}
     830 |  # Constant_283__24
            %"_val_214__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     831 |  # Reshape_284__24
            %"_val_215__24"<?,?> ⬅️ ::Reshape(%"_val_213__24", %"_val_214__24") {allowzero=0}
     832 |  # Constant_285__24
            %"_val_216__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     833 |  # Cast_286__24
            %"_val_217__24"<?,?> ⬅️ ::Cast(%"_val_216__24") {to=7}
     834 |  # Constant_287__24
            %"_val_218__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     835 |  # Reshape_288__24
            %"_val_219__24"<?,?> ⬅️ ::Reshape(%"_val_217__24", %"_val_218__24") {allowzero=0}
     836 |  # Constant_289__24
            %"_val_220__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     837 |  # Cast_290__24
            %"_val_221__24"<?,?> ⬅️ ::Cast(%"_val_220__24") {to=7}
     838 |  # Constant_291__24
            %"_val_222__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     839 |  # Reshape_292__24
            %"_val_223__24"<?,?> ⬅️ ::Reshape(%"_val_221__24", %"_val_222__24") {allowzero=0}
     840 |  # Constant_293__24
            %"_val_224__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     841 |  # Cast_294__24
            %"_val_225__24"<?,?> ⬅️ ::Cast(%"_val_224__24") {to=7}
     842 |  # Constant_295__24
            %"_val_226__24"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     843 |  # Reshape_296__24
            %"_val_227__24"<?,?> ⬅️ ::Reshape(%"_val_225__24", %"_val_226__24") {allowzero=0}
     844 |  # Slice_297__24
            %"slice_33__24"<?,?> ⬅️ ::Slice(%"slice_32__24", %"_val_215__24", %"_val_219__24", %"_val_223__24", %"_val_227__24")
     845 |  # n0__84
            %"alpha__84"<?,?> ⬅️ ::Constant() {value_float=1.0}
     846 |  # n1__84
            %"alpha_0__84"<?,?> ⬅️ ::CastLike(%"alpha__84", %"slice_33__24")
     847 |  # n2__84
            %"other_1__84"<?,?> ⬅️ ::Mul(%"slice_33__24", %"alpha_0__84")
     848 |  # n3__84
            %"add_5__24"<?,?> ⬅️ ::Add(%"div__24", %"other_1__84")
     849 |  # Softmax_299__24
            %"_softmax__24"<?,?> ⬅️ ::Softmax(%"add_5__24") {axis=-1}
     850 |  # n0__85
            %"clone_2__24"<?,?> ⬅️ ::Identity(%"_softmax__24")
     851 |  # Constant_301__24
            %"_val_232__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     852 |  # Reshape_302__24
            %"_val_233__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_232__24") {allowzero=0}
     853 |  # Constant_303__24
            %"_val_234__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     854 |  # Reshape_304__24
            %"_val_235__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_5__24", %"_val_234__24") {allowzero=0}
     855 |  # Constant_305__24
            %"_val_236__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     856 |  # Reshape_306__24
            %"_val_237__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_6__24", %"_val_236__24") {allowzero=0}
     857 |  # Constant_307__24
            %"_val_238__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     858 |  # Concat_308__24
            %"_val_239__24"<?,?> ⬅️ ::Concat(%"_val_233__24", %"_val_238__24", %"_val_235__24", %"_val_237__24") {axis=0}
     859 |  # n0__86
            %"size_0__86"<?,?> ⬅️ ::Cast(%"_val_239__24") {to=7}
     860 |  # n1__86
            %"size_1__86"<?,?> ⬅️ ::Abs(%"size_0__86")
     861 |  # n2__86
            %"expand_7__24"<?,?> ⬅️ ::Expand(%"clone_2__24", %"size_1__86")
     862 |  # Constant_310__24
            %"_val_241__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='')}
     863 |  # n0__87
            %"mul_11__24"<?,?> ⬅️ ::Mul(%"model_1_1", %"_val_241__24")
     864 |  # Constant_312__24
            %"_val_243__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     865 |  # Reshape_313__24
            %"_val_244__24"<?,?> ⬅️ ::Reshape(%"mul_11__24", %"_val_243__24") {allowzero=0}
     866 |  # Constant_314__24
            %"_val_245__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     867 |  # Reshape_315__24
            %"_val_246__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_5__24", %"_val_245__24") {allowzero=0}
     868 |  # Constant_316__24
            %"_val_247__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     869 |  # Reshape_317__24
            %"_val_248__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_6__24", %"_val_247__24") {allowzero=0}
     870 |  # Concat_318__24
            %"_val_249__24"<?,?> ⬅️ ::Concat(%"_val_244__24", %"_val_246__24", %"_val_248__24") {axis=0}
     871 |  # n0__88
            %"size_0__88"<?,?> ⬅️ ::Cast(%"_val_249__24") {to=7}
     872 |  # n1__88
            %"view_16__24"<?,?> ⬅️ ::Reshape(%"expand_7__24", %"size_0__88")
     873 |  # n0__89
            %"shape__89"<?,?> ⬅️ ::Shape(%"model_1_3")
     874 |  # n1__89
            %"dim__89"<?,?> ⬅️ ::Constant() {value_int=2}
     875 |  # n2__89
            %"tmp__89"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     876 |  # n3__89
            %"start__89"<?,?> ⬅️ ::Reshape(%"dim__89", %"tmp__89")
     877 |  # n4__89
            %"dim_0__89"<?,?> ⬅️ ::Constant() {value_int=2}
     878 |  # n5__89
            %"int64_1__89"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     879 |  # n6__89
            %"tmp_1__89"<?,?> ⬅️ ::Add(%"dim_0__89", %"int64_1__89")
     880 |  # n7__89
            %"tmp_2__89"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     881 |  # n8__89
            %"end__89"<?,?> ⬅️ ::Reshape(%"tmp_1__89", %"tmp_2__89")
     882 |  # n9__89
            %"sym_size_int_7__24"<?,?> ⬅️ ::Slice(%"shape__89", %"start__89", %"end__89")
     883 |  # Constant_321__24
            %"_val_252__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     884 |  # Reshape_322__24
            %"_val_253__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_252__24") {allowzero=0}
     885 |  # Constant_323__24
            %"_val_254__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     886 |  # Reshape_324__24
            %"_val_255__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_7__24", %"_val_254__24") {allowzero=0}
     887 |  # Constant_325__24
            %"_val_256__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     888 |  # Constant_326__24
            %"_val_257__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     889 |  # Concat_327__24
            %"_val_258__24"<?,?> ⬅️ ::Concat(%"_val_253__24", %"_val_256__24", %"_val_255__24", %"_val_257__24") {axis=0}
     890 |  # n0__90
            %"size_0__90"<?,?> ⬅️ ::Cast(%"_val_258__24") {to=7}
     891 |  # n1__90
            %"size_1__90"<?,?> ⬅️ ::Abs(%"size_0__90")
     892 |  # n2__90
            %"expand_8__24"<?,?> ⬅️ ::Expand(%"model_1_3", %"size_1__90")
     893 |  # Constant_329__24
            %"_val_260__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     894 |  # Reshape_330__24
            %"_val_261__24"<?,?> ⬅️ ::Reshape(%"mul_11__24", %"_val_260__24") {allowzero=0}
     895 |  # Constant_331__24
            %"_val_262__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     896 |  # Reshape_332__24
            %"_val_263__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_7__24", %"_val_262__24") {allowzero=0}
     897 |  # Constant_333__24
            %"_val_264__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     898 |  # Concat_334__24
            %"_val_265__24"<?,?> ⬅️ ::Concat(%"_val_261__24", %"_val_263__24", %"_val_264__24") {axis=0}
     899 |  # n0__91
            %"size_0__91"<?,?> ⬅️ ::Cast(%"_val_265__24") {to=7}
     900 |  # n1__91
            %"view_17__24"<?,?> ⬅️ ::Reshape(%"expand_8__24", %"size_0__91")
     901 |  # n0__92
            %"bmm_2__24"<?,?> ⬅️ ::MatMul(%"view_16__24", %"view_17__24")
     902 |  # Constant_337__24
            %"_val_268__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     903 |  # Reshape_338__24
            %"_val_269__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_268__24") {allowzero=0}
     904 |  # Constant_339__24
            %"_val_270__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     905 |  # Reshape_340__24
            %"_val_271__24"<?,?> ⬅️ ::Reshape(%"sym_size_int_5__24", %"_val_270__24") {allowzero=0}
     906 |  # Constant_341__24
            %"_val_272__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     907 |  # Constant_342__24
            %"_val_273__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     908 |  # Concat_343__24
            %"_val_274__24"<?,?> ⬅️ ::Concat(%"_val_269__24", %"_val_272__24", %"_val_271__24", %"_val_273__24") {axis=0}
     909 |  # n0__93
            %"size_0__93"<?,?> ⬅️ ::Cast(%"_val_274__24") {to=7}
     910 |  # n1__93
            %"view_18__24"<?,?> ⬅️ ::Reshape(%"bmm_2__24", %"size_0__93")
     911 |  # Transpose_345__24
            %"transpose_5__24"<?,?> ⬅️ ::Transpose(%"view_18__24") {perm=[0, 2, 1, 3]}
     912 |  # n0__94
            %"clone_3__24"<?,?> ⬅️ ::Identity(%"transpose_5__24")
     913 |  # Constant_347__24
            %"_val_278__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     914 |  # Reshape_348__24
            %"_val_279__24"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_278__24") {allowzero=0}
     915 |  # Constant_349__24
            %"_val_280__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     916 |  # Reshape_350__24
            %"_val_281__24"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_280__24") {allowzero=0}
     917 |  # Constant_351__24
            %"_val_282__24"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     918 |  # Concat_352__24
            %"_val_283__24"<?,?> ⬅️ ::Concat(%"_val_279__24", %"_val_281__24", %"_val_282__24") {axis=0}
     919 |  # n0__95
            %"size_0__95"<?,?> ⬅️ ::Cast(%"_val_283__24") {to=7}
     920 |  # n1__95
            %"view_19__24"<?,?> ⬅️ ::Reshape(%"clone_3__24", %"size_0__95")
     921 |  # Transpose_5__96
            %"t_3__96"<?,?> ⬅️ ::Transpose(%"model.layers.0.self_attn.o_proj.weight") {perm=[1, 0]}
     922 |  # n0__97
            %"mul_12__96"<?,?> ⬅️ ::Mul(%"model_1_1", %"sym_size_int_5__24")
     923 |  # Constant_7__96
            %"_val_6__96"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     924 |  # Reshape_8__96
            %"_val_7__96"<?,?> ⬅️ ::Reshape(%"mul_12__96", %"_val_6__96") {allowzero=0}
     925 |  # Constant_9__96
            %"_val_8__96"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     926 |  # Concat_10__96
            %"_val_9__96"<?,?> ⬅️ ::Concat(%"_val_7__96", %"_val_8__96") {axis=0}
     927 |  # n0__98
            %"size_0__98"<?,?> ⬅️ ::Cast(%"_val_9__96") {to=7}
     928 |  # n1__98
            %"view_20__96"<?,?> ⬅️ ::Reshape(%"view_19__24", %"size_0__98")
     929 |  # n0__99
            %"mm_3__96"<?,?> ⬅️ ::MatMul(%"view_20__96", %"t_3__96")
     930 |  # Constant_13__96
            %"_val_12__96"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     931 |  # Reshape_14__96
            %"_val_13__96"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_12__96") {allowzero=0}
     932 |  # Constant_15__96
            %"_val_14__96"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     933 |  # Reshape_16__96
            %"_val_15__96"<?,?> ⬅️ ::Reshape(%"sym_size_int_5__24", %"_val_14__96") {allowzero=0}
     934 |  # Constant_17__96
            %"_val_16__96"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
     935 |  # Concat_18__96
            %"_val_17__96"<?,?> ⬅️ ::Concat(%"_val_13__96", %"_val_15__96", %"_val_16__96") {axis=0}
     936 |  # n0__100
            %"size_0__100"<?,?> ⬅️ ::Cast(%"_val_17__96") {to=7}
     937 |  # n1__100
            %"model_layers_0_self_attn_1_3__22"<?,?> ⬅️ ::Reshape(%"mm_3__96", %"size_0__100")
     938 |  # n0__101
            %"alpha__101"<?,?> ⬅️ ::Constant() {value_float=1.0}
     939 |  # n1__101
            %"alpha_0__101"<?,?> ⬅️ ::CastLike(%"alpha__101", %"model_layers_0_self_attn_1_3__22")
     940 |  # n2__101
            %"other_1__101"<?,?> ⬅️ ::Mul(%"model_layers_0_self_attn_1_3__22", %"alpha_0__101")
     941 |  # n3__101
            %"add_6__22"<?,?> ⬅️ ::Add(%"model_embed_tokens_1__1", %"other_1__101")
     942 |  # n0__102
            %"model_layers_0_post_attention_layernorm_1__22"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_6__22", %"model.layers.0.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
     943 |  # n0__103
            %"gate_proj_weight_t__103"<?,?> ⬅️ ::Transpose(%"model.layers.0.mlp.gate_proj.weight") {perm=[1, 0]}
     944 |  # n1__103
            %"up_proj_weight_t__103"<?,?> ⬅️ ::Transpose(%"model.layers.0.mlp.up_proj.weight") {perm=[1, 0]}
     945 |  # n2__103
            %"down_proj_weight_t__103"<?,?> ⬅️ ::Transpose(%"model.layers.0.mlp.down_proj.weight") {perm=[1, 0]}
     946 |  # n3__103
            %"gate_proj_output__103"<?,?> ⬅️ ::MatMul(%"model_layers_0_post_attention_layernorm_1__22", %"gate_proj_weight_t__103")
     947 |  # n4__103
            %"up_proj_output__103"<?,?> ⬅️ ::MatMul(%"model_layers_0_post_attention_layernorm_1__22", %"up_proj_weight_t__103")
     948 |  # n5__103
            %"gate_times_up_output__103"<?,?> ⬅️ ::Mul(%"gate_proj_output__103", %"up_proj_output__103")
     949 |  # n6__103
            %"act_fn_output__103"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__103")
     950 |  # n7__103
            %"model_layers_0_mlp_1__22"<?,?> ⬅️ ::MatMul(%"act_fn_output__103", %"down_proj_weight_t__103")
     951 |  # n0__104
            %"alpha__104"<?,?> ⬅️ ::Constant() {value_float=1.0}
     952 |  # n1__104
            %"alpha_0__104"<?,?> ⬅️ ::CastLike(%"alpha__104", %"model_layers_0_mlp_1__22")
     953 |  # n2__104
            %"other_1__104"<?,?> ⬅️ ::Mul(%"model_layers_0_mlp_1__22", %"alpha_0__104")
     954 |  # n3__104
            %"model_layers_0_1_3__1"<?,?> ⬅️ ::Add(%"add_6__22", %"other_1__104")
     955 |  # n0__106
            %"model_layers_1_input_layernorm_1__105"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_0_1_3__1", %"model.layers.1.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
     956 |  # n0__107
            %"tmp__107"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
     957 |  # n1__107
            %"q__107"<?,?> ⬅️ ::MatMul(%"model_layers_1_input_layernorm_1__105", %"tmp__107")
     958 |  # n2__107
            %"tmp_0__107"<?,?> ⬅️ ::Transpose(%"model.layers.1.self_attn.q_proj.weight") {perm=[1, 0]}
     959 |  # n3__107
            %"k__107"<?,?> ⬅️ ::MatMul(%"model_layers_1_input_layernorm_1__105", %"tmp_0__107")
     960 |  # n4__107
            %"tmp_1__107"<?,?> ⬅️ ::Transpose(%"model.layers.1.self_attn.k_proj.weight") {perm=[1, 0]}
     961 |  # n5__107
            %"v__107"<?,?> ⬅️ ::MatMul(%"model_layers_1_input_layernorm_1__105", %"tmp_1__107")
     962 |  # n6__107
            %"const__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
     963 |  # n7__107
            %"tmp_2__107"<?,?> ⬅️ ::Squeeze(%"model.layers.1.self_attn.v_proj.weight", %"const__107")
     964 |  # n8__107
            %"int64_0_1d__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
     965 |  # n9__107
            %"cos_sin_gather_size__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
     966 |  # n10__107
            %"int64_1_1d__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
     967 |  # n11__107
            %"cos__107"<?,?> ⬅️ ::Slice(%"tmp_2__107", %"int64_0_1d__107", %"cos_sin_gather_size__107", %"int64_1_1d__107")
     968 |  # n12__107
            %"const_3__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
     969 |  # n13__107
            %"tmp_4__107"<?,?> ⬅️ ::Squeeze(%"model.layers.1.self_attn.rotary_emb.inv_freq", %"const_3__107")
     970 |  # n14__107
            %"int64_0_1d_5__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
     971 |  # n15__107
            %"cos_sin_gather_size_6__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
     972 |  # n16__107
            %"int64_1_1d_7__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
     973 |  # n17__107
            %"sin__107"<?,?> ⬅️ ::Slice(%"tmp_4__107", %"int64_0_1d_5__107", %"cos_sin_gather_size_6__107", %"int64_1_1d_7__107")
     974 |  # n18__107
            %"path__107"<?,?> ⬅️ ::Shape(%"value_states_1")
     975 |  # n19__107
            %"int64_1__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     976 |  # n20__107
            %"path2__107"<?,?> ⬅️ ::Gather(%"path__107", %"int64_1__107") {axis=0}
     977 |  # n21__107
            %"total_seq_lengths__107"<?,?> ⬅️ ::Cast(%"path2__107") {to=6}
     978 |  # n22__107
            %"int64_1_1d_8__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
     979 |  # n23__107
            %"temp__107"<?,?> ⬅️ ::ReduceSum(%"value_states_1", %"int64_1_1d_8__107")
     980 |  # n24__107
            %"int64_1_1d_9__107"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
     981 |  # n25__107
            %"int64_1_1d_9_cast__107"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__107", %"temp__107")
     982 |  # n26__107
            %"temp2__107"<?,?> ⬅️ ::Sub(%"temp__107", %"int64_1_1d_9_cast__107")
     983 |  # n27__107
            %"seqlens_k__107"<?,?> ⬅️ ::Cast(%"temp2__107") {to=6}
     984 |  # n28__107
            %"gqa_output__107"<?,?>, %"model_1_5"<FLOAT,[s0,32,s1,128]>, %"model_1_4"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__107", %"k__107", %"v__107", %"l_position_ids_", %"key_states_1", %"seqlens_k__107", %"total_seq_lengths__107", %"cos__107", %"sin__107") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
     985 |  # n29__107
            %"tmp_10__107"<?,?> ⬅️ ::Transpose(%"model.layers.1.self_attn.o_proj.weight") {perm=[1, 0]}
     986 |  # n30__107
            %"model_layers_1_self_attn_1_2__105"<?,?> ⬅️ ::MatMul(%"gqa_output__107", %"tmp_10__107")
     987 |  # n0__108
            %"alpha__108"<?,?> ⬅️ ::Constant() {value_float=1.0}
     988 |  # n1__108
            %"alpha_0__108"<?,?> ⬅️ ::CastLike(%"alpha__108", %"model_layers_1_self_attn_1_2__105")
     989 |  # n2__108
            %"other_1__108"<?,?> ⬅️ ::Mul(%"model_layers_1_self_attn_1_2__105", %"alpha_0__108")
     990 |  # n3__108
            %"add_13__105"<?,?> ⬅️ ::Add(%"model_layers_0_1_3__1", %"other_1__108")
     991 |  # n0__109
            %"model_layers_1_post_attention_layernorm_1__105"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_13__105", %"model.layers.1.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
     992 |  # n0__110
            %"gate_proj_weight_t__110"<?,?> ⬅️ ::Transpose(%"model.layers.1.mlp.gate_proj.weight") {perm=[1, 0]}
     993 |  # n1__110
            %"up_proj_weight_t__110"<?,?> ⬅️ ::Transpose(%"model.layers.1.mlp.up_proj.weight") {perm=[1, 0]}
     994 |  # n2__110
            %"down_proj_weight_t__110"<?,?> ⬅️ ::Transpose(%"model.layers.1.mlp.down_proj.weight") {perm=[1, 0]}
     995 |  # n3__110
            %"gate_proj_output__110"<?,?> ⬅️ ::MatMul(%"model_layers_1_post_attention_layernorm_1__105", %"gate_proj_weight_t__110")
     996 |  # n4__110
            %"up_proj_output__110"<?,?> ⬅️ ::MatMul(%"model_layers_1_post_attention_layernorm_1__105", %"up_proj_weight_t__110")
     997 |  # n5__110
            %"gate_times_up_output__110"<?,?> ⬅️ ::Mul(%"gate_proj_output__110", %"up_proj_output__110")
     998 |  # n6__110
            %"act_fn_output__110"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__110")
     999 |  # n7__110
            %"model_layers_1_mlp_1__105"<?,?> ⬅️ ::MatMul(%"act_fn_output__110", %"down_proj_weight_t__110")
    1000 |  # n0__111
            %"alpha__111"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1001 |  # n1__111
            %"alpha_0__111"<?,?> ⬅️ ::CastLike(%"alpha__111", %"model_layers_1_mlp_1__105")
    1002 |  # n2__111
            %"other_1__111"<?,?> ⬅️ ::Mul(%"model_layers_1_mlp_1__105", %"alpha_0__111")
    1003 |  # n3__111
            %"model_layers_1_1_2__1"<?,?> ⬅️ ::Add(%"add_13__105", %"other_1__111")
    1004 |  # n0__113
            %"model_layers_2_input_layernorm_1__112"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_1_1_2__1", %"model.layers.2.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1005 |  # n0__114
            %"tmp__114"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1006 |  # n1__114
            %"q__114"<?,?> ⬅️ ::MatMul(%"model_layers_2_input_layernorm_1__112", %"tmp__114")
    1007 |  # n2__114
            %"tmp_0__114"<?,?> ⬅️ ::Transpose(%"model.layers.2.self_attn.q_proj.weight") {perm=[1, 0]}
    1008 |  # n3__114
            %"k__114"<?,?> ⬅️ ::MatMul(%"model_layers_2_input_layernorm_1__112", %"tmp_0__114")
    1009 |  # n4__114
            %"tmp_1__114"<?,?> ⬅️ ::Transpose(%"model.layers.2.self_attn.k_proj.weight") {perm=[1, 0]}
    1010 |  # n5__114
            %"v__114"<?,?> ⬅️ ::MatMul(%"model_layers_2_input_layernorm_1__112", %"tmp_1__114")
    1011 |  # n6__114
            %"const__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1012 |  # n7__114
            %"tmp_2__114"<?,?> ⬅️ ::Squeeze(%"model.layers.2.self_attn.v_proj.weight", %"const__114")
    1013 |  # n8__114
            %"int64_0_1d__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1014 |  # n9__114
            %"cos_sin_gather_size__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1015 |  # n10__114
            %"int64_1_1d__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1016 |  # n11__114
            %"cos__114"<?,?> ⬅️ ::Slice(%"tmp_2__114", %"int64_0_1d__114", %"cos_sin_gather_size__114", %"int64_1_1d__114")
    1017 |  # n12__114
            %"const_3__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1018 |  # n13__114
            %"tmp_4__114"<?,?> ⬅️ ::Squeeze(%"model.layers.2.self_attn.rotary_emb.inv_freq", %"const_3__114")
    1019 |  # n14__114
            %"int64_0_1d_5__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1020 |  # n15__114
            %"cos_sin_gather_size_6__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1021 |  # n16__114
            %"int64_1_1d_7__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1022 |  # n17__114
            %"sin__114"<?,?> ⬅️ ::Slice(%"tmp_4__114", %"int64_0_1d_5__114", %"cos_sin_gather_size_6__114", %"int64_1_1d_7__114")
    1023 |  # n18__114
            %"path__114"<?,?> ⬅️ ::Shape(%"value_states_2")
    1024 |  # n19__114
            %"int64_1__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1025 |  # n20__114
            %"path2__114"<?,?> ⬅️ ::Gather(%"path__114", %"int64_1__114") {axis=0}
    1026 |  # n21__114
            %"total_seq_lengths__114"<?,?> ⬅️ ::Cast(%"path2__114") {to=6}
    1027 |  # n22__114
            %"int64_1_1d_8__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1028 |  # n23__114
            %"temp__114"<?,?> ⬅️ ::ReduceSum(%"value_states_2", %"int64_1_1d_8__114")
    1029 |  # n24__114
            %"int64_1_1d_9__114"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1030 |  # n25__114
            %"int64_1_1d_9_cast__114"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__114", %"temp__114")
    1031 |  # n26__114
            %"temp2__114"<?,?> ⬅️ ::Sub(%"temp__114", %"int64_1_1d_9_cast__114")
    1032 |  # n27__114
            %"seqlens_k__114"<?,?> ⬅️ ::Cast(%"temp2__114") {to=6}
    1033 |  # n28__114
            %"gqa_output__114"<?,?>, %"model_1_7"<FLOAT,[s0,32,s1,128]>, %"model_1_6"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__114", %"k__114", %"v__114", %"l_position_ids_", %"key_states_2", %"seqlens_k__114", %"total_seq_lengths__114", %"cos__114", %"sin__114") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1034 |  # n29__114
            %"tmp_10__114"<?,?> ⬅️ ::Transpose(%"model.layers.2.self_attn.o_proj.weight") {perm=[1, 0]}
    1035 |  # n30__114
            %"model_layers_2_self_attn_1_2__112"<?,?> ⬅️ ::MatMul(%"gqa_output__114", %"tmp_10__114")
    1036 |  # n0__115
            %"alpha__115"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1037 |  # n1__115
            %"alpha_0__115"<?,?> ⬅️ ::CastLike(%"alpha__115", %"model_layers_2_self_attn_1_2__112")
    1038 |  # n2__115
            %"other_1__115"<?,?> ⬅️ ::Mul(%"model_layers_2_self_attn_1_2__112", %"alpha_0__115")
    1039 |  # n3__115
            %"add_20__112"<?,?> ⬅️ ::Add(%"model_layers_1_1_2__1", %"other_1__115")
    1040 |  # n0__116
            %"model_layers_2_post_attention_layernorm_1__112"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_20__112", %"model.layers.2.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1041 |  # n0__117
            %"gate_proj_weight_t__117"<?,?> ⬅️ ::Transpose(%"model.layers.2.mlp.gate_proj.weight") {perm=[1, 0]}
    1042 |  # n1__117
            %"up_proj_weight_t__117"<?,?> ⬅️ ::Transpose(%"model.layers.2.mlp.up_proj.weight") {perm=[1, 0]}
    1043 |  # n2__117
            %"down_proj_weight_t__117"<?,?> ⬅️ ::Transpose(%"model.layers.2.mlp.down_proj.weight") {perm=[1, 0]}
    1044 |  # n3__117
            %"gate_proj_output__117"<?,?> ⬅️ ::MatMul(%"model_layers_2_post_attention_layernorm_1__112", %"gate_proj_weight_t__117")
    1045 |  # n4__117
            %"up_proj_output__117"<?,?> ⬅️ ::MatMul(%"model_layers_2_post_attention_layernorm_1__112", %"up_proj_weight_t__117")
    1046 |  # n5__117
            %"gate_times_up_output__117"<?,?> ⬅️ ::Mul(%"gate_proj_output__117", %"up_proj_output__117")
    1047 |  # n6__117
            %"act_fn_output__117"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__117")
    1048 |  # n7__117
            %"model_layers_2_mlp_1__112"<?,?> ⬅️ ::MatMul(%"act_fn_output__117", %"down_proj_weight_t__117")
    1049 |  # n0__118
            %"alpha__118"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1050 |  # n1__118
            %"alpha_0__118"<?,?> ⬅️ ::CastLike(%"alpha__118", %"model_layers_2_mlp_1__112")
    1051 |  # n2__118
            %"other_1__118"<?,?> ⬅️ ::Mul(%"model_layers_2_mlp_1__112", %"alpha_0__118")
    1052 |  # n3__118
            %"model_layers_2_1_2__1"<?,?> ⬅️ ::Add(%"add_20__112", %"other_1__118")
    1053 |  # n0__120
            %"model_layers_3_input_layernorm_1__119"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_2_1_2__1", %"model.layers.3.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1054 |  # n0__121
            %"tmp__121"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1055 |  # n1__121
            %"q__121"<?,?> ⬅️ ::MatMul(%"model_layers_3_input_layernorm_1__119", %"tmp__121")
    1056 |  # n2__121
            %"tmp_0__121"<?,?> ⬅️ ::Transpose(%"model.layers.3.self_attn.q_proj.weight") {perm=[1, 0]}
    1057 |  # n3__121
            %"k__121"<?,?> ⬅️ ::MatMul(%"model_layers_3_input_layernorm_1__119", %"tmp_0__121")
    1058 |  # n4__121
            %"tmp_1__121"<?,?> ⬅️ ::Transpose(%"model.layers.3.self_attn.k_proj.weight") {perm=[1, 0]}
    1059 |  # n5__121
            %"v__121"<?,?> ⬅️ ::MatMul(%"model_layers_3_input_layernorm_1__119", %"tmp_1__121")
    1060 |  # n6__121
            %"const__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1061 |  # n7__121
            %"tmp_2__121"<?,?> ⬅️ ::Squeeze(%"model.layers.3.self_attn.v_proj.weight", %"const__121")
    1062 |  # n8__121
            %"int64_0_1d__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1063 |  # n9__121
            %"cos_sin_gather_size__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1064 |  # n10__121
            %"int64_1_1d__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1065 |  # n11__121
            %"cos__121"<?,?> ⬅️ ::Slice(%"tmp_2__121", %"int64_0_1d__121", %"cos_sin_gather_size__121", %"int64_1_1d__121")
    1066 |  # n12__121
            %"const_3__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1067 |  # n13__121
            %"tmp_4__121"<?,?> ⬅️ ::Squeeze(%"model.layers.3.self_attn.rotary_emb.inv_freq", %"const_3__121")
    1068 |  # n14__121
            %"int64_0_1d_5__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1069 |  # n15__121
            %"cos_sin_gather_size_6__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1070 |  # n16__121
            %"int64_1_1d_7__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1071 |  # n17__121
            %"sin__121"<?,?> ⬅️ ::Slice(%"tmp_4__121", %"int64_0_1d_5__121", %"cos_sin_gather_size_6__121", %"int64_1_1d_7__121")
    1072 |  # n18__121
            %"path__121"<?,?> ⬅️ ::Shape(%"value_states_3")
    1073 |  # n19__121
            %"int64_1__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1074 |  # n20__121
            %"path2__121"<?,?> ⬅️ ::Gather(%"path__121", %"int64_1__121") {axis=0}
    1075 |  # n21__121
            %"total_seq_lengths__121"<?,?> ⬅️ ::Cast(%"path2__121") {to=6}
    1076 |  # n22__121
            %"int64_1_1d_8__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1077 |  # n23__121
            %"temp__121"<?,?> ⬅️ ::ReduceSum(%"value_states_3", %"int64_1_1d_8__121")
    1078 |  # n24__121
            %"int64_1_1d_9__121"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1079 |  # n25__121
            %"int64_1_1d_9_cast__121"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__121", %"temp__121")
    1080 |  # n26__121
            %"temp2__121"<?,?> ⬅️ ::Sub(%"temp__121", %"int64_1_1d_9_cast__121")
    1081 |  # n27__121
            %"seqlens_k__121"<?,?> ⬅️ ::Cast(%"temp2__121") {to=6}
    1082 |  # n28__121
            %"gqa_output__121"<?,?>, %"model_1_9"<FLOAT,[s0,32,s1,128]>, %"model_1_8"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__121", %"k__121", %"v__121", %"l_position_ids_", %"key_states_3", %"seqlens_k__121", %"total_seq_lengths__121", %"cos__121", %"sin__121") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1083 |  # n29__121
            %"tmp_10__121"<?,?> ⬅️ ::Transpose(%"model.layers.3.self_attn.o_proj.weight") {perm=[1, 0]}
    1084 |  # n30__121
            %"model_layers_3_self_attn_1_2__119"<?,?> ⬅️ ::MatMul(%"gqa_output__121", %"tmp_10__121")
    1085 |  # n0__122
            %"alpha__122"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1086 |  # n1__122
            %"alpha_0__122"<?,?> ⬅️ ::CastLike(%"alpha__122", %"model_layers_3_self_attn_1_2__119")
    1087 |  # n2__122
            %"other_1__122"<?,?> ⬅️ ::Mul(%"model_layers_3_self_attn_1_2__119", %"alpha_0__122")
    1088 |  # n3__122
            %"add_27__119"<?,?> ⬅️ ::Add(%"model_layers_2_1_2__1", %"other_1__122")
    1089 |  # n0__123
            %"model_layers_3_post_attention_layernorm_1__119"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_27__119", %"model.layers.3.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1090 |  # n0__124
            %"gate_proj_weight_t__124"<?,?> ⬅️ ::Transpose(%"model.layers.3.mlp.gate_proj.weight") {perm=[1, 0]}
    1091 |  # n1__124
            %"up_proj_weight_t__124"<?,?> ⬅️ ::Transpose(%"model.layers.3.mlp.up_proj.weight") {perm=[1, 0]}
    1092 |  # n2__124
            %"down_proj_weight_t__124"<?,?> ⬅️ ::Transpose(%"model.layers.3.mlp.down_proj.weight") {perm=[1, 0]}
    1093 |  # n3__124
            %"gate_proj_output__124"<?,?> ⬅️ ::MatMul(%"model_layers_3_post_attention_layernorm_1__119", %"gate_proj_weight_t__124")
    1094 |  # n4__124
            %"up_proj_output__124"<?,?> ⬅️ ::MatMul(%"model_layers_3_post_attention_layernorm_1__119", %"up_proj_weight_t__124")
    1095 |  # n5__124
            %"gate_times_up_output__124"<?,?> ⬅️ ::Mul(%"gate_proj_output__124", %"up_proj_output__124")
    1096 |  # n6__124
            %"act_fn_output__124"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__124")
    1097 |  # n7__124
            %"model_layers_3_mlp_1__119"<?,?> ⬅️ ::MatMul(%"act_fn_output__124", %"down_proj_weight_t__124")
    1098 |  # n0__125
            %"alpha__125"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1099 |  # n1__125
            %"alpha_0__125"<?,?> ⬅️ ::CastLike(%"alpha__125", %"model_layers_3_mlp_1__119")
    1100 |  # n2__125
            %"other_1__125"<?,?> ⬅️ ::Mul(%"model_layers_3_mlp_1__119", %"alpha_0__125")
    1101 |  # n3__125
            %"model_layers_3_1_2__1"<?,?> ⬅️ ::Add(%"add_27__119", %"other_1__125")
    1102 |  # n0__127
            %"model_layers_4_input_layernorm_1__126"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_3_1_2__1", %"model.layers.4.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1103 |  # n0__128
            %"tmp__128"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1104 |  # n1__128
            %"q__128"<?,?> ⬅️ ::MatMul(%"model_layers_4_input_layernorm_1__126", %"tmp__128")
    1105 |  # n2__128
            %"tmp_0__128"<?,?> ⬅️ ::Transpose(%"model.layers.4.self_attn.q_proj.weight") {perm=[1, 0]}
    1106 |  # n3__128
            %"k__128"<?,?> ⬅️ ::MatMul(%"model_layers_4_input_layernorm_1__126", %"tmp_0__128")
    1107 |  # n4__128
            %"tmp_1__128"<?,?> ⬅️ ::Transpose(%"model.layers.4.self_attn.k_proj.weight") {perm=[1, 0]}
    1108 |  # n5__128
            %"v__128"<?,?> ⬅️ ::MatMul(%"model_layers_4_input_layernorm_1__126", %"tmp_1__128")
    1109 |  # n6__128
            %"const__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1110 |  # n7__128
            %"tmp_2__128"<?,?> ⬅️ ::Squeeze(%"model.layers.4.self_attn.v_proj.weight", %"const__128")
    1111 |  # n8__128
            %"int64_0_1d__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1112 |  # n9__128
            %"cos_sin_gather_size__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1113 |  # n10__128
            %"int64_1_1d__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1114 |  # n11__128
            %"cos__128"<?,?> ⬅️ ::Slice(%"tmp_2__128", %"int64_0_1d__128", %"cos_sin_gather_size__128", %"int64_1_1d__128")
    1115 |  # n12__128
            %"const_3__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1116 |  # n13__128
            %"tmp_4__128"<?,?> ⬅️ ::Squeeze(%"model.layers.4.self_attn.rotary_emb.inv_freq", %"const_3__128")
    1117 |  # n14__128
            %"int64_0_1d_5__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1118 |  # n15__128
            %"cos_sin_gather_size_6__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1119 |  # n16__128
            %"int64_1_1d_7__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1120 |  # n17__128
            %"sin__128"<?,?> ⬅️ ::Slice(%"tmp_4__128", %"int64_0_1d_5__128", %"cos_sin_gather_size_6__128", %"int64_1_1d_7__128")
    1121 |  # n18__128
            %"path__128"<?,?> ⬅️ ::Shape(%"value_states_4")
    1122 |  # n19__128
            %"int64_1__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1123 |  # n20__128
            %"path2__128"<?,?> ⬅️ ::Gather(%"path__128", %"int64_1__128") {axis=0}
    1124 |  # n21__128
            %"total_seq_lengths__128"<?,?> ⬅️ ::Cast(%"path2__128") {to=6}
    1125 |  # n22__128
            %"int64_1_1d_8__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1126 |  # n23__128
            %"temp__128"<?,?> ⬅️ ::ReduceSum(%"value_states_4", %"int64_1_1d_8__128")
    1127 |  # n24__128
            %"int64_1_1d_9__128"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1128 |  # n25__128
            %"int64_1_1d_9_cast__128"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__128", %"temp__128")
    1129 |  # n26__128
            %"temp2__128"<?,?> ⬅️ ::Sub(%"temp__128", %"int64_1_1d_9_cast__128")
    1130 |  # n27__128
            %"seqlens_k__128"<?,?> ⬅️ ::Cast(%"temp2__128") {to=6}
    1131 |  # n28__128
            %"gqa_output__128"<?,?>, %"model_1_11"<FLOAT,[s0,32,s1,128]>, %"model_1_10"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__128", %"k__128", %"v__128", %"l_position_ids_", %"key_states_4", %"seqlens_k__128", %"total_seq_lengths__128", %"cos__128", %"sin__128") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1132 |  # n29__128
            %"tmp_10__128"<?,?> ⬅️ ::Transpose(%"model.layers.4.self_attn.o_proj.weight") {perm=[1, 0]}
    1133 |  # n30__128
            %"model_layers_4_self_attn_1_2__126"<?,?> ⬅️ ::MatMul(%"gqa_output__128", %"tmp_10__128")
    1134 |  # n0__129
            %"alpha__129"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1135 |  # n1__129
            %"alpha_0__129"<?,?> ⬅️ ::CastLike(%"alpha__129", %"model_layers_4_self_attn_1_2__126")
    1136 |  # n2__129
            %"other_1__129"<?,?> ⬅️ ::Mul(%"model_layers_4_self_attn_1_2__126", %"alpha_0__129")
    1137 |  # n3__129
            %"add_34__126"<?,?> ⬅️ ::Add(%"model_layers_3_1_2__1", %"other_1__129")
    1138 |  # n0__130
            %"model_layers_4_post_attention_layernorm_1__126"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_34__126", %"model.layers.4.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1139 |  # n0__131
            %"gate_proj_weight_t__131"<?,?> ⬅️ ::Transpose(%"model.layers.4.mlp.gate_proj.weight") {perm=[1, 0]}
    1140 |  # n1__131
            %"up_proj_weight_t__131"<?,?> ⬅️ ::Transpose(%"model.layers.4.mlp.up_proj.weight") {perm=[1, 0]}
    1141 |  # n2__131
            %"down_proj_weight_t__131"<?,?> ⬅️ ::Transpose(%"model.layers.4.mlp.down_proj.weight") {perm=[1, 0]}
    1142 |  # n3__131
            %"gate_proj_output__131"<?,?> ⬅️ ::MatMul(%"model_layers_4_post_attention_layernorm_1__126", %"gate_proj_weight_t__131")
    1143 |  # n4__131
            %"up_proj_output__131"<?,?> ⬅️ ::MatMul(%"model_layers_4_post_attention_layernorm_1__126", %"up_proj_weight_t__131")
    1144 |  # n5__131
            %"gate_times_up_output__131"<?,?> ⬅️ ::Mul(%"gate_proj_output__131", %"up_proj_output__131")
    1145 |  # n6__131
            %"act_fn_output__131"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__131")
    1146 |  # n7__131
            %"model_layers_4_mlp_1__126"<?,?> ⬅️ ::MatMul(%"act_fn_output__131", %"down_proj_weight_t__131")
    1147 |  # n0__132
            %"alpha__132"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1148 |  # n1__132
            %"alpha_0__132"<?,?> ⬅️ ::CastLike(%"alpha__132", %"model_layers_4_mlp_1__126")
    1149 |  # n2__132
            %"other_1__132"<?,?> ⬅️ ::Mul(%"model_layers_4_mlp_1__126", %"alpha_0__132")
    1150 |  # n3__132
            %"model_layers_4_1_2__1"<?,?> ⬅️ ::Add(%"add_34__126", %"other_1__132")
    1151 |  # n0__134
            %"model_layers_5_input_layernorm_1__133"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_4_1_2__1", %"model.layers.5.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1152 |  # n0__135
            %"tmp__135"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1153 |  # n1__135
            %"q__135"<?,?> ⬅️ ::MatMul(%"model_layers_5_input_layernorm_1__133", %"tmp__135")
    1154 |  # n2__135
            %"tmp_0__135"<?,?> ⬅️ ::Transpose(%"model.layers.5.self_attn.q_proj.weight") {perm=[1, 0]}
    1155 |  # n3__135
            %"k__135"<?,?> ⬅️ ::MatMul(%"model_layers_5_input_layernorm_1__133", %"tmp_0__135")
    1156 |  # n4__135
            %"tmp_1__135"<?,?> ⬅️ ::Transpose(%"model.layers.5.self_attn.k_proj.weight") {perm=[1, 0]}
    1157 |  # n5__135
            %"v__135"<?,?> ⬅️ ::MatMul(%"model_layers_5_input_layernorm_1__133", %"tmp_1__135")
    1158 |  # n6__135
            %"const__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1159 |  # n7__135
            %"tmp_2__135"<?,?> ⬅️ ::Squeeze(%"model.layers.5.self_attn.v_proj.weight", %"const__135")
    1160 |  # n8__135
            %"int64_0_1d__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1161 |  # n9__135
            %"cos_sin_gather_size__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1162 |  # n10__135
            %"int64_1_1d__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1163 |  # n11__135
            %"cos__135"<?,?> ⬅️ ::Slice(%"tmp_2__135", %"int64_0_1d__135", %"cos_sin_gather_size__135", %"int64_1_1d__135")
    1164 |  # n12__135
            %"const_3__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1165 |  # n13__135
            %"tmp_4__135"<?,?> ⬅️ ::Squeeze(%"model.layers.5.self_attn.rotary_emb.inv_freq", %"const_3__135")
    1166 |  # n14__135
            %"int64_0_1d_5__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1167 |  # n15__135
            %"cos_sin_gather_size_6__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1168 |  # n16__135
            %"int64_1_1d_7__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1169 |  # n17__135
            %"sin__135"<?,?> ⬅️ ::Slice(%"tmp_4__135", %"int64_0_1d_5__135", %"cos_sin_gather_size_6__135", %"int64_1_1d_7__135")
    1170 |  # n18__135
            %"path__135"<?,?> ⬅️ ::Shape(%"value_states_5")
    1171 |  # n19__135
            %"int64_1__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1172 |  # n20__135
            %"path2__135"<?,?> ⬅️ ::Gather(%"path__135", %"int64_1__135") {axis=0}
    1173 |  # n21__135
            %"total_seq_lengths__135"<?,?> ⬅️ ::Cast(%"path2__135") {to=6}
    1174 |  # n22__135
            %"int64_1_1d_8__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1175 |  # n23__135
            %"temp__135"<?,?> ⬅️ ::ReduceSum(%"value_states_5", %"int64_1_1d_8__135")
    1176 |  # n24__135
            %"int64_1_1d_9__135"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1177 |  # n25__135
            %"int64_1_1d_9_cast__135"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__135", %"temp__135")
    1178 |  # n26__135
            %"temp2__135"<?,?> ⬅️ ::Sub(%"temp__135", %"int64_1_1d_9_cast__135")
    1179 |  # n27__135
            %"seqlens_k__135"<?,?> ⬅️ ::Cast(%"temp2__135") {to=6}
    1180 |  # n28__135
            %"gqa_output__135"<?,?>, %"model_1_13"<FLOAT,[s0,32,s1,128]>, %"model_1_12"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__135", %"k__135", %"v__135", %"l_position_ids_", %"key_states_5", %"seqlens_k__135", %"total_seq_lengths__135", %"cos__135", %"sin__135") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1181 |  # n29__135
            %"tmp_10__135"<?,?> ⬅️ ::Transpose(%"model.layers.5.self_attn.o_proj.weight") {perm=[1, 0]}
    1182 |  # n30__135
            %"model_layers_5_self_attn_1_2__133"<?,?> ⬅️ ::MatMul(%"gqa_output__135", %"tmp_10__135")
    1183 |  # n0__136
            %"alpha__136"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1184 |  # n1__136
            %"alpha_0__136"<?,?> ⬅️ ::CastLike(%"alpha__136", %"model_layers_5_self_attn_1_2__133")
    1185 |  # n2__136
            %"other_1__136"<?,?> ⬅️ ::Mul(%"model_layers_5_self_attn_1_2__133", %"alpha_0__136")
    1186 |  # n3__136
            %"add_41__133"<?,?> ⬅️ ::Add(%"model_layers_4_1_2__1", %"other_1__136")
    1187 |  # n0__137
            %"model_layers_5_post_attention_layernorm_1__133"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_41__133", %"model.layers.5.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1188 |  # n0__138
            %"gate_proj_weight_t__138"<?,?> ⬅️ ::Transpose(%"model.layers.5.mlp.gate_proj.weight") {perm=[1, 0]}
    1189 |  # n1__138
            %"up_proj_weight_t__138"<?,?> ⬅️ ::Transpose(%"model.layers.5.mlp.up_proj.weight") {perm=[1, 0]}
    1190 |  # n2__138
            %"down_proj_weight_t__138"<?,?> ⬅️ ::Transpose(%"model.layers.5.mlp.down_proj.weight") {perm=[1, 0]}
    1191 |  # n3__138
            %"gate_proj_output__138"<?,?> ⬅️ ::MatMul(%"model_layers_5_post_attention_layernorm_1__133", %"gate_proj_weight_t__138")
    1192 |  # n4__138
            %"up_proj_output__138"<?,?> ⬅️ ::MatMul(%"model_layers_5_post_attention_layernorm_1__133", %"up_proj_weight_t__138")
    1193 |  # n5__138
            %"gate_times_up_output__138"<?,?> ⬅️ ::Mul(%"gate_proj_output__138", %"up_proj_output__138")
    1194 |  # n6__138
            %"act_fn_output__138"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__138")
    1195 |  # n7__138
            %"model_layers_5_mlp_1__133"<?,?> ⬅️ ::MatMul(%"act_fn_output__138", %"down_proj_weight_t__138")
    1196 |  # n0__139
            %"alpha__139"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1197 |  # n1__139
            %"alpha_0__139"<?,?> ⬅️ ::CastLike(%"alpha__139", %"model_layers_5_mlp_1__133")
    1198 |  # n2__139
            %"other_1__139"<?,?> ⬅️ ::Mul(%"model_layers_5_mlp_1__133", %"alpha_0__139")
    1199 |  # n3__139
            %"model_layers_5_1_2__1"<?,?> ⬅️ ::Add(%"add_41__133", %"other_1__139")
    1200 |  # n0__141
            %"model_layers_6_input_layernorm_1__140"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_5_1_2__1", %"model.layers.6.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1201 |  # n0__142
            %"tmp__142"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1202 |  # n1__142
            %"q__142"<?,?> ⬅️ ::MatMul(%"model_layers_6_input_layernorm_1__140", %"tmp__142")
    1203 |  # n2__142
            %"tmp_0__142"<?,?> ⬅️ ::Transpose(%"model.layers.6.self_attn.q_proj.weight") {perm=[1, 0]}
    1204 |  # n3__142
            %"k__142"<?,?> ⬅️ ::MatMul(%"model_layers_6_input_layernorm_1__140", %"tmp_0__142")
    1205 |  # n4__142
            %"tmp_1__142"<?,?> ⬅️ ::Transpose(%"model.layers.6.self_attn.k_proj.weight") {perm=[1, 0]}
    1206 |  # n5__142
            %"v__142"<?,?> ⬅️ ::MatMul(%"model_layers_6_input_layernorm_1__140", %"tmp_1__142")
    1207 |  # n6__142
            %"const__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1208 |  # n7__142
            %"tmp_2__142"<?,?> ⬅️ ::Squeeze(%"model.layers.6.self_attn.v_proj.weight", %"const__142")
    1209 |  # n8__142
            %"int64_0_1d__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1210 |  # n9__142
            %"cos_sin_gather_size__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1211 |  # n10__142
            %"int64_1_1d__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1212 |  # n11__142
            %"cos__142"<?,?> ⬅️ ::Slice(%"tmp_2__142", %"int64_0_1d__142", %"cos_sin_gather_size__142", %"int64_1_1d__142")
    1213 |  # n12__142
            %"const_3__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1214 |  # n13__142
            %"tmp_4__142"<?,?> ⬅️ ::Squeeze(%"model.layers.6.self_attn.rotary_emb.inv_freq", %"const_3__142")
    1215 |  # n14__142
            %"int64_0_1d_5__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1216 |  # n15__142
            %"cos_sin_gather_size_6__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1217 |  # n16__142
            %"int64_1_1d_7__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1218 |  # n17__142
            %"sin__142"<?,?> ⬅️ ::Slice(%"tmp_4__142", %"int64_0_1d_5__142", %"cos_sin_gather_size_6__142", %"int64_1_1d_7__142")
    1219 |  # n18__142
            %"path__142"<?,?> ⬅️ ::Shape(%"value_states_6")
    1220 |  # n19__142
            %"int64_1__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1221 |  # n20__142
            %"path2__142"<?,?> ⬅️ ::Gather(%"path__142", %"int64_1__142") {axis=0}
    1222 |  # n21__142
            %"total_seq_lengths__142"<?,?> ⬅️ ::Cast(%"path2__142") {to=6}
    1223 |  # n22__142
            %"int64_1_1d_8__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1224 |  # n23__142
            %"temp__142"<?,?> ⬅️ ::ReduceSum(%"value_states_6", %"int64_1_1d_8__142")
    1225 |  # n24__142
            %"int64_1_1d_9__142"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1226 |  # n25__142
            %"int64_1_1d_9_cast__142"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__142", %"temp__142")
    1227 |  # n26__142
            %"temp2__142"<?,?> ⬅️ ::Sub(%"temp__142", %"int64_1_1d_9_cast__142")
    1228 |  # n27__142
            %"seqlens_k__142"<?,?> ⬅️ ::Cast(%"temp2__142") {to=6}
    1229 |  # n28__142
            %"gqa_output__142"<?,?>, %"model_1_15"<FLOAT,[s0,32,s1,128]>, %"model_1_14"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__142", %"k__142", %"v__142", %"l_position_ids_", %"key_states_6", %"seqlens_k__142", %"total_seq_lengths__142", %"cos__142", %"sin__142") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1230 |  # n29__142
            %"tmp_10__142"<?,?> ⬅️ ::Transpose(%"model.layers.6.self_attn.o_proj.weight") {perm=[1, 0]}
    1231 |  # n30__142
            %"model_layers_6_self_attn_1_2__140"<?,?> ⬅️ ::MatMul(%"gqa_output__142", %"tmp_10__142")
    1232 |  # n0__143
            %"alpha__143"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1233 |  # n1__143
            %"alpha_0__143"<?,?> ⬅️ ::CastLike(%"alpha__143", %"model_layers_6_self_attn_1_2__140")
    1234 |  # n2__143
            %"other_1__143"<?,?> ⬅️ ::Mul(%"model_layers_6_self_attn_1_2__140", %"alpha_0__143")
    1235 |  # n3__143
            %"add_48__140"<?,?> ⬅️ ::Add(%"model_layers_5_1_2__1", %"other_1__143")
    1236 |  # n0__144
            %"model_layers_6_post_attention_layernorm_1__140"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_48__140", %"model.layers.6.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1237 |  # n0__145
            %"gate_proj_weight_t__145"<?,?> ⬅️ ::Transpose(%"model.layers.6.mlp.gate_proj.weight") {perm=[1, 0]}
    1238 |  # n1__145
            %"up_proj_weight_t__145"<?,?> ⬅️ ::Transpose(%"model.layers.6.mlp.up_proj.weight") {perm=[1, 0]}
    1239 |  # n2__145
            %"down_proj_weight_t__145"<?,?> ⬅️ ::Transpose(%"model.layers.6.mlp.down_proj.weight") {perm=[1, 0]}
    1240 |  # n3__145
            %"gate_proj_output__145"<?,?> ⬅️ ::MatMul(%"model_layers_6_post_attention_layernorm_1__140", %"gate_proj_weight_t__145")
    1241 |  # n4__145
            %"up_proj_output__145"<?,?> ⬅️ ::MatMul(%"model_layers_6_post_attention_layernorm_1__140", %"up_proj_weight_t__145")
    1242 |  # n5__145
            %"gate_times_up_output__145"<?,?> ⬅️ ::Mul(%"gate_proj_output__145", %"up_proj_output__145")
    1243 |  # n6__145
            %"act_fn_output__145"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__145")
    1244 |  # n7__145
            %"model_layers_6_mlp_1__140"<?,?> ⬅️ ::MatMul(%"act_fn_output__145", %"down_proj_weight_t__145")
    1245 |  # n0__146
            %"alpha__146"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1246 |  # n1__146
            %"alpha_0__146"<?,?> ⬅️ ::CastLike(%"alpha__146", %"model_layers_6_mlp_1__140")
    1247 |  # n2__146
            %"other_1__146"<?,?> ⬅️ ::Mul(%"model_layers_6_mlp_1__140", %"alpha_0__146")
    1248 |  # n3__146
            %"model_layers_6_1_2__1"<?,?> ⬅️ ::Add(%"add_48__140", %"other_1__146")
    1249 |  # n0__148
            %"model_layers_7_input_layernorm_1__147"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_6_1_2__1", %"model.layers.7.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1250 |  # n0__149
            %"tmp__149"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1251 |  # n1__149
            %"q__149"<?,?> ⬅️ ::MatMul(%"model_layers_7_input_layernorm_1__147", %"tmp__149")
    1252 |  # n2__149
            %"tmp_0__149"<?,?> ⬅️ ::Transpose(%"model.layers.7.self_attn.q_proj.weight") {perm=[1, 0]}
    1253 |  # n3__149
            %"k__149"<?,?> ⬅️ ::MatMul(%"model_layers_7_input_layernorm_1__147", %"tmp_0__149")
    1254 |  # n4__149
            %"tmp_1__149"<?,?> ⬅️ ::Transpose(%"model.layers.7.self_attn.k_proj.weight") {perm=[1, 0]}
    1255 |  # n5__149
            %"v__149"<?,?> ⬅️ ::MatMul(%"model_layers_7_input_layernorm_1__147", %"tmp_1__149")
    1256 |  # n6__149
            %"const__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1257 |  # n7__149
            %"tmp_2__149"<?,?> ⬅️ ::Squeeze(%"model.layers.7.self_attn.v_proj.weight", %"const__149")
    1258 |  # n8__149
            %"int64_0_1d__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1259 |  # n9__149
            %"cos_sin_gather_size__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1260 |  # n10__149
            %"int64_1_1d__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1261 |  # n11__149
            %"cos__149"<?,?> ⬅️ ::Slice(%"tmp_2__149", %"int64_0_1d__149", %"cos_sin_gather_size__149", %"int64_1_1d__149")
    1262 |  # n12__149
            %"const_3__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1263 |  # n13__149
            %"tmp_4__149"<?,?> ⬅️ ::Squeeze(%"model.layers.7.self_attn.rotary_emb.inv_freq", %"const_3__149")
    1264 |  # n14__149
            %"int64_0_1d_5__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1265 |  # n15__149
            %"cos_sin_gather_size_6__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1266 |  # n16__149
            %"int64_1_1d_7__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1267 |  # n17__149
            %"sin__149"<?,?> ⬅️ ::Slice(%"tmp_4__149", %"int64_0_1d_5__149", %"cos_sin_gather_size_6__149", %"int64_1_1d_7__149")
    1268 |  # n18__149
            %"path__149"<?,?> ⬅️ ::Shape(%"value_states_7")
    1269 |  # n19__149
            %"int64_1__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1270 |  # n20__149
            %"path2__149"<?,?> ⬅️ ::Gather(%"path__149", %"int64_1__149") {axis=0}
    1271 |  # n21__149
            %"total_seq_lengths__149"<?,?> ⬅️ ::Cast(%"path2__149") {to=6}
    1272 |  # n22__149
            %"int64_1_1d_8__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1273 |  # n23__149
            %"temp__149"<?,?> ⬅️ ::ReduceSum(%"value_states_7", %"int64_1_1d_8__149")
    1274 |  # n24__149
            %"int64_1_1d_9__149"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1275 |  # n25__149
            %"int64_1_1d_9_cast__149"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__149", %"temp__149")
    1276 |  # n26__149
            %"temp2__149"<?,?> ⬅️ ::Sub(%"temp__149", %"int64_1_1d_9_cast__149")
    1277 |  # n27__149
            %"seqlens_k__149"<?,?> ⬅️ ::Cast(%"temp2__149") {to=6}
    1278 |  # n28__149
            %"gqa_output__149"<?,?>, %"model_1_17"<FLOAT,[s0,32,s1,128]>, %"model_1_16"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__149", %"k__149", %"v__149", %"l_position_ids_", %"key_states_7", %"seqlens_k__149", %"total_seq_lengths__149", %"cos__149", %"sin__149") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1279 |  # n29__149
            %"tmp_10__149"<?,?> ⬅️ ::Transpose(%"model.layers.7.self_attn.o_proj.weight") {perm=[1, 0]}
    1280 |  # n30__149
            %"model_layers_7_self_attn_1_2__147"<?,?> ⬅️ ::MatMul(%"gqa_output__149", %"tmp_10__149")
    1281 |  # n0__150
            %"alpha__150"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1282 |  # n1__150
            %"alpha_0__150"<?,?> ⬅️ ::CastLike(%"alpha__150", %"model_layers_7_self_attn_1_2__147")
    1283 |  # n2__150
            %"other_1__150"<?,?> ⬅️ ::Mul(%"model_layers_7_self_attn_1_2__147", %"alpha_0__150")
    1284 |  # n3__150
            %"add_55__147"<?,?> ⬅️ ::Add(%"model_layers_6_1_2__1", %"other_1__150")
    1285 |  # n0__151
            %"model_layers_7_post_attention_layernorm_1__147"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_55__147", %"model.layers.7.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1286 |  # n0__152
            %"gate_proj_weight_t__152"<?,?> ⬅️ ::Transpose(%"model.layers.7.mlp.gate_proj.weight") {perm=[1, 0]}
    1287 |  # n1__152
            %"up_proj_weight_t__152"<?,?> ⬅️ ::Transpose(%"model.layers.7.mlp.up_proj.weight") {perm=[1, 0]}
    1288 |  # n2__152
            %"down_proj_weight_t__152"<?,?> ⬅️ ::Transpose(%"model.layers.7.mlp.down_proj.weight") {perm=[1, 0]}
    1289 |  # n3__152
            %"gate_proj_output__152"<?,?> ⬅️ ::MatMul(%"model_layers_7_post_attention_layernorm_1__147", %"gate_proj_weight_t__152")
    1290 |  # n4__152
            %"up_proj_output__152"<?,?> ⬅️ ::MatMul(%"model_layers_7_post_attention_layernorm_1__147", %"up_proj_weight_t__152")
    1291 |  # n5__152
            %"gate_times_up_output__152"<?,?> ⬅️ ::Mul(%"gate_proj_output__152", %"up_proj_output__152")
    1292 |  # n6__152
            %"act_fn_output__152"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__152")
    1293 |  # n7__152
            %"model_layers_7_mlp_1__147"<?,?> ⬅️ ::MatMul(%"act_fn_output__152", %"down_proj_weight_t__152")
    1294 |  # n0__153
            %"alpha__153"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1295 |  # n1__153
            %"alpha_0__153"<?,?> ⬅️ ::CastLike(%"alpha__153", %"model_layers_7_mlp_1__147")
    1296 |  # n2__153
            %"other_1__153"<?,?> ⬅️ ::Mul(%"model_layers_7_mlp_1__147", %"alpha_0__153")
    1297 |  # n3__153
            %"model_layers_7_1_2__1"<?,?> ⬅️ ::Add(%"add_55__147", %"other_1__153")
    1298 |  # n0__155
            %"model_layers_8_input_layernorm_1__154"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_7_1_2__1", %"model.layers.8.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1299 |  # n0__156
            %"tmp__156"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1300 |  # n1__156
            %"q__156"<?,?> ⬅️ ::MatMul(%"model_layers_8_input_layernorm_1__154", %"tmp__156")
    1301 |  # n2__156
            %"tmp_0__156"<?,?> ⬅️ ::Transpose(%"model.layers.8.self_attn.q_proj.weight") {perm=[1, 0]}
    1302 |  # n3__156
            %"k__156"<?,?> ⬅️ ::MatMul(%"model_layers_8_input_layernorm_1__154", %"tmp_0__156")
    1303 |  # n4__156
            %"tmp_1__156"<?,?> ⬅️ ::Transpose(%"model.layers.8.self_attn.k_proj.weight") {perm=[1, 0]}
    1304 |  # n5__156
            %"v__156"<?,?> ⬅️ ::MatMul(%"model_layers_8_input_layernorm_1__154", %"tmp_1__156")
    1305 |  # n6__156
            %"const__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1306 |  # n7__156
            %"tmp_2__156"<?,?> ⬅️ ::Squeeze(%"model.layers.8.self_attn.v_proj.weight", %"const__156")
    1307 |  # n8__156
            %"int64_0_1d__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1308 |  # n9__156
            %"cos_sin_gather_size__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1309 |  # n10__156
            %"int64_1_1d__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1310 |  # n11__156
            %"cos__156"<?,?> ⬅️ ::Slice(%"tmp_2__156", %"int64_0_1d__156", %"cos_sin_gather_size__156", %"int64_1_1d__156")
    1311 |  # n12__156
            %"const_3__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1312 |  # n13__156
            %"tmp_4__156"<?,?> ⬅️ ::Squeeze(%"model.layers.8.self_attn.rotary_emb.inv_freq", %"const_3__156")
    1313 |  # n14__156
            %"int64_0_1d_5__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1314 |  # n15__156
            %"cos_sin_gather_size_6__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1315 |  # n16__156
            %"int64_1_1d_7__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1316 |  # n17__156
            %"sin__156"<?,?> ⬅️ ::Slice(%"tmp_4__156", %"int64_0_1d_5__156", %"cos_sin_gather_size_6__156", %"int64_1_1d_7__156")
    1317 |  # n18__156
            %"path__156"<?,?> ⬅️ ::Shape(%"value_states_8")
    1318 |  # n19__156
            %"int64_1__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1319 |  # n20__156
            %"path2__156"<?,?> ⬅️ ::Gather(%"path__156", %"int64_1__156") {axis=0}
    1320 |  # n21__156
            %"total_seq_lengths__156"<?,?> ⬅️ ::Cast(%"path2__156") {to=6}
    1321 |  # n22__156
            %"int64_1_1d_8__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1322 |  # n23__156
            %"temp__156"<?,?> ⬅️ ::ReduceSum(%"value_states_8", %"int64_1_1d_8__156")
    1323 |  # n24__156
            %"int64_1_1d_9__156"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1324 |  # n25__156
            %"int64_1_1d_9_cast__156"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__156", %"temp__156")
    1325 |  # n26__156
            %"temp2__156"<?,?> ⬅️ ::Sub(%"temp__156", %"int64_1_1d_9_cast__156")
    1326 |  # n27__156
            %"seqlens_k__156"<?,?> ⬅️ ::Cast(%"temp2__156") {to=6}
    1327 |  # n28__156
            %"gqa_output__156"<?,?>, %"model_1_19"<FLOAT,[s0,32,s1,128]>, %"model_1_18"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__156", %"k__156", %"v__156", %"l_position_ids_", %"key_states_8", %"seqlens_k__156", %"total_seq_lengths__156", %"cos__156", %"sin__156") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1328 |  # n29__156
            %"tmp_10__156"<?,?> ⬅️ ::Transpose(%"model.layers.8.self_attn.o_proj.weight") {perm=[1, 0]}
    1329 |  # n30__156
            %"model_layers_8_self_attn_1_2__154"<?,?> ⬅️ ::MatMul(%"gqa_output__156", %"tmp_10__156")
    1330 |  # n0__157
            %"alpha__157"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1331 |  # n1__157
            %"alpha_0__157"<?,?> ⬅️ ::CastLike(%"alpha__157", %"model_layers_8_self_attn_1_2__154")
    1332 |  # n2__157
            %"other_1__157"<?,?> ⬅️ ::Mul(%"model_layers_8_self_attn_1_2__154", %"alpha_0__157")
    1333 |  # n3__157
            %"add_62__154"<?,?> ⬅️ ::Add(%"model_layers_7_1_2__1", %"other_1__157")
    1334 |  # n0__158
            %"model_layers_8_post_attention_layernorm_1__154"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_62__154", %"model.layers.8.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1335 |  # n0__159
            %"gate_proj_weight_t__159"<?,?> ⬅️ ::Transpose(%"model.layers.8.mlp.gate_proj.weight") {perm=[1, 0]}
    1336 |  # n1__159
            %"up_proj_weight_t__159"<?,?> ⬅️ ::Transpose(%"model.layers.8.mlp.up_proj.weight") {perm=[1, 0]}
    1337 |  # n2__159
            %"down_proj_weight_t__159"<?,?> ⬅️ ::Transpose(%"model.layers.8.mlp.down_proj.weight") {perm=[1, 0]}
    1338 |  # n3__159
            %"gate_proj_output__159"<?,?> ⬅️ ::MatMul(%"model_layers_8_post_attention_layernorm_1__154", %"gate_proj_weight_t__159")
    1339 |  # n4__159
            %"up_proj_output__159"<?,?> ⬅️ ::MatMul(%"model_layers_8_post_attention_layernorm_1__154", %"up_proj_weight_t__159")
    1340 |  # n5__159
            %"gate_times_up_output__159"<?,?> ⬅️ ::Mul(%"gate_proj_output__159", %"up_proj_output__159")
    1341 |  # n6__159
            %"act_fn_output__159"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__159")
    1342 |  # n7__159
            %"model_layers_8_mlp_1__154"<?,?> ⬅️ ::MatMul(%"act_fn_output__159", %"down_proj_weight_t__159")
    1343 |  # n0__160
            %"alpha__160"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1344 |  # n1__160
            %"alpha_0__160"<?,?> ⬅️ ::CastLike(%"alpha__160", %"model_layers_8_mlp_1__154")
    1345 |  # n2__160
            %"other_1__160"<?,?> ⬅️ ::Mul(%"model_layers_8_mlp_1__154", %"alpha_0__160")
    1346 |  # n3__160
            %"model_layers_8_1_2__1"<?,?> ⬅️ ::Add(%"add_62__154", %"other_1__160")
    1347 |  # n0__162
            %"model_layers_9_input_layernorm_1__161"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_8_1_2__1", %"model.layers.9.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1348 |  # n0__163
            %"tmp__163"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1349 |  # n1__163
            %"q__163"<?,?> ⬅️ ::MatMul(%"model_layers_9_input_layernorm_1__161", %"tmp__163")
    1350 |  # n2__163
            %"tmp_0__163"<?,?> ⬅️ ::Transpose(%"model.layers.9.self_attn.q_proj.weight") {perm=[1, 0]}
    1351 |  # n3__163
            %"k__163"<?,?> ⬅️ ::MatMul(%"model_layers_9_input_layernorm_1__161", %"tmp_0__163")
    1352 |  # n4__163
            %"tmp_1__163"<?,?> ⬅️ ::Transpose(%"model.layers.9.self_attn.k_proj.weight") {perm=[1, 0]}
    1353 |  # n5__163
            %"v__163"<?,?> ⬅️ ::MatMul(%"model_layers_9_input_layernorm_1__161", %"tmp_1__163")
    1354 |  # n6__163
            %"const__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1355 |  # n7__163
            %"tmp_2__163"<?,?> ⬅️ ::Squeeze(%"model.layers.9.self_attn.v_proj.weight", %"const__163")
    1356 |  # n8__163
            %"int64_0_1d__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1357 |  # n9__163
            %"cos_sin_gather_size__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1358 |  # n10__163
            %"int64_1_1d__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1359 |  # n11__163
            %"cos__163"<?,?> ⬅️ ::Slice(%"tmp_2__163", %"int64_0_1d__163", %"cos_sin_gather_size__163", %"int64_1_1d__163")
    1360 |  # n12__163
            %"const_3__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1361 |  # n13__163
            %"tmp_4__163"<?,?> ⬅️ ::Squeeze(%"model.layers.9.self_attn.rotary_emb.inv_freq", %"const_3__163")
    1362 |  # n14__163
            %"int64_0_1d_5__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1363 |  # n15__163
            %"cos_sin_gather_size_6__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1364 |  # n16__163
            %"int64_1_1d_7__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1365 |  # n17__163
            %"sin__163"<?,?> ⬅️ ::Slice(%"tmp_4__163", %"int64_0_1d_5__163", %"cos_sin_gather_size_6__163", %"int64_1_1d_7__163")
    1366 |  # n18__163
            %"path__163"<?,?> ⬅️ ::Shape(%"value_states_9")
    1367 |  # n19__163
            %"int64_1__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1368 |  # n20__163
            %"path2__163"<?,?> ⬅️ ::Gather(%"path__163", %"int64_1__163") {axis=0}
    1369 |  # n21__163
            %"total_seq_lengths__163"<?,?> ⬅️ ::Cast(%"path2__163") {to=6}
    1370 |  # n22__163
            %"int64_1_1d_8__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1371 |  # n23__163
            %"temp__163"<?,?> ⬅️ ::ReduceSum(%"value_states_9", %"int64_1_1d_8__163")
    1372 |  # n24__163
            %"int64_1_1d_9__163"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1373 |  # n25__163
            %"int64_1_1d_9_cast__163"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__163", %"temp__163")
    1374 |  # n26__163
            %"temp2__163"<?,?> ⬅️ ::Sub(%"temp__163", %"int64_1_1d_9_cast__163")
    1375 |  # n27__163
            %"seqlens_k__163"<?,?> ⬅️ ::Cast(%"temp2__163") {to=6}
    1376 |  # n28__163
            %"gqa_output__163"<?,?>, %"model_1_21"<FLOAT,[s0,32,s1,128]>, %"model_1_20"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__163", %"k__163", %"v__163", %"l_position_ids_", %"key_states_9", %"seqlens_k__163", %"total_seq_lengths__163", %"cos__163", %"sin__163") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1377 |  # n29__163
            %"tmp_10__163"<?,?> ⬅️ ::Transpose(%"model.layers.9.self_attn.o_proj.weight") {perm=[1, 0]}
    1378 |  # n30__163
            %"model_layers_9_self_attn_1_2__161"<?,?> ⬅️ ::MatMul(%"gqa_output__163", %"tmp_10__163")
    1379 |  # n0__164
            %"alpha__164"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1380 |  # n1__164
            %"alpha_0__164"<?,?> ⬅️ ::CastLike(%"alpha__164", %"model_layers_9_self_attn_1_2__161")
    1381 |  # n2__164
            %"other_1__164"<?,?> ⬅️ ::Mul(%"model_layers_9_self_attn_1_2__161", %"alpha_0__164")
    1382 |  # n3__164
            %"add_69__161"<?,?> ⬅️ ::Add(%"model_layers_8_1_2__1", %"other_1__164")
    1383 |  # n0__165
            %"model_layers_9_post_attention_layernorm_1__161"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_69__161", %"model.layers.9.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1384 |  # n0__166
            %"gate_proj_weight_t__166"<?,?> ⬅️ ::Transpose(%"model.layers.9.mlp.gate_proj.weight") {perm=[1, 0]}
    1385 |  # n1__166
            %"up_proj_weight_t__166"<?,?> ⬅️ ::Transpose(%"model.layers.9.mlp.up_proj.weight") {perm=[1, 0]}
    1386 |  # n2__166
            %"down_proj_weight_t__166"<?,?> ⬅️ ::Transpose(%"model.layers.9.mlp.down_proj.weight") {perm=[1, 0]}
    1387 |  # n3__166
            %"gate_proj_output__166"<?,?> ⬅️ ::MatMul(%"model_layers_9_post_attention_layernorm_1__161", %"gate_proj_weight_t__166")
    1388 |  # n4__166
            %"up_proj_output__166"<?,?> ⬅️ ::MatMul(%"model_layers_9_post_attention_layernorm_1__161", %"up_proj_weight_t__166")
    1389 |  # n5__166
            %"gate_times_up_output__166"<?,?> ⬅️ ::Mul(%"gate_proj_output__166", %"up_proj_output__166")
    1390 |  # n6__166
            %"act_fn_output__166"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__166")
    1391 |  # n7__166
            %"model_layers_9_mlp_1__161"<?,?> ⬅️ ::MatMul(%"act_fn_output__166", %"down_proj_weight_t__166")
    1392 |  # n0__167
            %"alpha__167"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1393 |  # n1__167
            %"alpha_0__167"<?,?> ⬅️ ::CastLike(%"alpha__167", %"model_layers_9_mlp_1__161")
    1394 |  # n2__167
            %"other_1__167"<?,?> ⬅️ ::Mul(%"model_layers_9_mlp_1__161", %"alpha_0__167")
    1395 |  # n3__167
            %"model_layers_9_1_2__1"<?,?> ⬅️ ::Add(%"add_69__161", %"other_1__167")
    1396 |  # n0__169
            %"model_layers_10_input_layernorm_1__168"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_9_1_2__1", %"model.layers.10.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1397 |  # n0__170
            %"tmp__170"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1398 |  # n1__170
            %"q__170"<?,?> ⬅️ ::MatMul(%"model_layers_10_input_layernorm_1__168", %"tmp__170")
    1399 |  # n2__170
            %"tmp_0__170"<?,?> ⬅️ ::Transpose(%"model.layers.10.self_attn.q_proj.weight") {perm=[1, 0]}
    1400 |  # n3__170
            %"k__170"<?,?> ⬅️ ::MatMul(%"model_layers_10_input_layernorm_1__168", %"tmp_0__170")
    1401 |  # n4__170
            %"tmp_1__170"<?,?> ⬅️ ::Transpose(%"model.layers.10.self_attn.k_proj.weight") {perm=[1, 0]}
    1402 |  # n5__170
            %"v__170"<?,?> ⬅️ ::MatMul(%"model_layers_10_input_layernorm_1__168", %"tmp_1__170")
    1403 |  # n6__170
            %"const__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1404 |  # n7__170
            %"tmp_2__170"<?,?> ⬅️ ::Squeeze(%"model.layers.10.self_attn.v_proj.weight", %"const__170")
    1405 |  # n8__170
            %"int64_0_1d__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1406 |  # n9__170
            %"cos_sin_gather_size__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1407 |  # n10__170
            %"int64_1_1d__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1408 |  # n11__170
            %"cos__170"<?,?> ⬅️ ::Slice(%"tmp_2__170", %"int64_0_1d__170", %"cos_sin_gather_size__170", %"int64_1_1d__170")
    1409 |  # n12__170
            %"const_3__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1410 |  # n13__170
            %"tmp_4__170"<?,?> ⬅️ ::Squeeze(%"model.layers.10.self_attn.rotary_emb.inv_freq", %"const_3__170")
    1411 |  # n14__170
            %"int64_0_1d_5__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1412 |  # n15__170
            %"cos_sin_gather_size_6__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1413 |  # n16__170
            %"int64_1_1d_7__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1414 |  # n17__170
            %"sin__170"<?,?> ⬅️ ::Slice(%"tmp_4__170", %"int64_0_1d_5__170", %"cos_sin_gather_size_6__170", %"int64_1_1d_7__170")
    1415 |  # n18__170
            %"path__170"<?,?> ⬅️ ::Shape(%"value_states_10")
    1416 |  # n19__170
            %"int64_1__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1417 |  # n20__170
            %"path2__170"<?,?> ⬅️ ::Gather(%"path__170", %"int64_1__170") {axis=0}
    1418 |  # n21__170
            %"total_seq_lengths__170"<?,?> ⬅️ ::Cast(%"path2__170") {to=6}
    1419 |  # n22__170
            %"int64_1_1d_8__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1420 |  # n23__170
            %"temp__170"<?,?> ⬅️ ::ReduceSum(%"value_states_10", %"int64_1_1d_8__170")
    1421 |  # n24__170
            %"int64_1_1d_9__170"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1422 |  # n25__170
            %"int64_1_1d_9_cast__170"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__170", %"temp__170")
    1423 |  # n26__170
            %"temp2__170"<?,?> ⬅️ ::Sub(%"temp__170", %"int64_1_1d_9_cast__170")
    1424 |  # n27__170
            %"seqlens_k__170"<?,?> ⬅️ ::Cast(%"temp2__170") {to=6}
    1425 |  # n28__170
            %"gqa_output__170"<?,?>, %"model_1_23"<FLOAT,[s0,32,s1,128]>, %"model_1_22"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__170", %"k__170", %"v__170", %"l_position_ids_", %"key_states_10", %"seqlens_k__170", %"total_seq_lengths__170", %"cos__170", %"sin__170") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1426 |  # n29__170
            %"tmp_10__170"<?,?> ⬅️ ::Transpose(%"model.layers.10.self_attn.o_proj.weight") {perm=[1, 0]}
    1427 |  # n30__170
            %"model_layers_10_self_attn_1_2__168"<?,?> ⬅️ ::MatMul(%"gqa_output__170", %"tmp_10__170")
    1428 |  # n0__171
            %"alpha__171"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1429 |  # n1__171
            %"alpha_0__171"<?,?> ⬅️ ::CastLike(%"alpha__171", %"model_layers_10_self_attn_1_2__168")
    1430 |  # n2__171
            %"other_1__171"<?,?> ⬅️ ::Mul(%"model_layers_10_self_attn_1_2__168", %"alpha_0__171")
    1431 |  # n3__171
            %"add_76__168"<?,?> ⬅️ ::Add(%"model_layers_9_1_2__1", %"other_1__171")
    1432 |  # n0__172
            %"model_layers_10_post_attention_layernorm_1__168"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_76__168", %"model.layers.10.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1433 |  # n0__173
            %"gate_proj_weight_t__173"<?,?> ⬅️ ::Transpose(%"model.layers.10.mlp.gate_proj.weight") {perm=[1, 0]}
    1434 |  # n1__173
            %"up_proj_weight_t__173"<?,?> ⬅️ ::Transpose(%"model.layers.10.mlp.up_proj.weight") {perm=[1, 0]}
    1435 |  # n2__173
            %"down_proj_weight_t__173"<?,?> ⬅️ ::Transpose(%"model.layers.10.mlp.down_proj.weight") {perm=[1, 0]}
    1436 |  # n3__173
            %"gate_proj_output__173"<?,?> ⬅️ ::MatMul(%"model_layers_10_post_attention_layernorm_1__168", %"gate_proj_weight_t__173")
    1437 |  # n4__173
            %"up_proj_output__173"<?,?> ⬅️ ::MatMul(%"model_layers_10_post_attention_layernorm_1__168", %"up_proj_weight_t__173")
    1438 |  # n5__173
            %"gate_times_up_output__173"<?,?> ⬅️ ::Mul(%"gate_proj_output__173", %"up_proj_output__173")
    1439 |  # n6__173
            %"act_fn_output__173"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__173")
    1440 |  # n7__173
            %"model_layers_10_mlp_1__168"<?,?> ⬅️ ::MatMul(%"act_fn_output__173", %"down_proj_weight_t__173")
    1441 |  # n0__174
            %"alpha__174"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1442 |  # n1__174
            %"alpha_0__174"<?,?> ⬅️ ::CastLike(%"alpha__174", %"model_layers_10_mlp_1__168")
    1443 |  # n2__174
            %"other_1__174"<?,?> ⬅️ ::Mul(%"model_layers_10_mlp_1__168", %"alpha_0__174")
    1444 |  # n3__174
            %"model_layers_10_1_2__1"<?,?> ⬅️ ::Add(%"add_76__168", %"other_1__174")
    1445 |  # n0__176
            %"model_layers_11_input_layernorm_1__175"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_10_1_2__1", %"model.layers.11.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1446 |  # n0__177
            %"tmp__177"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1447 |  # n1__177
            %"q__177"<?,?> ⬅️ ::MatMul(%"model_layers_11_input_layernorm_1__175", %"tmp__177")
    1448 |  # n2__177
            %"tmp_0__177"<?,?> ⬅️ ::Transpose(%"model.layers.11.self_attn.q_proj.weight") {perm=[1, 0]}
    1449 |  # n3__177
            %"k__177"<?,?> ⬅️ ::MatMul(%"model_layers_11_input_layernorm_1__175", %"tmp_0__177")
    1450 |  # n4__177
            %"tmp_1__177"<?,?> ⬅️ ::Transpose(%"model.layers.11.self_attn.k_proj.weight") {perm=[1, 0]}
    1451 |  # n5__177
            %"v__177"<?,?> ⬅️ ::MatMul(%"model_layers_11_input_layernorm_1__175", %"tmp_1__177")
    1452 |  # n6__177
            %"const__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1453 |  # n7__177
            %"tmp_2__177"<?,?> ⬅️ ::Squeeze(%"model.layers.11.self_attn.v_proj.weight", %"const__177")
    1454 |  # n8__177
            %"int64_0_1d__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1455 |  # n9__177
            %"cos_sin_gather_size__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1456 |  # n10__177
            %"int64_1_1d__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1457 |  # n11__177
            %"cos__177"<?,?> ⬅️ ::Slice(%"tmp_2__177", %"int64_0_1d__177", %"cos_sin_gather_size__177", %"int64_1_1d__177")
    1458 |  # n12__177
            %"const_3__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1459 |  # n13__177
            %"tmp_4__177"<?,?> ⬅️ ::Squeeze(%"model.layers.11.self_attn.rotary_emb.inv_freq", %"const_3__177")
    1460 |  # n14__177
            %"int64_0_1d_5__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1461 |  # n15__177
            %"cos_sin_gather_size_6__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1462 |  # n16__177
            %"int64_1_1d_7__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1463 |  # n17__177
            %"sin__177"<?,?> ⬅️ ::Slice(%"tmp_4__177", %"int64_0_1d_5__177", %"cos_sin_gather_size_6__177", %"int64_1_1d_7__177")
    1464 |  # n18__177
            %"path__177"<?,?> ⬅️ ::Shape(%"value_states_11")
    1465 |  # n19__177
            %"int64_1__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1466 |  # n20__177
            %"path2__177"<?,?> ⬅️ ::Gather(%"path__177", %"int64_1__177") {axis=0}
    1467 |  # n21__177
            %"total_seq_lengths__177"<?,?> ⬅️ ::Cast(%"path2__177") {to=6}
    1468 |  # n22__177
            %"int64_1_1d_8__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1469 |  # n23__177
            %"temp__177"<?,?> ⬅️ ::ReduceSum(%"value_states_11", %"int64_1_1d_8__177")
    1470 |  # n24__177
            %"int64_1_1d_9__177"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1471 |  # n25__177
            %"int64_1_1d_9_cast__177"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__177", %"temp__177")
    1472 |  # n26__177
            %"temp2__177"<?,?> ⬅️ ::Sub(%"temp__177", %"int64_1_1d_9_cast__177")
    1473 |  # n27__177
            %"seqlens_k__177"<?,?> ⬅️ ::Cast(%"temp2__177") {to=6}
    1474 |  # n28__177
            %"gqa_output__177"<?,?>, %"model_1_25"<FLOAT,[s0,32,s1,128]>, %"model_1_24"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__177", %"k__177", %"v__177", %"l_position_ids_", %"key_states_11", %"seqlens_k__177", %"total_seq_lengths__177", %"cos__177", %"sin__177") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1475 |  # n29__177
            %"tmp_10__177"<?,?> ⬅️ ::Transpose(%"model.layers.11.self_attn.o_proj.weight") {perm=[1, 0]}
    1476 |  # n30__177
            %"model_layers_11_self_attn_1_2__175"<?,?> ⬅️ ::MatMul(%"gqa_output__177", %"tmp_10__177")
    1477 |  # n0__178
            %"alpha__178"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1478 |  # n1__178
            %"alpha_0__178"<?,?> ⬅️ ::CastLike(%"alpha__178", %"model_layers_11_self_attn_1_2__175")
    1479 |  # n2__178
            %"other_1__178"<?,?> ⬅️ ::Mul(%"model_layers_11_self_attn_1_2__175", %"alpha_0__178")
    1480 |  # n3__178
            %"add_83__175"<?,?> ⬅️ ::Add(%"model_layers_10_1_2__1", %"other_1__178")
    1481 |  # n0__179
            %"model_layers_11_post_attention_layernorm_1__175"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_83__175", %"model.layers.11.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1482 |  # n0__180
            %"gate_proj_weight_t__180"<?,?> ⬅️ ::Transpose(%"model.layers.11.mlp.gate_proj.weight") {perm=[1, 0]}
    1483 |  # n1__180
            %"up_proj_weight_t__180"<?,?> ⬅️ ::Transpose(%"model.layers.11.mlp.up_proj.weight") {perm=[1, 0]}
    1484 |  # n2__180
            %"down_proj_weight_t__180"<?,?> ⬅️ ::Transpose(%"model.layers.11.mlp.down_proj.weight") {perm=[1, 0]}
    1485 |  # n3__180
            %"gate_proj_output__180"<?,?> ⬅️ ::MatMul(%"model_layers_11_post_attention_layernorm_1__175", %"gate_proj_weight_t__180")
    1486 |  # n4__180
            %"up_proj_output__180"<?,?> ⬅️ ::MatMul(%"model_layers_11_post_attention_layernorm_1__175", %"up_proj_weight_t__180")
    1487 |  # n5__180
            %"gate_times_up_output__180"<?,?> ⬅️ ::Mul(%"gate_proj_output__180", %"up_proj_output__180")
    1488 |  # n6__180
            %"act_fn_output__180"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__180")
    1489 |  # n7__180
            %"model_layers_11_mlp_1__175"<?,?> ⬅️ ::MatMul(%"act_fn_output__180", %"down_proj_weight_t__180")
    1490 |  # n0__181
            %"alpha__181"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1491 |  # n1__181
            %"alpha_0__181"<?,?> ⬅️ ::CastLike(%"alpha__181", %"model_layers_11_mlp_1__175")
    1492 |  # n2__181
            %"other_1__181"<?,?> ⬅️ ::Mul(%"model_layers_11_mlp_1__175", %"alpha_0__181")
    1493 |  # n3__181
            %"model_layers_11_1_2__1"<?,?> ⬅️ ::Add(%"add_83__175", %"other_1__181")
    1494 |  # n0__183
            %"model_layers_12_input_layernorm_1__182"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_11_1_2__1", %"model.layers.12.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1495 |  # n0__184
            %"tmp__184"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1496 |  # n1__184
            %"q__184"<?,?> ⬅️ ::MatMul(%"model_layers_12_input_layernorm_1__182", %"tmp__184")
    1497 |  # n2__184
            %"tmp_0__184"<?,?> ⬅️ ::Transpose(%"model.layers.12.self_attn.q_proj.weight") {perm=[1, 0]}
    1498 |  # n3__184
            %"k__184"<?,?> ⬅️ ::MatMul(%"model_layers_12_input_layernorm_1__182", %"tmp_0__184")
    1499 |  # n4__184
            %"tmp_1__184"<?,?> ⬅️ ::Transpose(%"model.layers.12.self_attn.k_proj.weight") {perm=[1, 0]}
    1500 |  # n5__184
            %"v__184"<?,?> ⬅️ ::MatMul(%"model_layers_12_input_layernorm_1__182", %"tmp_1__184")
    1501 |  # n6__184
            %"const__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1502 |  # n7__184
            %"tmp_2__184"<?,?> ⬅️ ::Squeeze(%"model.layers.12.self_attn.v_proj.weight", %"const__184")
    1503 |  # n8__184
            %"int64_0_1d__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1504 |  # n9__184
            %"cos_sin_gather_size__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1505 |  # n10__184
            %"int64_1_1d__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1506 |  # n11__184
            %"cos__184"<?,?> ⬅️ ::Slice(%"tmp_2__184", %"int64_0_1d__184", %"cos_sin_gather_size__184", %"int64_1_1d__184")
    1507 |  # n12__184
            %"const_3__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1508 |  # n13__184
            %"tmp_4__184"<?,?> ⬅️ ::Squeeze(%"model.layers.12.self_attn.rotary_emb.inv_freq", %"const_3__184")
    1509 |  # n14__184
            %"int64_0_1d_5__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1510 |  # n15__184
            %"cos_sin_gather_size_6__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1511 |  # n16__184
            %"int64_1_1d_7__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1512 |  # n17__184
            %"sin__184"<?,?> ⬅️ ::Slice(%"tmp_4__184", %"int64_0_1d_5__184", %"cos_sin_gather_size_6__184", %"int64_1_1d_7__184")
    1513 |  # n18__184
            %"path__184"<?,?> ⬅️ ::Shape(%"value_states_12")
    1514 |  # n19__184
            %"int64_1__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1515 |  # n20__184
            %"path2__184"<?,?> ⬅️ ::Gather(%"path__184", %"int64_1__184") {axis=0}
    1516 |  # n21__184
            %"total_seq_lengths__184"<?,?> ⬅️ ::Cast(%"path2__184") {to=6}
    1517 |  # n22__184
            %"int64_1_1d_8__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1518 |  # n23__184
            %"temp__184"<?,?> ⬅️ ::ReduceSum(%"value_states_12", %"int64_1_1d_8__184")
    1519 |  # n24__184
            %"int64_1_1d_9__184"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1520 |  # n25__184
            %"int64_1_1d_9_cast__184"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__184", %"temp__184")
    1521 |  # n26__184
            %"temp2__184"<?,?> ⬅️ ::Sub(%"temp__184", %"int64_1_1d_9_cast__184")
    1522 |  # n27__184
            %"seqlens_k__184"<?,?> ⬅️ ::Cast(%"temp2__184") {to=6}
    1523 |  # n28__184
            %"gqa_output__184"<?,?>, %"model_1_27"<FLOAT,[s0,32,s1,128]>, %"model_1_26"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__184", %"k__184", %"v__184", %"l_position_ids_", %"key_states_12", %"seqlens_k__184", %"total_seq_lengths__184", %"cos__184", %"sin__184") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1524 |  # n29__184
            %"tmp_10__184"<?,?> ⬅️ ::Transpose(%"model.layers.12.self_attn.o_proj.weight") {perm=[1, 0]}
    1525 |  # n30__184
            %"model_layers_12_self_attn_1_2__182"<?,?> ⬅️ ::MatMul(%"gqa_output__184", %"tmp_10__184")
    1526 |  # n0__185
            %"alpha__185"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1527 |  # n1__185
            %"alpha_0__185"<?,?> ⬅️ ::CastLike(%"alpha__185", %"model_layers_12_self_attn_1_2__182")
    1528 |  # n2__185
            %"other_1__185"<?,?> ⬅️ ::Mul(%"model_layers_12_self_attn_1_2__182", %"alpha_0__185")
    1529 |  # n3__185
            %"add_90__182"<?,?> ⬅️ ::Add(%"model_layers_11_1_2__1", %"other_1__185")
    1530 |  # n0__186
            %"model_layers_12_post_attention_layernorm_1__182"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_90__182", %"model.layers.12.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1531 |  # n0__187
            %"gate_proj_weight_t__187"<?,?> ⬅️ ::Transpose(%"model.layers.12.mlp.gate_proj.weight") {perm=[1, 0]}
    1532 |  # n1__187
            %"up_proj_weight_t__187"<?,?> ⬅️ ::Transpose(%"model.layers.12.mlp.up_proj.weight") {perm=[1, 0]}
    1533 |  # n2__187
            %"down_proj_weight_t__187"<?,?> ⬅️ ::Transpose(%"model.layers.12.mlp.down_proj.weight") {perm=[1, 0]}
    1534 |  # n3__187
            %"gate_proj_output__187"<?,?> ⬅️ ::MatMul(%"model_layers_12_post_attention_layernorm_1__182", %"gate_proj_weight_t__187")
    1535 |  # n4__187
            %"up_proj_output__187"<?,?> ⬅️ ::MatMul(%"model_layers_12_post_attention_layernorm_1__182", %"up_proj_weight_t__187")
    1536 |  # n5__187
            %"gate_times_up_output__187"<?,?> ⬅️ ::Mul(%"gate_proj_output__187", %"up_proj_output__187")
    1537 |  # n6__187
            %"act_fn_output__187"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__187")
    1538 |  # n7__187
            %"model_layers_12_mlp_1__182"<?,?> ⬅️ ::MatMul(%"act_fn_output__187", %"down_proj_weight_t__187")
    1539 |  # n0__188
            %"alpha__188"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1540 |  # n1__188
            %"alpha_0__188"<?,?> ⬅️ ::CastLike(%"alpha__188", %"model_layers_12_mlp_1__182")
    1541 |  # n2__188
            %"other_1__188"<?,?> ⬅️ ::Mul(%"model_layers_12_mlp_1__182", %"alpha_0__188")
    1542 |  # n3__188
            %"model_layers_12_1_2__1"<?,?> ⬅️ ::Add(%"add_90__182", %"other_1__188")
    1543 |  # n0__190
            %"model_layers_13_input_layernorm_1__189"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_12_1_2__1", %"model.layers.13.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1544 |  # n0__191
            %"tmp__191"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1545 |  # n1__191
            %"q__191"<?,?> ⬅️ ::MatMul(%"model_layers_13_input_layernorm_1__189", %"tmp__191")
    1546 |  # n2__191
            %"tmp_0__191"<?,?> ⬅️ ::Transpose(%"model.layers.13.self_attn.q_proj.weight") {perm=[1, 0]}
    1547 |  # n3__191
            %"k__191"<?,?> ⬅️ ::MatMul(%"model_layers_13_input_layernorm_1__189", %"tmp_0__191")
    1548 |  # n4__191
            %"tmp_1__191"<?,?> ⬅️ ::Transpose(%"model.layers.13.self_attn.k_proj.weight") {perm=[1, 0]}
    1549 |  # n5__191
            %"v__191"<?,?> ⬅️ ::MatMul(%"model_layers_13_input_layernorm_1__189", %"tmp_1__191")
    1550 |  # n6__191
            %"const__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1551 |  # n7__191
            %"tmp_2__191"<?,?> ⬅️ ::Squeeze(%"model.layers.13.self_attn.v_proj.weight", %"const__191")
    1552 |  # n8__191
            %"int64_0_1d__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1553 |  # n9__191
            %"cos_sin_gather_size__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1554 |  # n10__191
            %"int64_1_1d__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1555 |  # n11__191
            %"cos__191"<?,?> ⬅️ ::Slice(%"tmp_2__191", %"int64_0_1d__191", %"cos_sin_gather_size__191", %"int64_1_1d__191")
    1556 |  # n12__191
            %"const_3__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1557 |  # n13__191
            %"tmp_4__191"<?,?> ⬅️ ::Squeeze(%"model.layers.13.self_attn.rotary_emb.inv_freq", %"const_3__191")
    1558 |  # n14__191
            %"int64_0_1d_5__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1559 |  # n15__191
            %"cos_sin_gather_size_6__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1560 |  # n16__191
            %"int64_1_1d_7__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1561 |  # n17__191
            %"sin__191"<?,?> ⬅️ ::Slice(%"tmp_4__191", %"int64_0_1d_5__191", %"cos_sin_gather_size_6__191", %"int64_1_1d_7__191")
    1562 |  # n18__191
            %"path__191"<?,?> ⬅️ ::Shape(%"value_states_13")
    1563 |  # n19__191
            %"int64_1__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1564 |  # n20__191
            %"path2__191"<?,?> ⬅️ ::Gather(%"path__191", %"int64_1__191") {axis=0}
    1565 |  # n21__191
            %"total_seq_lengths__191"<?,?> ⬅️ ::Cast(%"path2__191") {to=6}
    1566 |  # n22__191
            %"int64_1_1d_8__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1567 |  # n23__191
            %"temp__191"<?,?> ⬅️ ::ReduceSum(%"value_states_13", %"int64_1_1d_8__191")
    1568 |  # n24__191
            %"int64_1_1d_9__191"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1569 |  # n25__191
            %"int64_1_1d_9_cast__191"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__191", %"temp__191")
    1570 |  # n26__191
            %"temp2__191"<?,?> ⬅️ ::Sub(%"temp__191", %"int64_1_1d_9_cast__191")
    1571 |  # n27__191
            %"seqlens_k__191"<?,?> ⬅️ ::Cast(%"temp2__191") {to=6}
    1572 |  # n28__191
            %"gqa_output__191"<?,?>, %"model_1_29"<FLOAT,[s0,32,s1,128]>, %"model_1_28"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__191", %"k__191", %"v__191", %"l_position_ids_", %"key_states_13", %"seqlens_k__191", %"total_seq_lengths__191", %"cos__191", %"sin__191") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1573 |  # n29__191
            %"tmp_10__191"<?,?> ⬅️ ::Transpose(%"model.layers.13.self_attn.o_proj.weight") {perm=[1, 0]}
    1574 |  # n30__191
            %"model_layers_13_self_attn_1_2__189"<?,?> ⬅️ ::MatMul(%"gqa_output__191", %"tmp_10__191")
    1575 |  # n0__192
            %"alpha__192"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1576 |  # n1__192
            %"alpha_0__192"<?,?> ⬅️ ::CastLike(%"alpha__192", %"model_layers_13_self_attn_1_2__189")
    1577 |  # n2__192
            %"other_1__192"<?,?> ⬅️ ::Mul(%"model_layers_13_self_attn_1_2__189", %"alpha_0__192")
    1578 |  # n3__192
            %"add_97__189"<?,?> ⬅️ ::Add(%"model_layers_12_1_2__1", %"other_1__192")
    1579 |  # n0__193
            %"model_layers_13_post_attention_layernorm_1__189"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_97__189", %"model.layers.13.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1580 |  # n0__194
            %"gate_proj_weight_t__194"<?,?> ⬅️ ::Transpose(%"model.layers.13.mlp.gate_proj.weight") {perm=[1, 0]}
    1581 |  # n1__194
            %"up_proj_weight_t__194"<?,?> ⬅️ ::Transpose(%"model.layers.13.mlp.up_proj.weight") {perm=[1, 0]}
    1582 |  # n2__194
            %"down_proj_weight_t__194"<?,?> ⬅️ ::Transpose(%"model.layers.13.mlp.down_proj.weight") {perm=[1, 0]}
    1583 |  # n3__194
            %"gate_proj_output__194"<?,?> ⬅️ ::MatMul(%"model_layers_13_post_attention_layernorm_1__189", %"gate_proj_weight_t__194")
    1584 |  # n4__194
            %"up_proj_output__194"<?,?> ⬅️ ::MatMul(%"model_layers_13_post_attention_layernorm_1__189", %"up_proj_weight_t__194")
    1585 |  # n5__194
            %"gate_times_up_output__194"<?,?> ⬅️ ::Mul(%"gate_proj_output__194", %"up_proj_output__194")
    1586 |  # n6__194
            %"act_fn_output__194"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__194")
    1587 |  # n7__194
            %"model_layers_13_mlp_1__189"<?,?> ⬅️ ::MatMul(%"act_fn_output__194", %"down_proj_weight_t__194")
    1588 |  # n0__195
            %"alpha__195"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1589 |  # n1__195
            %"alpha_0__195"<?,?> ⬅️ ::CastLike(%"alpha__195", %"model_layers_13_mlp_1__189")
    1590 |  # n2__195
            %"other_1__195"<?,?> ⬅️ ::Mul(%"model_layers_13_mlp_1__189", %"alpha_0__195")
    1591 |  # n3__195
            %"model_layers_13_1_2__1"<?,?> ⬅️ ::Add(%"add_97__189", %"other_1__195")
    1592 |  # n0__197
            %"model_layers_14_input_layernorm_1__196"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_13_1_2__1", %"model.layers.14.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1593 |  # n0__198
            %"tmp__198"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1594 |  # n1__198
            %"q__198"<?,?> ⬅️ ::MatMul(%"model_layers_14_input_layernorm_1__196", %"tmp__198")
    1595 |  # n2__198
            %"tmp_0__198"<?,?> ⬅️ ::Transpose(%"model.layers.14.self_attn.q_proj.weight") {perm=[1, 0]}
    1596 |  # n3__198
            %"k__198"<?,?> ⬅️ ::MatMul(%"model_layers_14_input_layernorm_1__196", %"tmp_0__198")
    1597 |  # n4__198
            %"tmp_1__198"<?,?> ⬅️ ::Transpose(%"model.layers.14.self_attn.k_proj.weight") {perm=[1, 0]}
    1598 |  # n5__198
            %"v__198"<?,?> ⬅️ ::MatMul(%"model_layers_14_input_layernorm_1__196", %"tmp_1__198")
    1599 |  # n6__198
            %"const__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1600 |  # n7__198
            %"tmp_2__198"<?,?> ⬅️ ::Squeeze(%"model.layers.14.self_attn.v_proj.weight", %"const__198")
    1601 |  # n8__198
            %"int64_0_1d__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1602 |  # n9__198
            %"cos_sin_gather_size__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1603 |  # n10__198
            %"int64_1_1d__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1604 |  # n11__198
            %"cos__198"<?,?> ⬅️ ::Slice(%"tmp_2__198", %"int64_0_1d__198", %"cos_sin_gather_size__198", %"int64_1_1d__198")
    1605 |  # n12__198
            %"const_3__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1606 |  # n13__198
            %"tmp_4__198"<?,?> ⬅️ ::Squeeze(%"model.layers.14.self_attn.rotary_emb.inv_freq", %"const_3__198")
    1607 |  # n14__198
            %"int64_0_1d_5__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1608 |  # n15__198
            %"cos_sin_gather_size_6__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1609 |  # n16__198
            %"int64_1_1d_7__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1610 |  # n17__198
            %"sin__198"<?,?> ⬅️ ::Slice(%"tmp_4__198", %"int64_0_1d_5__198", %"cos_sin_gather_size_6__198", %"int64_1_1d_7__198")
    1611 |  # n18__198
            %"path__198"<?,?> ⬅️ ::Shape(%"value_states_14")
    1612 |  # n19__198
            %"int64_1__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1613 |  # n20__198
            %"path2__198"<?,?> ⬅️ ::Gather(%"path__198", %"int64_1__198") {axis=0}
    1614 |  # n21__198
            %"total_seq_lengths__198"<?,?> ⬅️ ::Cast(%"path2__198") {to=6}
    1615 |  # n22__198
            %"int64_1_1d_8__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1616 |  # n23__198
            %"temp__198"<?,?> ⬅️ ::ReduceSum(%"value_states_14", %"int64_1_1d_8__198")
    1617 |  # n24__198
            %"int64_1_1d_9__198"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1618 |  # n25__198
            %"int64_1_1d_9_cast__198"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__198", %"temp__198")
    1619 |  # n26__198
            %"temp2__198"<?,?> ⬅️ ::Sub(%"temp__198", %"int64_1_1d_9_cast__198")
    1620 |  # n27__198
            %"seqlens_k__198"<?,?> ⬅️ ::Cast(%"temp2__198") {to=6}
    1621 |  # n28__198
            %"gqa_output__198"<?,?>, %"model_1_31"<FLOAT,[s0,32,s1,128]>, %"model_1_30"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__198", %"k__198", %"v__198", %"l_position_ids_", %"key_states_14", %"seqlens_k__198", %"total_seq_lengths__198", %"cos__198", %"sin__198") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1622 |  # n29__198
            %"tmp_10__198"<?,?> ⬅️ ::Transpose(%"model.layers.14.self_attn.o_proj.weight") {perm=[1, 0]}
    1623 |  # n30__198
            %"model_layers_14_self_attn_1_2__196"<?,?> ⬅️ ::MatMul(%"gqa_output__198", %"tmp_10__198")
    1624 |  # n0__199
            %"alpha__199"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1625 |  # n1__199
            %"alpha_0__199"<?,?> ⬅️ ::CastLike(%"alpha__199", %"model_layers_14_self_attn_1_2__196")
    1626 |  # n2__199
            %"other_1__199"<?,?> ⬅️ ::Mul(%"model_layers_14_self_attn_1_2__196", %"alpha_0__199")
    1627 |  # n3__199
            %"add_104__196"<?,?> ⬅️ ::Add(%"model_layers_13_1_2__1", %"other_1__199")
    1628 |  # n0__200
            %"model_layers_14_post_attention_layernorm_1__196"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_104__196", %"model.layers.14.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1629 |  # n0__201
            %"gate_proj_weight_t__201"<?,?> ⬅️ ::Transpose(%"model.layers.14.mlp.gate_proj.weight") {perm=[1, 0]}
    1630 |  # n1__201
            %"up_proj_weight_t__201"<?,?> ⬅️ ::Transpose(%"model.layers.14.mlp.up_proj.weight") {perm=[1, 0]}
    1631 |  # n2__201
            %"down_proj_weight_t__201"<?,?> ⬅️ ::Transpose(%"model.layers.14.mlp.down_proj.weight") {perm=[1, 0]}
    1632 |  # n3__201
            %"gate_proj_output__201"<?,?> ⬅️ ::MatMul(%"model_layers_14_post_attention_layernorm_1__196", %"gate_proj_weight_t__201")
    1633 |  # n4__201
            %"up_proj_output__201"<?,?> ⬅️ ::MatMul(%"model_layers_14_post_attention_layernorm_1__196", %"up_proj_weight_t__201")
    1634 |  # n5__201
            %"gate_times_up_output__201"<?,?> ⬅️ ::Mul(%"gate_proj_output__201", %"up_proj_output__201")
    1635 |  # n6__201
            %"act_fn_output__201"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__201")
    1636 |  # n7__201
            %"model_layers_14_mlp_1__196"<?,?> ⬅️ ::MatMul(%"act_fn_output__201", %"down_proj_weight_t__201")
    1637 |  # n0__202
            %"alpha__202"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1638 |  # n1__202
            %"alpha_0__202"<?,?> ⬅️ ::CastLike(%"alpha__202", %"model_layers_14_mlp_1__196")
    1639 |  # n2__202
            %"other_1__202"<?,?> ⬅️ ::Mul(%"model_layers_14_mlp_1__196", %"alpha_0__202")
    1640 |  # n3__202
            %"model_layers_14_1_2__1"<?,?> ⬅️ ::Add(%"add_104__196", %"other_1__202")
    1641 |  # n0__204
            %"model_layers_15_input_layernorm_1__203"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_14_1_2__1", %"model.layers.15.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1642 |  # n0__205
            %"tmp__205"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1643 |  # n1__205
            %"q__205"<?,?> ⬅️ ::MatMul(%"model_layers_15_input_layernorm_1__203", %"tmp__205")
    1644 |  # n2__205
            %"tmp_0__205"<?,?> ⬅️ ::Transpose(%"model.layers.15.self_attn.q_proj.weight") {perm=[1, 0]}
    1645 |  # n3__205
            %"k__205"<?,?> ⬅️ ::MatMul(%"model_layers_15_input_layernorm_1__203", %"tmp_0__205")
    1646 |  # n4__205
            %"tmp_1__205"<?,?> ⬅️ ::Transpose(%"model.layers.15.self_attn.k_proj.weight") {perm=[1, 0]}
    1647 |  # n5__205
            %"v__205"<?,?> ⬅️ ::MatMul(%"model_layers_15_input_layernorm_1__203", %"tmp_1__205")
    1648 |  # n6__205
            %"const__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1649 |  # n7__205
            %"tmp_2__205"<?,?> ⬅️ ::Squeeze(%"model.layers.15.self_attn.v_proj.weight", %"const__205")
    1650 |  # n8__205
            %"int64_0_1d__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1651 |  # n9__205
            %"cos_sin_gather_size__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1652 |  # n10__205
            %"int64_1_1d__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1653 |  # n11__205
            %"cos__205"<?,?> ⬅️ ::Slice(%"tmp_2__205", %"int64_0_1d__205", %"cos_sin_gather_size__205", %"int64_1_1d__205")
    1654 |  # n12__205
            %"const_3__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1655 |  # n13__205
            %"tmp_4__205"<?,?> ⬅️ ::Squeeze(%"model.layers.15.self_attn.rotary_emb.inv_freq", %"const_3__205")
    1656 |  # n14__205
            %"int64_0_1d_5__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1657 |  # n15__205
            %"cos_sin_gather_size_6__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1658 |  # n16__205
            %"int64_1_1d_7__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1659 |  # n17__205
            %"sin__205"<?,?> ⬅️ ::Slice(%"tmp_4__205", %"int64_0_1d_5__205", %"cos_sin_gather_size_6__205", %"int64_1_1d_7__205")
    1660 |  # n18__205
            %"path__205"<?,?> ⬅️ ::Shape(%"value_states_15")
    1661 |  # n19__205
            %"int64_1__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1662 |  # n20__205
            %"path2__205"<?,?> ⬅️ ::Gather(%"path__205", %"int64_1__205") {axis=0}
    1663 |  # n21__205
            %"total_seq_lengths__205"<?,?> ⬅️ ::Cast(%"path2__205") {to=6}
    1664 |  # n22__205
            %"int64_1_1d_8__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1665 |  # n23__205
            %"temp__205"<?,?> ⬅️ ::ReduceSum(%"value_states_15", %"int64_1_1d_8__205")
    1666 |  # n24__205
            %"int64_1_1d_9__205"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1667 |  # n25__205
            %"int64_1_1d_9_cast__205"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__205", %"temp__205")
    1668 |  # n26__205
            %"temp2__205"<?,?> ⬅️ ::Sub(%"temp__205", %"int64_1_1d_9_cast__205")
    1669 |  # n27__205
            %"seqlens_k__205"<?,?> ⬅️ ::Cast(%"temp2__205") {to=6}
    1670 |  # n28__205
            %"gqa_output__205"<?,?>, %"model_1_33"<FLOAT,[s0,32,s1,128]>, %"model_1_32"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__205", %"k__205", %"v__205", %"l_position_ids_", %"key_states_15", %"seqlens_k__205", %"total_seq_lengths__205", %"cos__205", %"sin__205") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1671 |  # n29__205
            %"tmp_10__205"<?,?> ⬅️ ::Transpose(%"model.layers.15.self_attn.o_proj.weight") {perm=[1, 0]}
    1672 |  # n30__205
            %"model_layers_15_self_attn_1_2__203"<?,?> ⬅️ ::MatMul(%"gqa_output__205", %"tmp_10__205")
    1673 |  # n0__206
            %"alpha__206"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1674 |  # n1__206
            %"alpha_0__206"<?,?> ⬅️ ::CastLike(%"alpha__206", %"model_layers_15_self_attn_1_2__203")
    1675 |  # n2__206
            %"other_1__206"<?,?> ⬅️ ::Mul(%"model_layers_15_self_attn_1_2__203", %"alpha_0__206")
    1676 |  # n3__206
            %"add_111__203"<?,?> ⬅️ ::Add(%"model_layers_14_1_2__1", %"other_1__206")
    1677 |  # n0__207
            %"model_layers_15_post_attention_layernorm_1__203"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_111__203", %"model.layers.15.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1678 |  # n0__208
            %"gate_proj_weight_t__208"<?,?> ⬅️ ::Transpose(%"model.layers.15.mlp.gate_proj.weight") {perm=[1, 0]}
    1679 |  # n1__208
            %"up_proj_weight_t__208"<?,?> ⬅️ ::Transpose(%"model.layers.15.mlp.up_proj.weight") {perm=[1, 0]}
    1680 |  # n2__208
            %"down_proj_weight_t__208"<?,?> ⬅️ ::Transpose(%"model.layers.15.mlp.down_proj.weight") {perm=[1, 0]}
    1681 |  # n3__208
            %"gate_proj_output__208"<?,?> ⬅️ ::MatMul(%"model_layers_15_post_attention_layernorm_1__203", %"gate_proj_weight_t__208")
    1682 |  # n4__208
            %"up_proj_output__208"<?,?> ⬅️ ::MatMul(%"model_layers_15_post_attention_layernorm_1__203", %"up_proj_weight_t__208")
    1683 |  # n5__208
            %"gate_times_up_output__208"<?,?> ⬅️ ::Mul(%"gate_proj_output__208", %"up_proj_output__208")
    1684 |  # n6__208
            %"act_fn_output__208"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__208")
    1685 |  # n7__208
            %"model_layers_15_mlp_1__203"<?,?> ⬅️ ::MatMul(%"act_fn_output__208", %"down_proj_weight_t__208")
    1686 |  # n0__209
            %"alpha__209"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1687 |  # n1__209
            %"alpha_0__209"<?,?> ⬅️ ::CastLike(%"alpha__209", %"model_layers_15_mlp_1__203")
    1688 |  # n2__209
            %"other_1__209"<?,?> ⬅️ ::Mul(%"model_layers_15_mlp_1__203", %"alpha_0__209")
    1689 |  # n3__209
            %"model_layers_15_1_2__1"<?,?> ⬅️ ::Add(%"add_111__203", %"other_1__209")
    1690 |  # n0__211
            %"model_layers_16_input_layernorm_1__210"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_15_1_2__1", %"model.layers.16.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1691 |  # n0__212
            %"tmp__212"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1692 |  # n1__212
            %"q__212"<?,?> ⬅️ ::MatMul(%"model_layers_16_input_layernorm_1__210", %"tmp__212")
    1693 |  # n2__212
            %"tmp_0__212"<?,?> ⬅️ ::Transpose(%"model.layers.16.self_attn.q_proj.weight") {perm=[1, 0]}
    1694 |  # n3__212
            %"k__212"<?,?> ⬅️ ::MatMul(%"model_layers_16_input_layernorm_1__210", %"tmp_0__212")
    1695 |  # n4__212
            %"tmp_1__212"<?,?> ⬅️ ::Transpose(%"model.layers.16.self_attn.k_proj.weight") {perm=[1, 0]}
    1696 |  # n5__212
            %"v__212"<?,?> ⬅️ ::MatMul(%"model_layers_16_input_layernorm_1__210", %"tmp_1__212")
    1697 |  # n6__212
            %"const__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1698 |  # n7__212
            %"tmp_2__212"<?,?> ⬅️ ::Squeeze(%"model.layers.16.self_attn.v_proj.weight", %"const__212")
    1699 |  # n8__212
            %"int64_0_1d__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1700 |  # n9__212
            %"cos_sin_gather_size__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1701 |  # n10__212
            %"int64_1_1d__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1702 |  # n11__212
            %"cos__212"<?,?> ⬅️ ::Slice(%"tmp_2__212", %"int64_0_1d__212", %"cos_sin_gather_size__212", %"int64_1_1d__212")
    1703 |  # n12__212
            %"const_3__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1704 |  # n13__212
            %"tmp_4__212"<?,?> ⬅️ ::Squeeze(%"model.layers.16.self_attn.rotary_emb.inv_freq", %"const_3__212")
    1705 |  # n14__212
            %"int64_0_1d_5__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1706 |  # n15__212
            %"cos_sin_gather_size_6__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1707 |  # n16__212
            %"int64_1_1d_7__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1708 |  # n17__212
            %"sin__212"<?,?> ⬅️ ::Slice(%"tmp_4__212", %"int64_0_1d_5__212", %"cos_sin_gather_size_6__212", %"int64_1_1d_7__212")
    1709 |  # n18__212
            %"path__212"<?,?> ⬅️ ::Shape(%"value_states_16")
    1710 |  # n19__212
            %"int64_1__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1711 |  # n20__212
            %"path2__212"<?,?> ⬅️ ::Gather(%"path__212", %"int64_1__212") {axis=0}
    1712 |  # n21__212
            %"total_seq_lengths__212"<?,?> ⬅️ ::Cast(%"path2__212") {to=6}
    1713 |  # n22__212
            %"int64_1_1d_8__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1714 |  # n23__212
            %"temp__212"<?,?> ⬅️ ::ReduceSum(%"value_states_16", %"int64_1_1d_8__212")
    1715 |  # n24__212
            %"int64_1_1d_9__212"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1716 |  # n25__212
            %"int64_1_1d_9_cast__212"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__212", %"temp__212")
    1717 |  # n26__212
            %"temp2__212"<?,?> ⬅️ ::Sub(%"temp__212", %"int64_1_1d_9_cast__212")
    1718 |  # n27__212
            %"seqlens_k__212"<?,?> ⬅️ ::Cast(%"temp2__212") {to=6}
    1719 |  # n28__212
            %"gqa_output__212"<?,?>, %"model_1_35"<FLOAT,[s0,32,s1,128]>, %"model_1_34"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__212", %"k__212", %"v__212", %"l_position_ids_", %"key_states_16", %"seqlens_k__212", %"total_seq_lengths__212", %"cos__212", %"sin__212") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1720 |  # n29__212
            %"tmp_10__212"<?,?> ⬅️ ::Transpose(%"model.layers.16.self_attn.o_proj.weight") {perm=[1, 0]}
    1721 |  # n30__212
            %"model_layers_16_self_attn_1_2__210"<?,?> ⬅️ ::MatMul(%"gqa_output__212", %"tmp_10__212")
    1722 |  # n0__213
            %"alpha__213"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1723 |  # n1__213
            %"alpha_0__213"<?,?> ⬅️ ::CastLike(%"alpha__213", %"model_layers_16_self_attn_1_2__210")
    1724 |  # n2__213
            %"other_1__213"<?,?> ⬅️ ::Mul(%"model_layers_16_self_attn_1_2__210", %"alpha_0__213")
    1725 |  # n3__213
            %"add_118__210"<?,?> ⬅️ ::Add(%"model_layers_15_1_2__1", %"other_1__213")
    1726 |  # n0__214
            %"model_layers_16_post_attention_layernorm_1__210"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_118__210", %"model.layers.16.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1727 |  # n0__215
            %"gate_proj_weight_t__215"<?,?> ⬅️ ::Transpose(%"model.layers.16.mlp.gate_proj.weight") {perm=[1, 0]}
    1728 |  # n1__215
            %"up_proj_weight_t__215"<?,?> ⬅️ ::Transpose(%"model.layers.16.mlp.up_proj.weight") {perm=[1, 0]}
    1729 |  # n2__215
            %"down_proj_weight_t__215"<?,?> ⬅️ ::Transpose(%"model.layers.16.mlp.down_proj.weight") {perm=[1, 0]}
    1730 |  # n3__215
            %"gate_proj_output__215"<?,?> ⬅️ ::MatMul(%"model_layers_16_post_attention_layernorm_1__210", %"gate_proj_weight_t__215")
    1731 |  # n4__215
            %"up_proj_output__215"<?,?> ⬅️ ::MatMul(%"model_layers_16_post_attention_layernorm_1__210", %"up_proj_weight_t__215")
    1732 |  # n5__215
            %"gate_times_up_output__215"<?,?> ⬅️ ::Mul(%"gate_proj_output__215", %"up_proj_output__215")
    1733 |  # n6__215
            %"act_fn_output__215"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__215")
    1734 |  # n7__215
            %"model_layers_16_mlp_1__210"<?,?> ⬅️ ::MatMul(%"act_fn_output__215", %"down_proj_weight_t__215")
    1735 |  # n0__216
            %"alpha__216"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1736 |  # n1__216
            %"alpha_0__216"<?,?> ⬅️ ::CastLike(%"alpha__216", %"model_layers_16_mlp_1__210")
    1737 |  # n2__216
            %"other_1__216"<?,?> ⬅️ ::Mul(%"model_layers_16_mlp_1__210", %"alpha_0__216")
    1738 |  # n3__216
            %"model_layers_16_1_2__1"<?,?> ⬅️ ::Add(%"add_118__210", %"other_1__216")
    1739 |  # n0__218
            %"model_layers_17_input_layernorm_1__217"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_16_1_2__1", %"model.layers.17.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1740 |  # n0__219
            %"tmp__219"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1741 |  # n1__219
            %"q__219"<?,?> ⬅️ ::MatMul(%"model_layers_17_input_layernorm_1__217", %"tmp__219")
    1742 |  # n2__219
            %"tmp_0__219"<?,?> ⬅️ ::Transpose(%"model.layers.17.self_attn.q_proj.weight") {perm=[1, 0]}
    1743 |  # n3__219
            %"k__219"<?,?> ⬅️ ::MatMul(%"model_layers_17_input_layernorm_1__217", %"tmp_0__219")
    1744 |  # n4__219
            %"tmp_1__219"<?,?> ⬅️ ::Transpose(%"model.layers.17.self_attn.k_proj.weight") {perm=[1, 0]}
    1745 |  # n5__219
            %"v__219"<?,?> ⬅️ ::MatMul(%"model_layers_17_input_layernorm_1__217", %"tmp_1__219")
    1746 |  # n6__219
            %"const__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1747 |  # n7__219
            %"tmp_2__219"<?,?> ⬅️ ::Squeeze(%"model.layers.17.self_attn.v_proj.weight", %"const__219")
    1748 |  # n8__219
            %"int64_0_1d__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1749 |  # n9__219
            %"cos_sin_gather_size__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1750 |  # n10__219
            %"int64_1_1d__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1751 |  # n11__219
            %"cos__219"<?,?> ⬅️ ::Slice(%"tmp_2__219", %"int64_0_1d__219", %"cos_sin_gather_size__219", %"int64_1_1d__219")
    1752 |  # n12__219
            %"const_3__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1753 |  # n13__219
            %"tmp_4__219"<?,?> ⬅️ ::Squeeze(%"model.layers.17.self_attn.rotary_emb.inv_freq", %"const_3__219")
    1754 |  # n14__219
            %"int64_0_1d_5__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1755 |  # n15__219
            %"cos_sin_gather_size_6__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1756 |  # n16__219
            %"int64_1_1d_7__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1757 |  # n17__219
            %"sin__219"<?,?> ⬅️ ::Slice(%"tmp_4__219", %"int64_0_1d_5__219", %"cos_sin_gather_size_6__219", %"int64_1_1d_7__219")
    1758 |  # n18__219
            %"path__219"<?,?> ⬅️ ::Shape(%"value_states_17")
    1759 |  # n19__219
            %"int64_1__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1760 |  # n20__219
            %"path2__219"<?,?> ⬅️ ::Gather(%"path__219", %"int64_1__219") {axis=0}
    1761 |  # n21__219
            %"total_seq_lengths__219"<?,?> ⬅️ ::Cast(%"path2__219") {to=6}
    1762 |  # n22__219
            %"int64_1_1d_8__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1763 |  # n23__219
            %"temp__219"<?,?> ⬅️ ::ReduceSum(%"value_states_17", %"int64_1_1d_8__219")
    1764 |  # n24__219
            %"int64_1_1d_9__219"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1765 |  # n25__219
            %"int64_1_1d_9_cast__219"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__219", %"temp__219")
    1766 |  # n26__219
            %"temp2__219"<?,?> ⬅️ ::Sub(%"temp__219", %"int64_1_1d_9_cast__219")
    1767 |  # n27__219
            %"seqlens_k__219"<?,?> ⬅️ ::Cast(%"temp2__219") {to=6}
    1768 |  # n28__219
            %"gqa_output__219"<?,?>, %"model_1_37"<FLOAT,[s0,32,s1,128]>, %"model_1_36"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__219", %"k__219", %"v__219", %"l_position_ids_", %"key_states_17", %"seqlens_k__219", %"total_seq_lengths__219", %"cos__219", %"sin__219") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1769 |  # n29__219
            %"tmp_10__219"<?,?> ⬅️ ::Transpose(%"model.layers.17.self_attn.o_proj.weight") {perm=[1, 0]}
    1770 |  # n30__219
            %"model_layers_17_self_attn_1_2__217"<?,?> ⬅️ ::MatMul(%"gqa_output__219", %"tmp_10__219")
    1771 |  # n0__220
            %"alpha__220"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1772 |  # n1__220
            %"alpha_0__220"<?,?> ⬅️ ::CastLike(%"alpha__220", %"model_layers_17_self_attn_1_2__217")
    1773 |  # n2__220
            %"other_1__220"<?,?> ⬅️ ::Mul(%"model_layers_17_self_attn_1_2__217", %"alpha_0__220")
    1774 |  # n3__220
            %"add_125__217"<?,?> ⬅️ ::Add(%"model_layers_16_1_2__1", %"other_1__220")
    1775 |  # n0__221
            %"model_layers_17_post_attention_layernorm_1__217"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_125__217", %"model.layers.17.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1776 |  # n0__222
            %"gate_proj_weight_t__222"<?,?> ⬅️ ::Transpose(%"model.layers.17.mlp.gate_proj.weight") {perm=[1, 0]}
    1777 |  # n1__222
            %"up_proj_weight_t__222"<?,?> ⬅️ ::Transpose(%"model.layers.17.mlp.up_proj.weight") {perm=[1, 0]}
    1778 |  # n2__222
            %"down_proj_weight_t__222"<?,?> ⬅️ ::Transpose(%"model.layers.17.mlp.down_proj.weight") {perm=[1, 0]}
    1779 |  # n3__222
            %"gate_proj_output__222"<?,?> ⬅️ ::MatMul(%"model_layers_17_post_attention_layernorm_1__217", %"gate_proj_weight_t__222")
    1780 |  # n4__222
            %"up_proj_output__222"<?,?> ⬅️ ::MatMul(%"model_layers_17_post_attention_layernorm_1__217", %"up_proj_weight_t__222")
    1781 |  # n5__222
            %"gate_times_up_output__222"<?,?> ⬅️ ::Mul(%"gate_proj_output__222", %"up_proj_output__222")
    1782 |  # n6__222
            %"act_fn_output__222"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__222")
    1783 |  # n7__222
            %"model_layers_17_mlp_1__217"<?,?> ⬅️ ::MatMul(%"act_fn_output__222", %"down_proj_weight_t__222")
    1784 |  # n0__223
            %"alpha__223"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1785 |  # n1__223
            %"alpha_0__223"<?,?> ⬅️ ::CastLike(%"alpha__223", %"model_layers_17_mlp_1__217")
    1786 |  # n2__223
            %"other_1__223"<?,?> ⬅️ ::Mul(%"model_layers_17_mlp_1__217", %"alpha_0__223")
    1787 |  # n3__223
            %"model_layers_17_1_2__1"<?,?> ⬅️ ::Add(%"add_125__217", %"other_1__223")
    1788 |  # n0__225
            %"model_layers_18_input_layernorm_1__224"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_17_1_2__1", %"model.layers.18.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1789 |  # n0__226
            %"tmp__226"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1790 |  # n1__226
            %"q__226"<?,?> ⬅️ ::MatMul(%"model_layers_18_input_layernorm_1__224", %"tmp__226")
    1791 |  # n2__226
            %"tmp_0__226"<?,?> ⬅️ ::Transpose(%"model.layers.18.self_attn.q_proj.weight") {perm=[1, 0]}
    1792 |  # n3__226
            %"k__226"<?,?> ⬅️ ::MatMul(%"model_layers_18_input_layernorm_1__224", %"tmp_0__226")
    1793 |  # n4__226
            %"tmp_1__226"<?,?> ⬅️ ::Transpose(%"model.layers.18.self_attn.k_proj.weight") {perm=[1, 0]}
    1794 |  # n5__226
            %"v__226"<?,?> ⬅️ ::MatMul(%"model_layers_18_input_layernorm_1__224", %"tmp_1__226")
    1795 |  # n6__226
            %"const__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1796 |  # n7__226
            %"tmp_2__226"<?,?> ⬅️ ::Squeeze(%"model.layers.18.self_attn.v_proj.weight", %"const__226")
    1797 |  # n8__226
            %"int64_0_1d__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1798 |  # n9__226
            %"cos_sin_gather_size__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1799 |  # n10__226
            %"int64_1_1d__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1800 |  # n11__226
            %"cos__226"<?,?> ⬅️ ::Slice(%"tmp_2__226", %"int64_0_1d__226", %"cos_sin_gather_size__226", %"int64_1_1d__226")
    1801 |  # n12__226
            %"const_3__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1802 |  # n13__226
            %"tmp_4__226"<?,?> ⬅️ ::Squeeze(%"model.layers.18.self_attn.rotary_emb.inv_freq", %"const_3__226")
    1803 |  # n14__226
            %"int64_0_1d_5__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1804 |  # n15__226
            %"cos_sin_gather_size_6__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1805 |  # n16__226
            %"int64_1_1d_7__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1806 |  # n17__226
            %"sin__226"<?,?> ⬅️ ::Slice(%"tmp_4__226", %"int64_0_1d_5__226", %"cos_sin_gather_size_6__226", %"int64_1_1d_7__226")
    1807 |  # n18__226
            %"path__226"<?,?> ⬅️ ::Shape(%"value_states_18")
    1808 |  # n19__226
            %"int64_1__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1809 |  # n20__226
            %"path2__226"<?,?> ⬅️ ::Gather(%"path__226", %"int64_1__226") {axis=0}
    1810 |  # n21__226
            %"total_seq_lengths__226"<?,?> ⬅️ ::Cast(%"path2__226") {to=6}
    1811 |  # n22__226
            %"int64_1_1d_8__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1812 |  # n23__226
            %"temp__226"<?,?> ⬅️ ::ReduceSum(%"value_states_18", %"int64_1_1d_8__226")
    1813 |  # n24__226
            %"int64_1_1d_9__226"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1814 |  # n25__226
            %"int64_1_1d_9_cast__226"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__226", %"temp__226")
    1815 |  # n26__226
            %"temp2__226"<?,?> ⬅️ ::Sub(%"temp__226", %"int64_1_1d_9_cast__226")
    1816 |  # n27__226
            %"seqlens_k__226"<?,?> ⬅️ ::Cast(%"temp2__226") {to=6}
    1817 |  # n28__226
            %"gqa_output__226"<?,?>, %"model_1_39"<FLOAT,[s0,32,s1,128]>, %"model_1_38"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__226", %"k__226", %"v__226", %"l_position_ids_", %"key_states_18", %"seqlens_k__226", %"total_seq_lengths__226", %"cos__226", %"sin__226") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1818 |  # n29__226
            %"tmp_10__226"<?,?> ⬅️ ::Transpose(%"model.layers.18.self_attn.o_proj.weight") {perm=[1, 0]}
    1819 |  # n30__226
            %"model_layers_18_self_attn_1_2__224"<?,?> ⬅️ ::MatMul(%"gqa_output__226", %"tmp_10__226")
    1820 |  # n0__227
            %"alpha__227"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1821 |  # n1__227
            %"alpha_0__227"<?,?> ⬅️ ::CastLike(%"alpha__227", %"model_layers_18_self_attn_1_2__224")
    1822 |  # n2__227
            %"other_1__227"<?,?> ⬅️ ::Mul(%"model_layers_18_self_attn_1_2__224", %"alpha_0__227")
    1823 |  # n3__227
            %"add_132__224"<?,?> ⬅️ ::Add(%"model_layers_17_1_2__1", %"other_1__227")
    1824 |  # n0__228
            %"model_layers_18_post_attention_layernorm_1__224"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_132__224", %"model.layers.18.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1825 |  # n0__229
            %"gate_proj_weight_t__229"<?,?> ⬅️ ::Transpose(%"model.layers.18.mlp.gate_proj.weight") {perm=[1, 0]}
    1826 |  # n1__229
            %"up_proj_weight_t__229"<?,?> ⬅️ ::Transpose(%"model.layers.18.mlp.up_proj.weight") {perm=[1, 0]}
    1827 |  # n2__229
            %"down_proj_weight_t__229"<?,?> ⬅️ ::Transpose(%"model.layers.18.mlp.down_proj.weight") {perm=[1, 0]}
    1828 |  # n3__229
            %"gate_proj_output__229"<?,?> ⬅️ ::MatMul(%"model_layers_18_post_attention_layernorm_1__224", %"gate_proj_weight_t__229")
    1829 |  # n4__229
            %"up_proj_output__229"<?,?> ⬅️ ::MatMul(%"model_layers_18_post_attention_layernorm_1__224", %"up_proj_weight_t__229")
    1830 |  # n5__229
            %"gate_times_up_output__229"<?,?> ⬅️ ::Mul(%"gate_proj_output__229", %"up_proj_output__229")
    1831 |  # n6__229
            %"act_fn_output__229"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__229")
    1832 |  # n7__229
            %"model_layers_18_mlp_1__224"<?,?> ⬅️ ::MatMul(%"act_fn_output__229", %"down_proj_weight_t__229")
    1833 |  # n0__230
            %"alpha__230"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1834 |  # n1__230
            %"alpha_0__230"<?,?> ⬅️ ::CastLike(%"alpha__230", %"model_layers_18_mlp_1__224")
    1835 |  # n2__230
            %"other_1__230"<?,?> ⬅️ ::Mul(%"model_layers_18_mlp_1__224", %"alpha_0__230")
    1836 |  # n3__230
            %"model_layers_18_1_2__1"<?,?> ⬅️ ::Add(%"add_132__224", %"other_1__230")
    1837 |  # n0__232
            %"model_layers_19_input_layernorm_1__231"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_18_1_2__1", %"model.layers.19.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1838 |  # n0__233
            %"tmp__233"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1839 |  # n1__233
            %"q__233"<?,?> ⬅️ ::MatMul(%"model_layers_19_input_layernorm_1__231", %"tmp__233")
    1840 |  # n2__233
            %"tmp_0__233"<?,?> ⬅️ ::Transpose(%"model.layers.19.self_attn.q_proj.weight") {perm=[1, 0]}
    1841 |  # n3__233
            %"k__233"<?,?> ⬅️ ::MatMul(%"model_layers_19_input_layernorm_1__231", %"tmp_0__233")
    1842 |  # n4__233
            %"tmp_1__233"<?,?> ⬅️ ::Transpose(%"model.layers.19.self_attn.k_proj.weight") {perm=[1, 0]}
    1843 |  # n5__233
            %"v__233"<?,?> ⬅️ ::MatMul(%"model_layers_19_input_layernorm_1__231", %"tmp_1__233")
    1844 |  # n6__233
            %"const__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1845 |  # n7__233
            %"tmp_2__233"<?,?> ⬅️ ::Squeeze(%"model.layers.19.self_attn.v_proj.weight", %"const__233")
    1846 |  # n8__233
            %"int64_0_1d__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1847 |  # n9__233
            %"cos_sin_gather_size__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1848 |  # n10__233
            %"int64_1_1d__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1849 |  # n11__233
            %"cos__233"<?,?> ⬅️ ::Slice(%"tmp_2__233", %"int64_0_1d__233", %"cos_sin_gather_size__233", %"int64_1_1d__233")
    1850 |  # n12__233
            %"const_3__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1851 |  # n13__233
            %"tmp_4__233"<?,?> ⬅️ ::Squeeze(%"model.layers.19.self_attn.rotary_emb.inv_freq", %"const_3__233")
    1852 |  # n14__233
            %"int64_0_1d_5__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1853 |  # n15__233
            %"cos_sin_gather_size_6__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1854 |  # n16__233
            %"int64_1_1d_7__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1855 |  # n17__233
            %"sin__233"<?,?> ⬅️ ::Slice(%"tmp_4__233", %"int64_0_1d_5__233", %"cos_sin_gather_size_6__233", %"int64_1_1d_7__233")
    1856 |  # n18__233
            %"path__233"<?,?> ⬅️ ::Shape(%"value_states_19")
    1857 |  # n19__233
            %"int64_1__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1858 |  # n20__233
            %"path2__233"<?,?> ⬅️ ::Gather(%"path__233", %"int64_1__233") {axis=0}
    1859 |  # n21__233
            %"total_seq_lengths__233"<?,?> ⬅️ ::Cast(%"path2__233") {to=6}
    1860 |  # n22__233
            %"int64_1_1d_8__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1861 |  # n23__233
            %"temp__233"<?,?> ⬅️ ::ReduceSum(%"value_states_19", %"int64_1_1d_8__233")
    1862 |  # n24__233
            %"int64_1_1d_9__233"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1863 |  # n25__233
            %"int64_1_1d_9_cast__233"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__233", %"temp__233")
    1864 |  # n26__233
            %"temp2__233"<?,?> ⬅️ ::Sub(%"temp__233", %"int64_1_1d_9_cast__233")
    1865 |  # n27__233
            %"seqlens_k__233"<?,?> ⬅️ ::Cast(%"temp2__233") {to=6}
    1866 |  # n28__233
            %"gqa_output__233"<?,?>, %"model_1_41"<FLOAT,[s0,32,s1,128]>, %"model_1_40"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__233", %"k__233", %"v__233", %"l_position_ids_", %"key_states_19", %"seqlens_k__233", %"total_seq_lengths__233", %"cos__233", %"sin__233") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1867 |  # n29__233
            %"tmp_10__233"<?,?> ⬅️ ::Transpose(%"model.layers.19.self_attn.o_proj.weight") {perm=[1, 0]}
    1868 |  # n30__233
            %"model_layers_19_self_attn_1_2__231"<?,?> ⬅️ ::MatMul(%"gqa_output__233", %"tmp_10__233")
    1869 |  # n0__234
            %"alpha__234"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1870 |  # n1__234
            %"alpha_0__234"<?,?> ⬅️ ::CastLike(%"alpha__234", %"model_layers_19_self_attn_1_2__231")
    1871 |  # n2__234
            %"other_1__234"<?,?> ⬅️ ::Mul(%"model_layers_19_self_attn_1_2__231", %"alpha_0__234")
    1872 |  # n3__234
            %"add_139__231"<?,?> ⬅️ ::Add(%"model_layers_18_1_2__1", %"other_1__234")
    1873 |  # n0__235
            %"model_layers_19_post_attention_layernorm_1__231"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_139__231", %"model.layers.19.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1874 |  # n0__236
            %"gate_proj_weight_t__236"<?,?> ⬅️ ::Transpose(%"model.layers.19.mlp.gate_proj.weight") {perm=[1, 0]}
    1875 |  # n1__236
            %"up_proj_weight_t__236"<?,?> ⬅️ ::Transpose(%"model.layers.19.mlp.up_proj.weight") {perm=[1, 0]}
    1876 |  # n2__236
            %"down_proj_weight_t__236"<?,?> ⬅️ ::Transpose(%"model.layers.19.mlp.down_proj.weight") {perm=[1, 0]}
    1877 |  # n3__236
            %"gate_proj_output__236"<?,?> ⬅️ ::MatMul(%"model_layers_19_post_attention_layernorm_1__231", %"gate_proj_weight_t__236")
    1878 |  # n4__236
            %"up_proj_output__236"<?,?> ⬅️ ::MatMul(%"model_layers_19_post_attention_layernorm_1__231", %"up_proj_weight_t__236")
    1879 |  # n5__236
            %"gate_times_up_output__236"<?,?> ⬅️ ::Mul(%"gate_proj_output__236", %"up_proj_output__236")
    1880 |  # n6__236
            %"act_fn_output__236"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__236")
    1881 |  # n7__236
            %"model_layers_19_mlp_1__231"<?,?> ⬅️ ::MatMul(%"act_fn_output__236", %"down_proj_weight_t__236")
    1882 |  # n0__237
            %"alpha__237"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1883 |  # n1__237
            %"alpha_0__237"<?,?> ⬅️ ::CastLike(%"alpha__237", %"model_layers_19_mlp_1__231")
    1884 |  # n2__237
            %"other_1__237"<?,?> ⬅️ ::Mul(%"model_layers_19_mlp_1__231", %"alpha_0__237")
    1885 |  # n3__237
            %"model_layers_19_1_2__1"<?,?> ⬅️ ::Add(%"add_139__231", %"other_1__237")
    1886 |  # n0__239
            %"model_layers_20_input_layernorm_1__238"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_19_1_2__1", %"model.layers.20.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1887 |  # n0__240
            %"tmp__240"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1888 |  # n1__240
            %"q__240"<?,?> ⬅️ ::MatMul(%"model_layers_20_input_layernorm_1__238", %"tmp__240")
    1889 |  # n2__240
            %"tmp_0__240"<?,?> ⬅️ ::Transpose(%"model.layers.20.self_attn.q_proj.weight") {perm=[1, 0]}
    1890 |  # n3__240
            %"k__240"<?,?> ⬅️ ::MatMul(%"model_layers_20_input_layernorm_1__238", %"tmp_0__240")
    1891 |  # n4__240
            %"tmp_1__240"<?,?> ⬅️ ::Transpose(%"model.layers.20.self_attn.k_proj.weight") {perm=[1, 0]}
    1892 |  # n5__240
            %"v__240"<?,?> ⬅️ ::MatMul(%"model_layers_20_input_layernorm_1__238", %"tmp_1__240")
    1893 |  # n6__240
            %"const__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1894 |  # n7__240
            %"tmp_2__240"<?,?> ⬅️ ::Squeeze(%"model.layers.20.self_attn.v_proj.weight", %"const__240")
    1895 |  # n8__240
            %"int64_0_1d__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1896 |  # n9__240
            %"cos_sin_gather_size__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1897 |  # n10__240
            %"int64_1_1d__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1898 |  # n11__240
            %"cos__240"<?,?> ⬅️ ::Slice(%"tmp_2__240", %"int64_0_1d__240", %"cos_sin_gather_size__240", %"int64_1_1d__240")
    1899 |  # n12__240
            %"const_3__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1900 |  # n13__240
            %"tmp_4__240"<?,?> ⬅️ ::Squeeze(%"model.layers.20.self_attn.rotary_emb.inv_freq", %"const_3__240")
    1901 |  # n14__240
            %"int64_0_1d_5__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1902 |  # n15__240
            %"cos_sin_gather_size_6__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1903 |  # n16__240
            %"int64_1_1d_7__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1904 |  # n17__240
            %"sin__240"<?,?> ⬅️ ::Slice(%"tmp_4__240", %"int64_0_1d_5__240", %"cos_sin_gather_size_6__240", %"int64_1_1d_7__240")
    1905 |  # n18__240
            %"path__240"<?,?> ⬅️ ::Shape(%"value_states_20")
    1906 |  # n19__240
            %"int64_1__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1907 |  # n20__240
            %"path2__240"<?,?> ⬅️ ::Gather(%"path__240", %"int64_1__240") {axis=0}
    1908 |  # n21__240
            %"total_seq_lengths__240"<?,?> ⬅️ ::Cast(%"path2__240") {to=6}
    1909 |  # n22__240
            %"int64_1_1d_8__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1910 |  # n23__240
            %"temp__240"<?,?> ⬅️ ::ReduceSum(%"value_states_20", %"int64_1_1d_8__240")
    1911 |  # n24__240
            %"int64_1_1d_9__240"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1912 |  # n25__240
            %"int64_1_1d_9_cast__240"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__240", %"temp__240")
    1913 |  # n26__240
            %"temp2__240"<?,?> ⬅️ ::Sub(%"temp__240", %"int64_1_1d_9_cast__240")
    1914 |  # n27__240
            %"seqlens_k__240"<?,?> ⬅️ ::Cast(%"temp2__240") {to=6}
    1915 |  # n28__240
            %"gqa_output__240"<?,?>, %"model_1_43"<FLOAT,[s0,32,s1,128]>, %"model_1_42"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__240", %"k__240", %"v__240", %"l_position_ids_", %"key_states_20", %"seqlens_k__240", %"total_seq_lengths__240", %"cos__240", %"sin__240") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1916 |  # n29__240
            %"tmp_10__240"<?,?> ⬅️ ::Transpose(%"model.layers.20.self_attn.o_proj.weight") {perm=[1, 0]}
    1917 |  # n30__240
            %"model_layers_20_self_attn_1_2__238"<?,?> ⬅️ ::MatMul(%"gqa_output__240", %"tmp_10__240")
    1918 |  # n0__241
            %"alpha__241"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1919 |  # n1__241
            %"alpha_0__241"<?,?> ⬅️ ::CastLike(%"alpha__241", %"model_layers_20_self_attn_1_2__238")
    1920 |  # n2__241
            %"other_1__241"<?,?> ⬅️ ::Mul(%"model_layers_20_self_attn_1_2__238", %"alpha_0__241")
    1921 |  # n3__241
            %"add_146__238"<?,?> ⬅️ ::Add(%"model_layers_19_1_2__1", %"other_1__241")
    1922 |  # n0__242
            %"model_layers_20_post_attention_layernorm_1__238"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_146__238", %"model.layers.20.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1923 |  # n0__243
            %"gate_proj_weight_t__243"<?,?> ⬅️ ::Transpose(%"model.layers.20.mlp.gate_proj.weight") {perm=[1, 0]}
    1924 |  # n1__243
            %"up_proj_weight_t__243"<?,?> ⬅️ ::Transpose(%"model.layers.20.mlp.up_proj.weight") {perm=[1, 0]}
    1925 |  # n2__243
            %"down_proj_weight_t__243"<?,?> ⬅️ ::Transpose(%"model.layers.20.mlp.down_proj.weight") {perm=[1, 0]}
    1926 |  # n3__243
            %"gate_proj_output__243"<?,?> ⬅️ ::MatMul(%"model_layers_20_post_attention_layernorm_1__238", %"gate_proj_weight_t__243")
    1927 |  # n4__243
            %"up_proj_output__243"<?,?> ⬅️ ::MatMul(%"model_layers_20_post_attention_layernorm_1__238", %"up_proj_weight_t__243")
    1928 |  # n5__243
            %"gate_times_up_output__243"<?,?> ⬅️ ::Mul(%"gate_proj_output__243", %"up_proj_output__243")
    1929 |  # n6__243
            %"act_fn_output__243"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__243")
    1930 |  # n7__243
            %"model_layers_20_mlp_1__238"<?,?> ⬅️ ::MatMul(%"act_fn_output__243", %"down_proj_weight_t__243")
    1931 |  # n0__244
            %"alpha__244"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1932 |  # n1__244
            %"alpha_0__244"<?,?> ⬅️ ::CastLike(%"alpha__244", %"model_layers_20_mlp_1__238")
    1933 |  # n2__244
            %"other_1__244"<?,?> ⬅️ ::Mul(%"model_layers_20_mlp_1__238", %"alpha_0__244")
    1934 |  # n3__244
            %"model_layers_20_1_2__1"<?,?> ⬅️ ::Add(%"add_146__238", %"other_1__244")
    1935 |  # n0__246
            %"model_layers_21_input_layernorm_1__245"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_20_1_2__1", %"model.layers.21.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1936 |  # n0__247
            %"tmp__247"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1937 |  # n1__247
            %"q__247"<?,?> ⬅️ ::MatMul(%"model_layers_21_input_layernorm_1__245", %"tmp__247")
    1938 |  # n2__247
            %"tmp_0__247"<?,?> ⬅️ ::Transpose(%"model.layers.21.self_attn.q_proj.weight") {perm=[1, 0]}
    1939 |  # n3__247
            %"k__247"<?,?> ⬅️ ::MatMul(%"model_layers_21_input_layernorm_1__245", %"tmp_0__247")
    1940 |  # n4__247
            %"tmp_1__247"<?,?> ⬅️ ::Transpose(%"model.layers.21.self_attn.k_proj.weight") {perm=[1, 0]}
    1941 |  # n5__247
            %"v__247"<?,?> ⬅️ ::MatMul(%"model_layers_21_input_layernorm_1__245", %"tmp_1__247")
    1942 |  # n6__247
            %"const__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1943 |  # n7__247
            %"tmp_2__247"<?,?> ⬅️ ::Squeeze(%"model.layers.21.self_attn.v_proj.weight", %"const__247")
    1944 |  # n8__247
            %"int64_0_1d__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1945 |  # n9__247
            %"cos_sin_gather_size__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1946 |  # n10__247
            %"int64_1_1d__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1947 |  # n11__247
            %"cos__247"<?,?> ⬅️ ::Slice(%"tmp_2__247", %"int64_0_1d__247", %"cos_sin_gather_size__247", %"int64_1_1d__247")
    1948 |  # n12__247
            %"const_3__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1949 |  # n13__247
            %"tmp_4__247"<?,?> ⬅️ ::Squeeze(%"model.layers.21.self_attn.rotary_emb.inv_freq", %"const_3__247")
    1950 |  # n14__247
            %"int64_0_1d_5__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    1951 |  # n15__247
            %"cos_sin_gather_size_6__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    1952 |  # n16__247
            %"int64_1_1d_7__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    1953 |  # n17__247
            %"sin__247"<?,?> ⬅️ ::Slice(%"tmp_4__247", %"int64_0_1d_5__247", %"cos_sin_gather_size_6__247", %"int64_1_1d_7__247")
    1954 |  # n18__247
            %"path__247"<?,?> ⬅️ ::Shape(%"value_states_21")
    1955 |  # n19__247
            %"int64_1__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    1956 |  # n20__247
            %"path2__247"<?,?> ⬅️ ::Gather(%"path__247", %"int64_1__247") {axis=0}
    1957 |  # n21__247
            %"total_seq_lengths__247"<?,?> ⬅️ ::Cast(%"path2__247") {to=6}
    1958 |  # n22__247
            %"int64_1_1d_8__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    1959 |  # n23__247
            %"temp__247"<?,?> ⬅️ ::ReduceSum(%"value_states_21", %"int64_1_1d_8__247")
    1960 |  # n24__247
            %"int64_1_1d_9__247"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    1961 |  # n25__247
            %"int64_1_1d_9_cast__247"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__247", %"temp__247")
    1962 |  # n26__247
            %"temp2__247"<?,?> ⬅️ ::Sub(%"temp__247", %"int64_1_1d_9_cast__247")
    1963 |  # n27__247
            %"seqlens_k__247"<?,?> ⬅️ ::Cast(%"temp2__247") {to=6}
    1964 |  # n28__247
            %"gqa_output__247"<?,?>, %"model_1_45"<FLOAT,[s0,32,s1,128]>, %"model_1_44"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__247", %"k__247", %"v__247", %"l_position_ids_", %"key_states_21", %"seqlens_k__247", %"total_seq_lengths__247", %"cos__247", %"sin__247") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    1965 |  # n29__247
            %"tmp_10__247"<?,?> ⬅️ ::Transpose(%"model.layers.21.self_attn.o_proj.weight") {perm=[1, 0]}
    1966 |  # n30__247
            %"model_layers_21_self_attn_1_2__245"<?,?> ⬅️ ::MatMul(%"gqa_output__247", %"tmp_10__247")
    1967 |  # n0__248
            %"alpha__248"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1968 |  # n1__248
            %"alpha_0__248"<?,?> ⬅️ ::CastLike(%"alpha__248", %"model_layers_21_self_attn_1_2__245")
    1969 |  # n2__248
            %"other_1__248"<?,?> ⬅️ ::Mul(%"model_layers_21_self_attn_1_2__245", %"alpha_0__248")
    1970 |  # n3__248
            %"add_153__245"<?,?> ⬅️ ::Add(%"model_layers_20_1_2__1", %"other_1__248")
    1971 |  # n0__249
            %"model_layers_21_post_attention_layernorm_1__245"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_153__245", %"model.layers.21.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1972 |  # n0__250
            %"gate_proj_weight_t__250"<?,?> ⬅️ ::Transpose(%"model.layers.21.mlp.gate_proj.weight") {perm=[1, 0]}
    1973 |  # n1__250
            %"up_proj_weight_t__250"<?,?> ⬅️ ::Transpose(%"model.layers.21.mlp.up_proj.weight") {perm=[1, 0]}
    1974 |  # n2__250
            %"down_proj_weight_t__250"<?,?> ⬅️ ::Transpose(%"model.layers.21.mlp.down_proj.weight") {perm=[1, 0]}
    1975 |  # n3__250
            %"gate_proj_output__250"<?,?> ⬅️ ::MatMul(%"model_layers_21_post_attention_layernorm_1__245", %"gate_proj_weight_t__250")
    1976 |  # n4__250
            %"up_proj_output__250"<?,?> ⬅️ ::MatMul(%"model_layers_21_post_attention_layernorm_1__245", %"up_proj_weight_t__250")
    1977 |  # n5__250
            %"gate_times_up_output__250"<?,?> ⬅️ ::Mul(%"gate_proj_output__250", %"up_proj_output__250")
    1978 |  # n6__250
            %"act_fn_output__250"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__250")
    1979 |  # n7__250
            %"model_layers_21_mlp_1__245"<?,?> ⬅️ ::MatMul(%"act_fn_output__250", %"down_proj_weight_t__250")
    1980 |  # n0__251
            %"alpha__251"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1981 |  # n1__251
            %"alpha_0__251"<?,?> ⬅️ ::CastLike(%"alpha__251", %"model_layers_21_mlp_1__245")
    1982 |  # n2__251
            %"other_1__251"<?,?> ⬅️ ::Mul(%"model_layers_21_mlp_1__245", %"alpha_0__251")
    1983 |  # n3__251
            %"model_layers_21_1_2__1"<?,?> ⬅️ ::Add(%"add_153__245", %"other_1__251")
    1984 |  # n0__253
            %"model_layers_22_input_layernorm_1__252"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_21_1_2__1", %"model.layers.22.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    1985 |  # n0__254
            %"tmp__254"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    1986 |  # n1__254
            %"q__254"<?,?> ⬅️ ::MatMul(%"model_layers_22_input_layernorm_1__252", %"tmp__254")
    1987 |  # n2__254
            %"tmp_0__254"<?,?> ⬅️ ::Transpose(%"model.layers.22.self_attn.q_proj.weight") {perm=[1, 0]}
    1988 |  # n3__254
            %"k__254"<?,?> ⬅️ ::MatMul(%"model_layers_22_input_layernorm_1__252", %"tmp_0__254")
    1989 |  # n4__254
            %"tmp_1__254"<?,?> ⬅️ ::Transpose(%"model.layers.22.self_attn.k_proj.weight") {perm=[1, 0]}
    1990 |  # n5__254
            %"v__254"<?,?> ⬅️ ::MatMul(%"model_layers_22_input_layernorm_1__252", %"tmp_1__254")
    1991 |  # n6__254
            %"const__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    1992 |  # n7__254
            %"tmp_2__254"<?,?> ⬅️ ::Squeeze(%"model.layers.22.self_attn.v_proj.weight", %"const__254")
    1993 |  # n8__254
            %"int64_0_1d__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    1994 |  # n9__254
            %"cos_sin_gather_size__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    1995 |  # n10__254
            %"int64_1_1d__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    1996 |  # n11__254
            %"cos__254"<?,?> ⬅️ ::Slice(%"tmp_2__254", %"int64_0_1d__254", %"cos_sin_gather_size__254", %"int64_1_1d__254")
    1997 |  # n12__254
            %"const_3__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    1998 |  # n13__254
            %"tmp_4__254"<?,?> ⬅️ ::Squeeze(%"model.layers.22.self_attn.rotary_emb.inv_freq", %"const_3__254")
    1999 |  # n14__254
            %"int64_0_1d_5__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2000 |  # n15__254
            %"cos_sin_gather_size_6__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2001 |  # n16__254
            %"int64_1_1d_7__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2002 |  # n17__254
            %"sin__254"<?,?> ⬅️ ::Slice(%"tmp_4__254", %"int64_0_1d_5__254", %"cos_sin_gather_size_6__254", %"int64_1_1d_7__254")
    2003 |  # n18__254
            %"path__254"<?,?> ⬅️ ::Shape(%"value_states_22")
    2004 |  # n19__254
            %"int64_1__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2005 |  # n20__254
            %"path2__254"<?,?> ⬅️ ::Gather(%"path__254", %"int64_1__254") {axis=0}
    2006 |  # n21__254
            %"total_seq_lengths__254"<?,?> ⬅️ ::Cast(%"path2__254") {to=6}
    2007 |  # n22__254
            %"int64_1_1d_8__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2008 |  # n23__254
            %"temp__254"<?,?> ⬅️ ::ReduceSum(%"value_states_22", %"int64_1_1d_8__254")
    2009 |  # n24__254
            %"int64_1_1d_9__254"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2010 |  # n25__254
            %"int64_1_1d_9_cast__254"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__254", %"temp__254")
    2011 |  # n26__254
            %"temp2__254"<?,?> ⬅️ ::Sub(%"temp__254", %"int64_1_1d_9_cast__254")
    2012 |  # n27__254
            %"seqlens_k__254"<?,?> ⬅️ ::Cast(%"temp2__254") {to=6}
    2013 |  # n28__254
            %"gqa_output__254"<?,?>, %"model_1_47"<FLOAT,[s0,32,s1,128]>, %"model_1_46"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__254", %"k__254", %"v__254", %"l_position_ids_", %"key_states_22", %"seqlens_k__254", %"total_seq_lengths__254", %"cos__254", %"sin__254") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2014 |  # n29__254
            %"tmp_10__254"<?,?> ⬅️ ::Transpose(%"model.layers.22.self_attn.o_proj.weight") {perm=[1, 0]}
    2015 |  # n30__254
            %"model_layers_22_self_attn_1_2__252"<?,?> ⬅️ ::MatMul(%"gqa_output__254", %"tmp_10__254")
    2016 |  # n0__255
            %"alpha__255"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2017 |  # n1__255
            %"alpha_0__255"<?,?> ⬅️ ::CastLike(%"alpha__255", %"model_layers_22_self_attn_1_2__252")
    2018 |  # n2__255
            %"other_1__255"<?,?> ⬅️ ::Mul(%"model_layers_22_self_attn_1_2__252", %"alpha_0__255")
    2019 |  # n3__255
            %"add_160__252"<?,?> ⬅️ ::Add(%"model_layers_21_1_2__1", %"other_1__255")
    2020 |  # n0__256
            %"model_layers_22_post_attention_layernorm_1__252"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_160__252", %"model.layers.22.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2021 |  # n0__257
            %"gate_proj_weight_t__257"<?,?> ⬅️ ::Transpose(%"model.layers.22.mlp.gate_proj.weight") {perm=[1, 0]}
    2022 |  # n1__257
            %"up_proj_weight_t__257"<?,?> ⬅️ ::Transpose(%"model.layers.22.mlp.up_proj.weight") {perm=[1, 0]}
    2023 |  # n2__257
            %"down_proj_weight_t__257"<?,?> ⬅️ ::Transpose(%"model.layers.22.mlp.down_proj.weight") {perm=[1, 0]}
    2024 |  # n3__257
            %"gate_proj_output__257"<?,?> ⬅️ ::MatMul(%"model_layers_22_post_attention_layernorm_1__252", %"gate_proj_weight_t__257")
    2025 |  # n4__257
            %"up_proj_output__257"<?,?> ⬅️ ::MatMul(%"model_layers_22_post_attention_layernorm_1__252", %"up_proj_weight_t__257")
    2026 |  # n5__257
            %"gate_times_up_output__257"<?,?> ⬅️ ::Mul(%"gate_proj_output__257", %"up_proj_output__257")
    2027 |  # n6__257
            %"act_fn_output__257"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__257")
    2028 |  # n7__257
            %"model_layers_22_mlp_1__252"<?,?> ⬅️ ::MatMul(%"act_fn_output__257", %"down_proj_weight_t__257")
    2029 |  # n0__258
            %"alpha__258"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2030 |  # n1__258
            %"alpha_0__258"<?,?> ⬅️ ::CastLike(%"alpha__258", %"model_layers_22_mlp_1__252")
    2031 |  # n2__258
            %"other_1__258"<?,?> ⬅️ ::Mul(%"model_layers_22_mlp_1__252", %"alpha_0__258")
    2032 |  # n3__258
            %"model_layers_22_1_2__1"<?,?> ⬅️ ::Add(%"add_160__252", %"other_1__258")
    2033 |  # n0__260
            %"model_layers_23_input_layernorm_1__259"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_22_1_2__1", %"model.layers.23.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2034 |  # n0__261
            %"tmp__261"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2035 |  # n1__261
            %"q__261"<?,?> ⬅️ ::MatMul(%"model_layers_23_input_layernorm_1__259", %"tmp__261")
    2036 |  # n2__261
            %"tmp_0__261"<?,?> ⬅️ ::Transpose(%"model.layers.23.self_attn.q_proj.weight") {perm=[1, 0]}
    2037 |  # n3__261
            %"k__261"<?,?> ⬅️ ::MatMul(%"model_layers_23_input_layernorm_1__259", %"tmp_0__261")
    2038 |  # n4__261
            %"tmp_1__261"<?,?> ⬅️ ::Transpose(%"model.layers.23.self_attn.k_proj.weight") {perm=[1, 0]}
    2039 |  # n5__261
            %"v__261"<?,?> ⬅️ ::MatMul(%"model_layers_23_input_layernorm_1__259", %"tmp_1__261")
    2040 |  # n6__261
            %"const__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2041 |  # n7__261
            %"tmp_2__261"<?,?> ⬅️ ::Squeeze(%"model.layers.23.self_attn.v_proj.weight", %"const__261")
    2042 |  # n8__261
            %"int64_0_1d__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2043 |  # n9__261
            %"cos_sin_gather_size__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2044 |  # n10__261
            %"int64_1_1d__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2045 |  # n11__261
            %"cos__261"<?,?> ⬅️ ::Slice(%"tmp_2__261", %"int64_0_1d__261", %"cos_sin_gather_size__261", %"int64_1_1d__261")
    2046 |  # n12__261
            %"const_3__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2047 |  # n13__261
            %"tmp_4__261"<?,?> ⬅️ ::Squeeze(%"model.layers.23.self_attn.rotary_emb.inv_freq", %"const_3__261")
    2048 |  # n14__261
            %"int64_0_1d_5__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2049 |  # n15__261
            %"cos_sin_gather_size_6__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2050 |  # n16__261
            %"int64_1_1d_7__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2051 |  # n17__261
            %"sin__261"<?,?> ⬅️ ::Slice(%"tmp_4__261", %"int64_0_1d_5__261", %"cos_sin_gather_size_6__261", %"int64_1_1d_7__261")
    2052 |  # n18__261
            %"path__261"<?,?> ⬅️ ::Shape(%"value_states_23")
    2053 |  # n19__261
            %"int64_1__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2054 |  # n20__261
            %"path2__261"<?,?> ⬅️ ::Gather(%"path__261", %"int64_1__261") {axis=0}
    2055 |  # n21__261
            %"total_seq_lengths__261"<?,?> ⬅️ ::Cast(%"path2__261") {to=6}
    2056 |  # n22__261
            %"int64_1_1d_8__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2057 |  # n23__261
            %"temp__261"<?,?> ⬅️ ::ReduceSum(%"value_states_23", %"int64_1_1d_8__261")
    2058 |  # n24__261
            %"int64_1_1d_9__261"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2059 |  # n25__261
            %"int64_1_1d_9_cast__261"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__261", %"temp__261")
    2060 |  # n26__261
            %"temp2__261"<?,?> ⬅️ ::Sub(%"temp__261", %"int64_1_1d_9_cast__261")
    2061 |  # n27__261
            %"seqlens_k__261"<?,?> ⬅️ ::Cast(%"temp2__261") {to=6}
    2062 |  # n28__261
            %"gqa_output__261"<?,?>, %"model_1_49"<FLOAT,[s0,32,s1,128]>, %"model_1_48"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__261", %"k__261", %"v__261", %"l_position_ids_", %"key_states_23", %"seqlens_k__261", %"total_seq_lengths__261", %"cos__261", %"sin__261") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2063 |  # n29__261
            %"tmp_10__261"<?,?> ⬅️ ::Transpose(%"model.layers.23.self_attn.o_proj.weight") {perm=[1, 0]}
    2064 |  # n30__261
            %"model_layers_23_self_attn_1_2__259"<?,?> ⬅️ ::MatMul(%"gqa_output__261", %"tmp_10__261")
    2065 |  # n0__262
            %"alpha__262"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2066 |  # n1__262
            %"alpha_0__262"<?,?> ⬅️ ::CastLike(%"alpha__262", %"model_layers_23_self_attn_1_2__259")
    2067 |  # n2__262
            %"other_1__262"<?,?> ⬅️ ::Mul(%"model_layers_23_self_attn_1_2__259", %"alpha_0__262")
    2068 |  # n3__262
            %"add_167__259"<?,?> ⬅️ ::Add(%"model_layers_22_1_2__1", %"other_1__262")
    2069 |  # n0__263
            %"model_layers_23_post_attention_layernorm_1__259"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_167__259", %"model.layers.23.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2070 |  # n0__264
            %"gate_proj_weight_t__264"<?,?> ⬅️ ::Transpose(%"model.layers.23.mlp.gate_proj.weight") {perm=[1, 0]}
    2071 |  # n1__264
            %"up_proj_weight_t__264"<?,?> ⬅️ ::Transpose(%"model.layers.23.mlp.up_proj.weight") {perm=[1, 0]}
    2072 |  # n2__264
            %"down_proj_weight_t__264"<?,?> ⬅️ ::Transpose(%"model.layers.23.mlp.down_proj.weight") {perm=[1, 0]}
    2073 |  # n3__264
            %"gate_proj_output__264"<?,?> ⬅️ ::MatMul(%"model_layers_23_post_attention_layernorm_1__259", %"gate_proj_weight_t__264")
    2074 |  # n4__264
            %"up_proj_output__264"<?,?> ⬅️ ::MatMul(%"model_layers_23_post_attention_layernorm_1__259", %"up_proj_weight_t__264")
    2075 |  # n5__264
            %"gate_times_up_output__264"<?,?> ⬅️ ::Mul(%"gate_proj_output__264", %"up_proj_output__264")
    2076 |  # n6__264
            %"act_fn_output__264"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__264")
    2077 |  # n7__264
            %"model_layers_23_mlp_1__259"<?,?> ⬅️ ::MatMul(%"act_fn_output__264", %"down_proj_weight_t__264")
    2078 |  # n0__265
            %"alpha__265"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2079 |  # n1__265
            %"alpha_0__265"<?,?> ⬅️ ::CastLike(%"alpha__265", %"model_layers_23_mlp_1__259")
    2080 |  # n2__265
            %"other_1__265"<?,?> ⬅️ ::Mul(%"model_layers_23_mlp_1__259", %"alpha_0__265")
    2081 |  # n3__265
            %"model_layers_23_1_2__1"<?,?> ⬅️ ::Add(%"add_167__259", %"other_1__265")
    2082 |  # n0__267
            %"model_layers_24_input_layernorm_1__266"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_23_1_2__1", %"model.layers.24.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2083 |  # n0__268
            %"tmp__268"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2084 |  # n1__268
            %"q__268"<?,?> ⬅️ ::MatMul(%"model_layers_24_input_layernorm_1__266", %"tmp__268")
    2085 |  # n2__268
            %"tmp_0__268"<?,?> ⬅️ ::Transpose(%"model.layers.24.self_attn.q_proj.weight") {perm=[1, 0]}
    2086 |  # n3__268
            %"k__268"<?,?> ⬅️ ::MatMul(%"model_layers_24_input_layernorm_1__266", %"tmp_0__268")
    2087 |  # n4__268
            %"tmp_1__268"<?,?> ⬅️ ::Transpose(%"model.layers.24.self_attn.k_proj.weight") {perm=[1, 0]}
    2088 |  # n5__268
            %"v__268"<?,?> ⬅️ ::MatMul(%"model_layers_24_input_layernorm_1__266", %"tmp_1__268")
    2089 |  # n6__268
            %"const__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2090 |  # n7__268
            %"tmp_2__268"<?,?> ⬅️ ::Squeeze(%"model.layers.24.self_attn.v_proj.weight", %"const__268")
    2091 |  # n8__268
            %"int64_0_1d__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2092 |  # n9__268
            %"cos_sin_gather_size__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2093 |  # n10__268
            %"int64_1_1d__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2094 |  # n11__268
            %"cos__268"<?,?> ⬅️ ::Slice(%"tmp_2__268", %"int64_0_1d__268", %"cos_sin_gather_size__268", %"int64_1_1d__268")
    2095 |  # n12__268
            %"const_3__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2096 |  # n13__268
            %"tmp_4__268"<?,?> ⬅️ ::Squeeze(%"model.layers.24.self_attn.rotary_emb.inv_freq", %"const_3__268")
    2097 |  # n14__268
            %"int64_0_1d_5__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2098 |  # n15__268
            %"cos_sin_gather_size_6__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2099 |  # n16__268
            %"int64_1_1d_7__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2100 |  # n17__268
            %"sin__268"<?,?> ⬅️ ::Slice(%"tmp_4__268", %"int64_0_1d_5__268", %"cos_sin_gather_size_6__268", %"int64_1_1d_7__268")
    2101 |  # n18__268
            %"path__268"<?,?> ⬅️ ::Shape(%"value_states_24")
    2102 |  # n19__268
            %"int64_1__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2103 |  # n20__268
            %"path2__268"<?,?> ⬅️ ::Gather(%"path__268", %"int64_1__268") {axis=0}
    2104 |  # n21__268
            %"total_seq_lengths__268"<?,?> ⬅️ ::Cast(%"path2__268") {to=6}
    2105 |  # n22__268
            %"int64_1_1d_8__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2106 |  # n23__268
            %"temp__268"<?,?> ⬅️ ::ReduceSum(%"value_states_24", %"int64_1_1d_8__268")
    2107 |  # n24__268
            %"int64_1_1d_9__268"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2108 |  # n25__268
            %"int64_1_1d_9_cast__268"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__268", %"temp__268")
    2109 |  # n26__268
            %"temp2__268"<?,?> ⬅️ ::Sub(%"temp__268", %"int64_1_1d_9_cast__268")
    2110 |  # n27__268
            %"seqlens_k__268"<?,?> ⬅️ ::Cast(%"temp2__268") {to=6}
    2111 |  # n28__268
            %"gqa_output__268"<?,?>, %"model_1_51"<FLOAT,[s0,32,s1,128]>, %"model_1_50"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__268", %"k__268", %"v__268", %"l_position_ids_", %"key_states_24", %"seqlens_k__268", %"total_seq_lengths__268", %"cos__268", %"sin__268") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2112 |  # n29__268
            %"tmp_10__268"<?,?> ⬅️ ::Transpose(%"model.layers.24.self_attn.o_proj.weight") {perm=[1, 0]}
    2113 |  # n30__268
            %"model_layers_24_self_attn_1_2__266"<?,?> ⬅️ ::MatMul(%"gqa_output__268", %"tmp_10__268")
    2114 |  # n0__269
            %"alpha__269"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2115 |  # n1__269
            %"alpha_0__269"<?,?> ⬅️ ::CastLike(%"alpha__269", %"model_layers_24_self_attn_1_2__266")
    2116 |  # n2__269
            %"other_1__269"<?,?> ⬅️ ::Mul(%"model_layers_24_self_attn_1_2__266", %"alpha_0__269")
    2117 |  # n3__269
            %"add_174__266"<?,?> ⬅️ ::Add(%"model_layers_23_1_2__1", %"other_1__269")
    2118 |  # n0__270
            %"model_layers_24_post_attention_layernorm_1__266"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_174__266", %"model.layers.24.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2119 |  # n0__271
            %"gate_proj_weight_t__271"<?,?> ⬅️ ::Transpose(%"model.layers.24.mlp.gate_proj.weight") {perm=[1, 0]}
    2120 |  # n1__271
            %"up_proj_weight_t__271"<?,?> ⬅️ ::Transpose(%"model.layers.24.mlp.up_proj.weight") {perm=[1, 0]}
    2121 |  # n2__271
            %"down_proj_weight_t__271"<?,?> ⬅️ ::Transpose(%"model.layers.24.mlp.down_proj.weight") {perm=[1, 0]}
    2122 |  # n3__271
            %"gate_proj_output__271"<?,?> ⬅️ ::MatMul(%"model_layers_24_post_attention_layernorm_1__266", %"gate_proj_weight_t__271")
    2123 |  # n4__271
            %"up_proj_output__271"<?,?> ⬅️ ::MatMul(%"model_layers_24_post_attention_layernorm_1__266", %"up_proj_weight_t__271")
    2124 |  # n5__271
            %"gate_times_up_output__271"<?,?> ⬅️ ::Mul(%"gate_proj_output__271", %"up_proj_output__271")
    2125 |  # n6__271
            %"act_fn_output__271"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__271")
    2126 |  # n7__271
            %"model_layers_24_mlp_1__266"<?,?> ⬅️ ::MatMul(%"act_fn_output__271", %"down_proj_weight_t__271")
    2127 |  # n0__272
            %"alpha__272"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2128 |  # n1__272
            %"alpha_0__272"<?,?> ⬅️ ::CastLike(%"alpha__272", %"model_layers_24_mlp_1__266")
    2129 |  # n2__272
            %"other_1__272"<?,?> ⬅️ ::Mul(%"model_layers_24_mlp_1__266", %"alpha_0__272")
    2130 |  # n3__272
            %"model_layers_24_1_2__1"<?,?> ⬅️ ::Add(%"add_174__266", %"other_1__272")
    2131 |  # n0__274
            %"model_layers_25_input_layernorm_1__273"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_24_1_2__1", %"model.layers.25.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2132 |  # n0__275
            %"tmp__275"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2133 |  # n1__275
            %"q__275"<?,?> ⬅️ ::MatMul(%"model_layers_25_input_layernorm_1__273", %"tmp__275")
    2134 |  # n2__275
            %"tmp_0__275"<?,?> ⬅️ ::Transpose(%"model.layers.25.self_attn.q_proj.weight") {perm=[1, 0]}
    2135 |  # n3__275
            %"k__275"<?,?> ⬅️ ::MatMul(%"model_layers_25_input_layernorm_1__273", %"tmp_0__275")
    2136 |  # n4__275
            %"tmp_1__275"<?,?> ⬅️ ::Transpose(%"model.layers.25.self_attn.k_proj.weight") {perm=[1, 0]}
    2137 |  # n5__275
            %"v__275"<?,?> ⬅️ ::MatMul(%"model_layers_25_input_layernorm_1__273", %"tmp_1__275")
    2138 |  # n6__275
            %"const__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2139 |  # n7__275
            %"tmp_2__275"<?,?> ⬅️ ::Squeeze(%"model.layers.25.self_attn.v_proj.weight", %"const__275")
    2140 |  # n8__275
            %"int64_0_1d__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2141 |  # n9__275
            %"cos_sin_gather_size__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2142 |  # n10__275
            %"int64_1_1d__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2143 |  # n11__275
            %"cos__275"<?,?> ⬅️ ::Slice(%"tmp_2__275", %"int64_0_1d__275", %"cos_sin_gather_size__275", %"int64_1_1d__275")
    2144 |  # n12__275
            %"const_3__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2145 |  # n13__275
            %"tmp_4__275"<?,?> ⬅️ ::Squeeze(%"model.layers.25.self_attn.rotary_emb.inv_freq", %"const_3__275")
    2146 |  # n14__275
            %"int64_0_1d_5__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2147 |  # n15__275
            %"cos_sin_gather_size_6__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2148 |  # n16__275
            %"int64_1_1d_7__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2149 |  # n17__275
            %"sin__275"<?,?> ⬅️ ::Slice(%"tmp_4__275", %"int64_0_1d_5__275", %"cos_sin_gather_size_6__275", %"int64_1_1d_7__275")
    2150 |  # n18__275
            %"path__275"<?,?> ⬅️ ::Shape(%"value_states_25")
    2151 |  # n19__275
            %"int64_1__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2152 |  # n20__275
            %"path2__275"<?,?> ⬅️ ::Gather(%"path__275", %"int64_1__275") {axis=0}
    2153 |  # n21__275
            %"total_seq_lengths__275"<?,?> ⬅️ ::Cast(%"path2__275") {to=6}
    2154 |  # n22__275
            %"int64_1_1d_8__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2155 |  # n23__275
            %"temp__275"<?,?> ⬅️ ::ReduceSum(%"value_states_25", %"int64_1_1d_8__275")
    2156 |  # n24__275
            %"int64_1_1d_9__275"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2157 |  # n25__275
            %"int64_1_1d_9_cast__275"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__275", %"temp__275")
    2158 |  # n26__275
            %"temp2__275"<?,?> ⬅️ ::Sub(%"temp__275", %"int64_1_1d_9_cast__275")
    2159 |  # n27__275
            %"seqlens_k__275"<?,?> ⬅️ ::Cast(%"temp2__275") {to=6}
    2160 |  # n28__275
            %"gqa_output__275"<?,?>, %"model_1_53"<FLOAT,[s0,32,s1,128]>, %"model_1_52"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__275", %"k__275", %"v__275", %"l_position_ids_", %"key_states_25", %"seqlens_k__275", %"total_seq_lengths__275", %"cos__275", %"sin__275") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2161 |  # n29__275
            %"tmp_10__275"<?,?> ⬅️ ::Transpose(%"model.layers.25.self_attn.o_proj.weight") {perm=[1, 0]}
    2162 |  # n30__275
            %"model_layers_25_self_attn_1_2__273"<?,?> ⬅️ ::MatMul(%"gqa_output__275", %"tmp_10__275")
    2163 |  # n0__276
            %"alpha__276"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2164 |  # n1__276
            %"alpha_0__276"<?,?> ⬅️ ::CastLike(%"alpha__276", %"model_layers_25_self_attn_1_2__273")
    2165 |  # n2__276
            %"other_1__276"<?,?> ⬅️ ::Mul(%"model_layers_25_self_attn_1_2__273", %"alpha_0__276")
    2166 |  # n3__276
            %"add_181__273"<?,?> ⬅️ ::Add(%"model_layers_24_1_2__1", %"other_1__276")
    2167 |  # n0__277
            %"model_layers_25_post_attention_layernorm_1__273"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_181__273", %"model.layers.25.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2168 |  # n0__278
            %"gate_proj_weight_t__278"<?,?> ⬅️ ::Transpose(%"model.layers.25.mlp.gate_proj.weight") {perm=[1, 0]}
    2169 |  # n1__278
            %"up_proj_weight_t__278"<?,?> ⬅️ ::Transpose(%"model.layers.25.mlp.up_proj.weight") {perm=[1, 0]}
    2170 |  # n2__278
            %"down_proj_weight_t__278"<?,?> ⬅️ ::Transpose(%"model.layers.25.mlp.down_proj.weight") {perm=[1, 0]}
    2171 |  # n3__278
            %"gate_proj_output__278"<?,?> ⬅️ ::MatMul(%"model_layers_25_post_attention_layernorm_1__273", %"gate_proj_weight_t__278")
    2172 |  # n4__278
            %"up_proj_output__278"<?,?> ⬅️ ::MatMul(%"model_layers_25_post_attention_layernorm_1__273", %"up_proj_weight_t__278")
    2173 |  # n5__278
            %"gate_times_up_output__278"<?,?> ⬅️ ::Mul(%"gate_proj_output__278", %"up_proj_output__278")
    2174 |  # n6__278
            %"act_fn_output__278"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__278")
    2175 |  # n7__278
            %"model_layers_25_mlp_1__273"<?,?> ⬅️ ::MatMul(%"act_fn_output__278", %"down_proj_weight_t__278")
    2176 |  # n0__279
            %"alpha__279"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2177 |  # n1__279
            %"alpha_0__279"<?,?> ⬅️ ::CastLike(%"alpha__279", %"model_layers_25_mlp_1__273")
    2178 |  # n2__279
            %"other_1__279"<?,?> ⬅️ ::Mul(%"model_layers_25_mlp_1__273", %"alpha_0__279")
    2179 |  # n3__279
            %"model_layers_25_1_2__1"<?,?> ⬅️ ::Add(%"add_181__273", %"other_1__279")
    2180 |  # n0__281
            %"model_layers_26_input_layernorm_1__280"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_25_1_2__1", %"model.layers.26.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2181 |  # n0__282
            %"tmp__282"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2182 |  # n1__282
            %"q__282"<?,?> ⬅️ ::MatMul(%"model_layers_26_input_layernorm_1__280", %"tmp__282")
    2183 |  # n2__282
            %"tmp_0__282"<?,?> ⬅️ ::Transpose(%"model.layers.26.self_attn.q_proj.weight") {perm=[1, 0]}
    2184 |  # n3__282
            %"k__282"<?,?> ⬅️ ::MatMul(%"model_layers_26_input_layernorm_1__280", %"tmp_0__282")
    2185 |  # n4__282
            %"tmp_1__282"<?,?> ⬅️ ::Transpose(%"model.layers.26.self_attn.k_proj.weight") {perm=[1, 0]}
    2186 |  # n5__282
            %"v__282"<?,?> ⬅️ ::MatMul(%"model_layers_26_input_layernorm_1__280", %"tmp_1__282")
    2187 |  # n6__282
            %"const__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2188 |  # n7__282
            %"tmp_2__282"<?,?> ⬅️ ::Squeeze(%"model.layers.26.self_attn.v_proj.weight", %"const__282")
    2189 |  # n8__282
            %"int64_0_1d__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2190 |  # n9__282
            %"cos_sin_gather_size__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2191 |  # n10__282
            %"int64_1_1d__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2192 |  # n11__282
            %"cos__282"<?,?> ⬅️ ::Slice(%"tmp_2__282", %"int64_0_1d__282", %"cos_sin_gather_size__282", %"int64_1_1d__282")
    2193 |  # n12__282
            %"const_3__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2194 |  # n13__282
            %"tmp_4__282"<?,?> ⬅️ ::Squeeze(%"model.layers.26.self_attn.rotary_emb.inv_freq", %"const_3__282")
    2195 |  # n14__282
            %"int64_0_1d_5__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2196 |  # n15__282
            %"cos_sin_gather_size_6__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2197 |  # n16__282
            %"int64_1_1d_7__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2198 |  # n17__282
            %"sin__282"<?,?> ⬅️ ::Slice(%"tmp_4__282", %"int64_0_1d_5__282", %"cos_sin_gather_size_6__282", %"int64_1_1d_7__282")
    2199 |  # n18__282
            %"path__282"<?,?> ⬅️ ::Shape(%"value_states_26")
    2200 |  # n19__282
            %"int64_1__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2201 |  # n20__282
            %"path2__282"<?,?> ⬅️ ::Gather(%"path__282", %"int64_1__282") {axis=0}
    2202 |  # n21__282
            %"total_seq_lengths__282"<?,?> ⬅️ ::Cast(%"path2__282") {to=6}
    2203 |  # n22__282
            %"int64_1_1d_8__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2204 |  # n23__282
            %"temp__282"<?,?> ⬅️ ::ReduceSum(%"value_states_26", %"int64_1_1d_8__282")
    2205 |  # n24__282
            %"int64_1_1d_9__282"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2206 |  # n25__282
            %"int64_1_1d_9_cast__282"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__282", %"temp__282")
    2207 |  # n26__282
            %"temp2__282"<?,?> ⬅️ ::Sub(%"temp__282", %"int64_1_1d_9_cast__282")
    2208 |  # n27__282
            %"seqlens_k__282"<?,?> ⬅️ ::Cast(%"temp2__282") {to=6}
    2209 |  # n28__282
            %"gqa_output__282"<?,?>, %"model_1_55"<FLOAT,[s0,32,s1,128]>, %"model_1_54"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__282", %"k__282", %"v__282", %"l_position_ids_", %"key_states_26", %"seqlens_k__282", %"total_seq_lengths__282", %"cos__282", %"sin__282") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2210 |  # n29__282
            %"tmp_10__282"<?,?> ⬅️ ::Transpose(%"model.layers.26.self_attn.o_proj.weight") {perm=[1, 0]}
    2211 |  # n30__282
            %"model_layers_26_self_attn_1_2__280"<?,?> ⬅️ ::MatMul(%"gqa_output__282", %"tmp_10__282")
    2212 |  # n0__283
            %"alpha__283"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2213 |  # n1__283
            %"alpha_0__283"<?,?> ⬅️ ::CastLike(%"alpha__283", %"model_layers_26_self_attn_1_2__280")
    2214 |  # n2__283
            %"other_1__283"<?,?> ⬅️ ::Mul(%"model_layers_26_self_attn_1_2__280", %"alpha_0__283")
    2215 |  # n3__283
            %"add_188__280"<?,?> ⬅️ ::Add(%"model_layers_25_1_2__1", %"other_1__283")
    2216 |  # n0__284
            %"model_layers_26_post_attention_layernorm_1__280"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_188__280", %"model.layers.26.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2217 |  # n0__285
            %"gate_proj_weight_t__285"<?,?> ⬅️ ::Transpose(%"model.layers.26.mlp.gate_proj.weight") {perm=[1, 0]}
    2218 |  # n1__285
            %"up_proj_weight_t__285"<?,?> ⬅️ ::Transpose(%"model.layers.26.mlp.up_proj.weight") {perm=[1, 0]}
    2219 |  # n2__285
            %"down_proj_weight_t__285"<?,?> ⬅️ ::Transpose(%"model.layers.26.mlp.down_proj.weight") {perm=[1, 0]}
    2220 |  # n3__285
            %"gate_proj_output__285"<?,?> ⬅️ ::MatMul(%"model_layers_26_post_attention_layernorm_1__280", %"gate_proj_weight_t__285")
    2221 |  # n4__285
            %"up_proj_output__285"<?,?> ⬅️ ::MatMul(%"model_layers_26_post_attention_layernorm_1__280", %"up_proj_weight_t__285")
    2222 |  # n5__285
            %"gate_times_up_output__285"<?,?> ⬅️ ::Mul(%"gate_proj_output__285", %"up_proj_output__285")
    2223 |  # n6__285
            %"act_fn_output__285"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__285")
    2224 |  # n7__285
            %"model_layers_26_mlp_1__280"<?,?> ⬅️ ::MatMul(%"act_fn_output__285", %"down_proj_weight_t__285")
    2225 |  # n0__286
            %"alpha__286"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2226 |  # n1__286
            %"alpha_0__286"<?,?> ⬅️ ::CastLike(%"alpha__286", %"model_layers_26_mlp_1__280")
    2227 |  # n2__286
            %"other_1__286"<?,?> ⬅️ ::Mul(%"model_layers_26_mlp_1__280", %"alpha_0__286")
    2228 |  # n3__286
            %"model_layers_26_1_2__1"<?,?> ⬅️ ::Add(%"add_188__280", %"other_1__286")
    2229 |  # n0__288
            %"model_layers_27_input_layernorm_1__287"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_26_1_2__1", %"model.layers.27.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2230 |  # n0__289
            %"tmp__289"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2231 |  # n1__289
            %"q__289"<?,?> ⬅️ ::MatMul(%"model_layers_27_input_layernorm_1__287", %"tmp__289")
    2232 |  # n2__289
            %"tmp_0__289"<?,?> ⬅️ ::Transpose(%"model.layers.27.self_attn.q_proj.weight") {perm=[1, 0]}
    2233 |  # n3__289
            %"k__289"<?,?> ⬅️ ::MatMul(%"model_layers_27_input_layernorm_1__287", %"tmp_0__289")
    2234 |  # n4__289
            %"tmp_1__289"<?,?> ⬅️ ::Transpose(%"model.layers.27.self_attn.k_proj.weight") {perm=[1, 0]}
    2235 |  # n5__289
            %"v__289"<?,?> ⬅️ ::MatMul(%"model_layers_27_input_layernorm_1__287", %"tmp_1__289")
    2236 |  # n6__289
            %"const__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2237 |  # n7__289
            %"tmp_2__289"<?,?> ⬅️ ::Squeeze(%"model.layers.27.self_attn.v_proj.weight", %"const__289")
    2238 |  # n8__289
            %"int64_0_1d__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2239 |  # n9__289
            %"cos_sin_gather_size__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2240 |  # n10__289
            %"int64_1_1d__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2241 |  # n11__289
            %"cos__289"<?,?> ⬅️ ::Slice(%"tmp_2__289", %"int64_0_1d__289", %"cos_sin_gather_size__289", %"int64_1_1d__289")
    2242 |  # n12__289
            %"const_3__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2243 |  # n13__289
            %"tmp_4__289"<?,?> ⬅️ ::Squeeze(%"model.layers.27.self_attn.rotary_emb.inv_freq", %"const_3__289")
    2244 |  # n14__289
            %"int64_0_1d_5__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2245 |  # n15__289
            %"cos_sin_gather_size_6__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2246 |  # n16__289
            %"int64_1_1d_7__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2247 |  # n17__289
            %"sin__289"<?,?> ⬅️ ::Slice(%"tmp_4__289", %"int64_0_1d_5__289", %"cos_sin_gather_size_6__289", %"int64_1_1d_7__289")
    2248 |  # n18__289
            %"path__289"<?,?> ⬅️ ::Shape(%"value_states_27")
    2249 |  # n19__289
            %"int64_1__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2250 |  # n20__289
            %"path2__289"<?,?> ⬅️ ::Gather(%"path__289", %"int64_1__289") {axis=0}
    2251 |  # n21__289
            %"total_seq_lengths__289"<?,?> ⬅️ ::Cast(%"path2__289") {to=6}
    2252 |  # n22__289
            %"int64_1_1d_8__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2253 |  # n23__289
            %"temp__289"<?,?> ⬅️ ::ReduceSum(%"value_states_27", %"int64_1_1d_8__289")
    2254 |  # n24__289
            %"int64_1_1d_9__289"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2255 |  # n25__289
            %"int64_1_1d_9_cast__289"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__289", %"temp__289")
    2256 |  # n26__289
            %"temp2__289"<?,?> ⬅️ ::Sub(%"temp__289", %"int64_1_1d_9_cast__289")
    2257 |  # n27__289
            %"seqlens_k__289"<?,?> ⬅️ ::Cast(%"temp2__289") {to=6}
    2258 |  # n28__289
            %"gqa_output__289"<?,?>, %"model_1_57"<FLOAT,[s0,32,s1,128]>, %"model_1_56"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__289", %"k__289", %"v__289", %"l_position_ids_", %"key_states_27", %"seqlens_k__289", %"total_seq_lengths__289", %"cos__289", %"sin__289") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2259 |  # n29__289
            %"tmp_10__289"<?,?> ⬅️ ::Transpose(%"model.layers.27.self_attn.o_proj.weight") {perm=[1, 0]}
    2260 |  # n30__289
            %"model_layers_27_self_attn_1_2__287"<?,?> ⬅️ ::MatMul(%"gqa_output__289", %"tmp_10__289")
    2261 |  # n0__290
            %"alpha__290"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2262 |  # n1__290
            %"alpha_0__290"<?,?> ⬅️ ::CastLike(%"alpha__290", %"model_layers_27_self_attn_1_2__287")
    2263 |  # n2__290
            %"other_1__290"<?,?> ⬅️ ::Mul(%"model_layers_27_self_attn_1_2__287", %"alpha_0__290")
    2264 |  # n3__290
            %"add_195__287"<?,?> ⬅️ ::Add(%"model_layers_26_1_2__1", %"other_1__290")
    2265 |  # n0__291
            %"model_layers_27_post_attention_layernorm_1__287"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_195__287", %"model.layers.27.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2266 |  # n0__292
            %"gate_proj_weight_t__292"<?,?> ⬅️ ::Transpose(%"model.layers.27.mlp.gate_proj.weight") {perm=[1, 0]}
    2267 |  # n1__292
            %"up_proj_weight_t__292"<?,?> ⬅️ ::Transpose(%"model.layers.27.mlp.up_proj.weight") {perm=[1, 0]}
    2268 |  # n2__292
            %"down_proj_weight_t__292"<?,?> ⬅️ ::Transpose(%"model.layers.27.mlp.down_proj.weight") {perm=[1, 0]}
    2269 |  # n3__292
            %"gate_proj_output__292"<?,?> ⬅️ ::MatMul(%"model_layers_27_post_attention_layernorm_1__287", %"gate_proj_weight_t__292")
    2270 |  # n4__292
            %"up_proj_output__292"<?,?> ⬅️ ::MatMul(%"model_layers_27_post_attention_layernorm_1__287", %"up_proj_weight_t__292")
    2271 |  # n5__292
            %"gate_times_up_output__292"<?,?> ⬅️ ::Mul(%"gate_proj_output__292", %"up_proj_output__292")
    2272 |  # n6__292
            %"act_fn_output__292"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__292")
    2273 |  # n7__292
            %"model_layers_27_mlp_1__287"<?,?> ⬅️ ::MatMul(%"act_fn_output__292", %"down_proj_weight_t__292")
    2274 |  # n0__293
            %"alpha__293"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2275 |  # n1__293
            %"alpha_0__293"<?,?> ⬅️ ::CastLike(%"alpha__293", %"model_layers_27_mlp_1__287")
    2276 |  # n2__293
            %"other_1__293"<?,?> ⬅️ ::Mul(%"model_layers_27_mlp_1__287", %"alpha_0__293")
    2277 |  # n3__293
            %"model_layers_27_1_2__1"<?,?> ⬅️ ::Add(%"add_195__287", %"other_1__293")
    2278 |  # n0__295
            %"model_layers_28_input_layernorm_1__294"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_27_1_2__1", %"model.layers.28.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2279 |  # n0__296
            %"tmp__296"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2280 |  # n1__296
            %"q__296"<?,?> ⬅️ ::MatMul(%"model_layers_28_input_layernorm_1__294", %"tmp__296")
    2281 |  # n2__296
            %"tmp_0__296"<?,?> ⬅️ ::Transpose(%"model.layers.28.self_attn.q_proj.weight") {perm=[1, 0]}
    2282 |  # n3__296
            %"k__296"<?,?> ⬅️ ::MatMul(%"model_layers_28_input_layernorm_1__294", %"tmp_0__296")
    2283 |  # n4__296
            %"tmp_1__296"<?,?> ⬅️ ::Transpose(%"model.layers.28.self_attn.k_proj.weight") {perm=[1, 0]}
    2284 |  # n5__296
            %"v__296"<?,?> ⬅️ ::MatMul(%"model_layers_28_input_layernorm_1__294", %"tmp_1__296")
    2285 |  # n6__296
            %"const__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2286 |  # n7__296
            %"tmp_2__296"<?,?> ⬅️ ::Squeeze(%"model.layers.28.self_attn.v_proj.weight", %"const__296")
    2287 |  # n8__296
            %"int64_0_1d__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2288 |  # n9__296
            %"cos_sin_gather_size__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2289 |  # n10__296
            %"int64_1_1d__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2290 |  # n11__296
            %"cos__296"<?,?> ⬅️ ::Slice(%"tmp_2__296", %"int64_0_1d__296", %"cos_sin_gather_size__296", %"int64_1_1d__296")
    2291 |  # n12__296
            %"const_3__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2292 |  # n13__296
            %"tmp_4__296"<?,?> ⬅️ ::Squeeze(%"model.layers.28.self_attn.rotary_emb.inv_freq", %"const_3__296")
    2293 |  # n14__296
            %"int64_0_1d_5__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2294 |  # n15__296
            %"cos_sin_gather_size_6__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2295 |  # n16__296
            %"int64_1_1d_7__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2296 |  # n17__296
            %"sin__296"<?,?> ⬅️ ::Slice(%"tmp_4__296", %"int64_0_1d_5__296", %"cos_sin_gather_size_6__296", %"int64_1_1d_7__296")
    2297 |  # n18__296
            %"path__296"<?,?> ⬅️ ::Shape(%"value_states_28")
    2298 |  # n19__296
            %"int64_1__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2299 |  # n20__296
            %"path2__296"<?,?> ⬅️ ::Gather(%"path__296", %"int64_1__296") {axis=0}
    2300 |  # n21__296
            %"total_seq_lengths__296"<?,?> ⬅️ ::Cast(%"path2__296") {to=6}
    2301 |  # n22__296
            %"int64_1_1d_8__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2302 |  # n23__296
            %"temp__296"<?,?> ⬅️ ::ReduceSum(%"value_states_28", %"int64_1_1d_8__296")
    2303 |  # n24__296
            %"int64_1_1d_9__296"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2304 |  # n25__296
            %"int64_1_1d_9_cast__296"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__296", %"temp__296")
    2305 |  # n26__296
            %"temp2__296"<?,?> ⬅️ ::Sub(%"temp__296", %"int64_1_1d_9_cast__296")
    2306 |  # n27__296
            %"seqlens_k__296"<?,?> ⬅️ ::Cast(%"temp2__296") {to=6}
    2307 |  # n28__296
            %"gqa_output__296"<?,?>, %"model_1_59"<FLOAT,[s0,32,s1,128]>, %"model_1_58"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__296", %"k__296", %"v__296", %"l_position_ids_", %"key_states_28", %"seqlens_k__296", %"total_seq_lengths__296", %"cos__296", %"sin__296") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2308 |  # n29__296
            %"tmp_10__296"<?,?> ⬅️ ::Transpose(%"model.layers.28.self_attn.o_proj.weight") {perm=[1, 0]}
    2309 |  # n30__296
            %"model_layers_28_self_attn_1_2__294"<?,?> ⬅️ ::MatMul(%"gqa_output__296", %"tmp_10__296")
    2310 |  # n0__297
            %"alpha__297"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2311 |  # n1__297
            %"alpha_0__297"<?,?> ⬅️ ::CastLike(%"alpha__297", %"model_layers_28_self_attn_1_2__294")
    2312 |  # n2__297
            %"other_1__297"<?,?> ⬅️ ::Mul(%"model_layers_28_self_attn_1_2__294", %"alpha_0__297")
    2313 |  # n3__297
            %"add_202__294"<?,?> ⬅️ ::Add(%"model_layers_27_1_2__1", %"other_1__297")
    2314 |  # n0__298
            %"model_layers_28_post_attention_layernorm_1__294"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_202__294", %"model.layers.28.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2315 |  # n0__299
            %"gate_proj_weight_t__299"<?,?> ⬅️ ::Transpose(%"model.layers.28.mlp.gate_proj.weight") {perm=[1, 0]}
    2316 |  # n1__299
            %"up_proj_weight_t__299"<?,?> ⬅️ ::Transpose(%"model.layers.28.mlp.up_proj.weight") {perm=[1, 0]}
    2317 |  # n2__299
            %"down_proj_weight_t__299"<?,?> ⬅️ ::Transpose(%"model.layers.28.mlp.down_proj.weight") {perm=[1, 0]}
    2318 |  # n3__299
            %"gate_proj_output__299"<?,?> ⬅️ ::MatMul(%"model_layers_28_post_attention_layernorm_1__294", %"gate_proj_weight_t__299")
    2319 |  # n4__299
            %"up_proj_output__299"<?,?> ⬅️ ::MatMul(%"model_layers_28_post_attention_layernorm_1__294", %"up_proj_weight_t__299")
    2320 |  # n5__299
            %"gate_times_up_output__299"<?,?> ⬅️ ::Mul(%"gate_proj_output__299", %"up_proj_output__299")
    2321 |  # n6__299
            %"act_fn_output__299"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__299")
    2322 |  # n7__299
            %"model_layers_28_mlp_1__294"<?,?> ⬅️ ::MatMul(%"act_fn_output__299", %"down_proj_weight_t__299")
    2323 |  # n0__300
            %"alpha__300"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2324 |  # n1__300
            %"alpha_0__300"<?,?> ⬅️ ::CastLike(%"alpha__300", %"model_layers_28_mlp_1__294")
    2325 |  # n2__300
            %"other_1__300"<?,?> ⬅️ ::Mul(%"model_layers_28_mlp_1__294", %"alpha_0__300")
    2326 |  # n3__300
            %"model_layers_28_1_2__1"<?,?> ⬅️ ::Add(%"add_202__294", %"other_1__300")
    2327 |  # n0__302
            %"model_layers_29_input_layernorm_1__301"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_28_1_2__1", %"model.layers.29.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2328 |  # n0__303
            %"tmp__303"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2329 |  # n1__303
            %"q__303"<?,?> ⬅️ ::MatMul(%"model_layers_29_input_layernorm_1__301", %"tmp__303")
    2330 |  # n2__303
            %"tmp_0__303"<?,?> ⬅️ ::Transpose(%"model.layers.29.self_attn.q_proj.weight") {perm=[1, 0]}
    2331 |  # n3__303
            %"k__303"<?,?> ⬅️ ::MatMul(%"model_layers_29_input_layernorm_1__301", %"tmp_0__303")
    2332 |  # n4__303
            %"tmp_1__303"<?,?> ⬅️ ::Transpose(%"model.layers.29.self_attn.k_proj.weight") {perm=[1, 0]}
    2333 |  # n5__303
            %"v__303"<?,?> ⬅️ ::MatMul(%"model_layers_29_input_layernorm_1__301", %"tmp_1__303")
    2334 |  # n6__303
            %"const__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2335 |  # n7__303
            %"tmp_2__303"<?,?> ⬅️ ::Squeeze(%"model.layers.29.self_attn.v_proj.weight", %"const__303")
    2336 |  # n8__303
            %"int64_0_1d__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2337 |  # n9__303
            %"cos_sin_gather_size__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2338 |  # n10__303
            %"int64_1_1d__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2339 |  # n11__303
            %"cos__303"<?,?> ⬅️ ::Slice(%"tmp_2__303", %"int64_0_1d__303", %"cos_sin_gather_size__303", %"int64_1_1d__303")
    2340 |  # n12__303
            %"const_3__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2341 |  # n13__303
            %"tmp_4__303"<?,?> ⬅️ ::Squeeze(%"model.layers.29.self_attn.rotary_emb.inv_freq", %"const_3__303")
    2342 |  # n14__303
            %"int64_0_1d_5__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2343 |  # n15__303
            %"cos_sin_gather_size_6__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2344 |  # n16__303
            %"int64_1_1d_7__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2345 |  # n17__303
            %"sin__303"<?,?> ⬅️ ::Slice(%"tmp_4__303", %"int64_0_1d_5__303", %"cos_sin_gather_size_6__303", %"int64_1_1d_7__303")
    2346 |  # n18__303
            %"path__303"<?,?> ⬅️ ::Shape(%"value_states_29")
    2347 |  # n19__303
            %"int64_1__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2348 |  # n20__303
            %"path2__303"<?,?> ⬅️ ::Gather(%"path__303", %"int64_1__303") {axis=0}
    2349 |  # n21__303
            %"total_seq_lengths__303"<?,?> ⬅️ ::Cast(%"path2__303") {to=6}
    2350 |  # n22__303
            %"int64_1_1d_8__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2351 |  # n23__303
            %"temp__303"<?,?> ⬅️ ::ReduceSum(%"value_states_29", %"int64_1_1d_8__303")
    2352 |  # n24__303
            %"int64_1_1d_9__303"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2353 |  # n25__303
            %"int64_1_1d_9_cast__303"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__303", %"temp__303")
    2354 |  # n26__303
            %"temp2__303"<?,?> ⬅️ ::Sub(%"temp__303", %"int64_1_1d_9_cast__303")
    2355 |  # n27__303
            %"seqlens_k__303"<?,?> ⬅️ ::Cast(%"temp2__303") {to=6}
    2356 |  # n28__303
            %"gqa_output__303"<?,?>, %"model_1_61"<FLOAT,[s0,32,s1,128]>, %"model_1_60"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__303", %"k__303", %"v__303", %"l_position_ids_", %"key_states_29", %"seqlens_k__303", %"total_seq_lengths__303", %"cos__303", %"sin__303") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2357 |  # n29__303
            %"tmp_10__303"<?,?> ⬅️ ::Transpose(%"model.layers.29.self_attn.o_proj.weight") {perm=[1, 0]}
    2358 |  # n30__303
            %"model_layers_29_self_attn_1_2__301"<?,?> ⬅️ ::MatMul(%"gqa_output__303", %"tmp_10__303")
    2359 |  # n0__304
            %"alpha__304"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2360 |  # n1__304
            %"alpha_0__304"<?,?> ⬅️ ::CastLike(%"alpha__304", %"model_layers_29_self_attn_1_2__301")
    2361 |  # n2__304
            %"other_1__304"<?,?> ⬅️ ::Mul(%"model_layers_29_self_attn_1_2__301", %"alpha_0__304")
    2362 |  # n3__304
            %"add_209__301"<?,?> ⬅️ ::Add(%"model_layers_28_1_2__1", %"other_1__304")
    2363 |  # n0__305
            %"model_layers_29_post_attention_layernorm_1__301"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_209__301", %"model.layers.29.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2364 |  # n0__306
            %"gate_proj_weight_t__306"<?,?> ⬅️ ::Transpose(%"model.layers.29.mlp.gate_proj.weight") {perm=[1, 0]}
    2365 |  # n1__306
            %"up_proj_weight_t__306"<?,?> ⬅️ ::Transpose(%"model.layers.29.mlp.up_proj.weight") {perm=[1, 0]}
    2366 |  # n2__306
            %"down_proj_weight_t__306"<?,?> ⬅️ ::Transpose(%"model.layers.29.mlp.down_proj.weight") {perm=[1, 0]}
    2367 |  # n3__306
            %"gate_proj_output__306"<?,?> ⬅️ ::MatMul(%"model_layers_29_post_attention_layernorm_1__301", %"gate_proj_weight_t__306")
    2368 |  # n4__306
            %"up_proj_output__306"<?,?> ⬅️ ::MatMul(%"model_layers_29_post_attention_layernorm_1__301", %"up_proj_weight_t__306")
    2369 |  # n5__306
            %"gate_times_up_output__306"<?,?> ⬅️ ::Mul(%"gate_proj_output__306", %"up_proj_output__306")
    2370 |  # n6__306
            %"act_fn_output__306"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__306")
    2371 |  # n7__306
            %"model_layers_29_mlp_1__301"<?,?> ⬅️ ::MatMul(%"act_fn_output__306", %"down_proj_weight_t__306")
    2372 |  # n0__307
            %"alpha__307"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2373 |  # n1__307
            %"alpha_0__307"<?,?> ⬅️ ::CastLike(%"alpha__307", %"model_layers_29_mlp_1__301")
    2374 |  # n2__307
            %"other_1__307"<?,?> ⬅️ ::Mul(%"model_layers_29_mlp_1__301", %"alpha_0__307")
    2375 |  # n3__307
            %"model_layers_29_1_2__1"<?,?> ⬅️ ::Add(%"add_209__301", %"other_1__307")
    2376 |  # n0__309
            %"model_layers_30_input_layernorm_1__308"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_29_1_2__1", %"model.layers.30.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2377 |  # n0__310
            %"tmp__310"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2378 |  # n1__310
            %"q__310"<?,?> ⬅️ ::MatMul(%"model_layers_30_input_layernorm_1__308", %"tmp__310")
    2379 |  # n2__310
            %"tmp_0__310"<?,?> ⬅️ ::Transpose(%"model.layers.30.self_attn.q_proj.weight") {perm=[1, 0]}
    2380 |  # n3__310
            %"k__310"<?,?> ⬅️ ::MatMul(%"model_layers_30_input_layernorm_1__308", %"tmp_0__310")
    2381 |  # n4__310
            %"tmp_1__310"<?,?> ⬅️ ::Transpose(%"model.layers.30.self_attn.k_proj.weight") {perm=[1, 0]}
    2382 |  # n5__310
            %"v__310"<?,?> ⬅️ ::MatMul(%"model_layers_30_input_layernorm_1__308", %"tmp_1__310")
    2383 |  # n6__310
            %"const__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2384 |  # n7__310
            %"tmp_2__310"<?,?> ⬅️ ::Squeeze(%"model.layers.30.self_attn.v_proj.weight", %"const__310")
    2385 |  # n8__310
            %"int64_0_1d__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2386 |  # n9__310
            %"cos_sin_gather_size__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2387 |  # n10__310
            %"int64_1_1d__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2388 |  # n11__310
            %"cos__310"<?,?> ⬅️ ::Slice(%"tmp_2__310", %"int64_0_1d__310", %"cos_sin_gather_size__310", %"int64_1_1d__310")
    2389 |  # n12__310
            %"const_3__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2390 |  # n13__310
            %"tmp_4__310"<?,?> ⬅️ ::Squeeze(%"model.layers.30.self_attn.rotary_emb.inv_freq", %"const_3__310")
    2391 |  # n14__310
            %"int64_0_1d_5__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2392 |  # n15__310
            %"cos_sin_gather_size_6__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2393 |  # n16__310
            %"int64_1_1d_7__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2394 |  # n17__310
            %"sin__310"<?,?> ⬅️ ::Slice(%"tmp_4__310", %"int64_0_1d_5__310", %"cos_sin_gather_size_6__310", %"int64_1_1d_7__310")
    2395 |  # n18__310
            %"path__310"<?,?> ⬅️ ::Shape(%"value_states_30")
    2396 |  # n19__310
            %"int64_1__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2397 |  # n20__310
            %"path2__310"<?,?> ⬅️ ::Gather(%"path__310", %"int64_1__310") {axis=0}
    2398 |  # n21__310
            %"total_seq_lengths__310"<?,?> ⬅️ ::Cast(%"path2__310") {to=6}
    2399 |  # n22__310
            %"int64_1_1d_8__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2400 |  # n23__310
            %"temp__310"<?,?> ⬅️ ::ReduceSum(%"value_states_30", %"int64_1_1d_8__310")
    2401 |  # n24__310
            %"int64_1_1d_9__310"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2402 |  # n25__310
            %"int64_1_1d_9_cast__310"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__310", %"temp__310")
    2403 |  # n26__310
            %"temp2__310"<?,?> ⬅️ ::Sub(%"temp__310", %"int64_1_1d_9_cast__310")
    2404 |  # n27__310
            %"seqlens_k__310"<?,?> ⬅️ ::Cast(%"temp2__310") {to=6}
    2405 |  # n28__310
            %"gqa_output__310"<?,?>, %"model_1_63"<FLOAT,[s0,32,s1,128]>, %"model_1_62"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__310", %"k__310", %"v__310", %"l_position_ids_", %"key_states_30", %"seqlens_k__310", %"total_seq_lengths__310", %"cos__310", %"sin__310") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2406 |  # n29__310
            %"tmp_10__310"<?,?> ⬅️ ::Transpose(%"model.layers.30.self_attn.o_proj.weight") {perm=[1, 0]}
    2407 |  # n30__310
            %"model_layers_30_self_attn_1_2__308"<?,?> ⬅️ ::MatMul(%"gqa_output__310", %"tmp_10__310")
    2408 |  # n0__311
            %"alpha__311"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2409 |  # n1__311
            %"alpha_0__311"<?,?> ⬅️ ::CastLike(%"alpha__311", %"model_layers_30_self_attn_1_2__308")
    2410 |  # n2__311
            %"other_1__311"<?,?> ⬅️ ::Mul(%"model_layers_30_self_attn_1_2__308", %"alpha_0__311")
    2411 |  # n3__311
            %"add_216__308"<?,?> ⬅️ ::Add(%"model_layers_29_1_2__1", %"other_1__311")
    2412 |  # n0__312
            %"model_layers_30_post_attention_layernorm_1__308"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_216__308", %"model.layers.30.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2413 |  # n0__313
            %"gate_proj_weight_t__313"<?,?> ⬅️ ::Transpose(%"model.layers.30.mlp.gate_proj.weight") {perm=[1, 0]}
    2414 |  # n1__313
            %"up_proj_weight_t__313"<?,?> ⬅️ ::Transpose(%"model.layers.30.mlp.up_proj.weight") {perm=[1, 0]}
    2415 |  # n2__313
            %"down_proj_weight_t__313"<?,?> ⬅️ ::Transpose(%"model.layers.30.mlp.down_proj.weight") {perm=[1, 0]}
    2416 |  # n3__313
            %"gate_proj_output__313"<?,?> ⬅️ ::MatMul(%"model_layers_30_post_attention_layernorm_1__308", %"gate_proj_weight_t__313")
    2417 |  # n4__313
            %"up_proj_output__313"<?,?> ⬅️ ::MatMul(%"model_layers_30_post_attention_layernorm_1__308", %"up_proj_weight_t__313")
    2418 |  # n5__313
            %"gate_times_up_output__313"<?,?> ⬅️ ::Mul(%"gate_proj_output__313", %"up_proj_output__313")
    2419 |  # n6__313
            %"act_fn_output__313"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__313")
    2420 |  # n7__313
            %"model_layers_30_mlp_1__308"<?,?> ⬅️ ::MatMul(%"act_fn_output__313", %"down_proj_weight_t__313")
    2421 |  # n0__314
            %"alpha__314"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2422 |  # n1__314
            %"alpha_0__314"<?,?> ⬅️ ::CastLike(%"alpha__314", %"model_layers_30_mlp_1__308")
    2423 |  # n2__314
            %"other_1__314"<?,?> ⬅️ ::Mul(%"model_layers_30_mlp_1__308", %"alpha_0__314")
    2424 |  # n3__314
            %"model_layers_30_1_2__1"<?,?> ⬅️ ::Add(%"add_216__308", %"other_1__314")
    2425 |  # n0__316
            %"model_layers_31_input_layernorm_1__315"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_30_1_2__1", %"model.layers.31.input_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2426 |  # n0__317
            %"tmp__317"<?,?> ⬅️ ::Transpose(%"slice_scatter_2__1") {perm=[1, 0]}
    2427 |  # n1__317
            %"q__317"<?,?> ⬅️ ::MatMul(%"model_layers_31_input_layernorm_1__315", %"tmp__317")
    2428 |  # n2__317
            %"tmp_0__317"<?,?> ⬅️ ::Transpose(%"model.layers.31.self_attn.q_proj.weight") {perm=[1, 0]}
    2429 |  # n3__317
            %"k__317"<?,?> ⬅️ ::MatMul(%"model_layers_31_input_layernorm_1__315", %"tmp_0__317")
    2430 |  # n4__317
            %"tmp_1__317"<?,?> ⬅️ ::Transpose(%"model.layers.31.self_attn.k_proj.weight") {perm=[1, 0]}
    2431 |  # n5__317
            %"v__317"<?,?> ⬅️ ::MatMul(%"model_layers_31_input_layernorm_1__315", %"tmp_1__317")
    2432 |  # n6__317
            %"const__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const')}
    2433 |  # n7__317
            %"tmp_2__317"<?,?> ⬅️ ::Squeeze(%"model.layers.31.self_attn.v_proj.weight", %"const__317")
    2434 |  # n8__317
            %"int64_0_1d__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d')}
    2435 |  # n9__317
            %"cos_sin_gather_size__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size')}
    2436 |  # n10__317
            %"int64_1_1d__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d')}
    2437 |  # n11__317
            %"cos__317"<?,?> ⬅️ ::Slice(%"tmp_2__317", %"int64_0_1d__317", %"cos_sin_gather_size__317", %"int64_1_1d__317")
    2438 |  # n12__317
            %"const_3__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[2]>(name='const_3')}
    2439 |  # n13__317
            %"tmp_4__317"<?,?> ⬅️ ::Squeeze(%"model.layers.31.self_attn.rotary_emb.inv_freq", %"const_3__317")
    2440 |  # n14__317
            %"int64_0_1d_5__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_0_1d_5')}
    2441 |  # n15__317
            %"cos_sin_gather_size_6__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='cos_sin_gather_size_6')}
    2442 |  # n16__317
            %"int64_1_1d_7__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_7')}
    2443 |  # n17__317
            %"sin__317"<?,?> ⬅️ ::Slice(%"tmp_4__317", %"int64_0_1d_5__317", %"cos_sin_gather_size_6__317", %"int64_1_1d_7__317")
    2444 |  # n18__317
            %"path__317"<?,?> ⬅️ ::Shape(%"value_states_31")
    2445 |  # n19__317
            %"int64_1__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
    2446 |  # n20__317
            %"path2__317"<?,?> ⬅️ ::Gather(%"path__317", %"int64_1__317") {axis=0}
    2447 |  # n21__317
            %"total_seq_lengths__317"<?,?> ⬅️ ::Cast(%"path2__317") {to=6}
    2448 |  # n22__317
            %"int64_1_1d_8__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_8')}
    2449 |  # n23__317
            %"temp__317"<?,?> ⬅️ ::ReduceSum(%"value_states_31", %"int64_1_1d_8__317")
    2450 |  # n24__317
            %"int64_1_1d_9__317"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='int64_1_1d_9')}
    2451 |  # n25__317
            %"int64_1_1d_9_cast__317"<?,?> ⬅️ ::CastLike(%"int64_1_1d_9__317", %"temp__317")
    2452 |  # n26__317
            %"temp2__317"<?,?> ⬅️ ::Sub(%"temp__317", %"int64_1_1d_9_cast__317")
    2453 |  # n27__317
            %"seqlens_k__317"<?,?> ⬅️ ::Cast(%"temp2__317") {to=6}
    2454 |  # n28__317
            %"gqa_output__317"<?,?>, %"model_1_65"<FLOAT,[s0,32,s1,128]>, %"model_1_64"<FLOAT,[s0,32,s1,128]> ⬅️ com.microsoft::GroupQueryAttention(%"q__317", %"k__317", %"v__317", %"l_position_ids_", %"key_states_31", %"seqlens_k__317", %"total_seq_lengths__317", %"cos__317", %"sin__317") {kv_num_heads=32, num_heads=32, do_rotary=1, rotary_interleaved=0}
    2455 |  # n29__317
            %"tmp_10__317"<?,?> ⬅️ ::Transpose(%"model.layers.31.self_attn.o_proj.weight") {perm=[1, 0]}
    2456 |  # n30__317
            %"model_layers_31_self_attn_1_2__315"<?,?> ⬅️ ::MatMul(%"gqa_output__317", %"tmp_10__317")
    2457 |  # n0__318
            %"alpha__318"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2458 |  # n1__318
            %"alpha_0__318"<?,?> ⬅️ ::CastLike(%"alpha__318", %"model_layers_31_self_attn_1_2__315")
    2459 |  # n2__318
            %"other_1__318"<?,?> ⬅️ ::Mul(%"model_layers_31_self_attn_1_2__315", %"alpha_0__318")
    2460 |  # n3__318
            %"add_223__315"<?,?> ⬅️ ::Add(%"model_layers_30_1_2__1", %"other_1__318")
    2461 |  # n0__319
            %"model_layers_31_post_attention_layernorm_1__315"<?,?> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"add_223__315", %"model.layers.31.post_attention_layernorm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2462 |  # n0__320
            %"gate_proj_weight_t__320"<?,?> ⬅️ ::Transpose(%"model.layers.31.mlp.gate_proj.weight") {perm=[1, 0]}
    2463 |  # n1__320
            %"up_proj_weight_t__320"<?,?> ⬅️ ::Transpose(%"model.layers.31.mlp.up_proj.weight") {perm=[1, 0]}
    2464 |  # n2__320
            %"down_proj_weight_t__320"<?,?> ⬅️ ::Transpose(%"model.layers.31.mlp.down_proj.weight") {perm=[1, 0]}
    2465 |  # n3__320
            %"gate_proj_output__320"<?,?> ⬅️ ::MatMul(%"model_layers_31_post_attention_layernorm_1__315", %"gate_proj_weight_t__320")
    2466 |  # n4__320
            %"up_proj_output__320"<?,?> ⬅️ ::MatMul(%"model_layers_31_post_attention_layernorm_1__315", %"up_proj_weight_t__320")
    2467 |  # n5__320
            %"gate_times_up_output__320"<?,?> ⬅️ ::Mul(%"gate_proj_output__320", %"up_proj_output__320")
    2468 |  # n6__320
            %"act_fn_output__320"<?,?> ⬅️ ::Sigmoid(%"gate_times_up_output__320")
    2469 |  # n7__320
            %"model_layers_31_mlp_1__315"<?,?> ⬅️ ::MatMul(%"act_fn_output__320", %"down_proj_weight_t__320")
    2470 |  # n0__321
            %"alpha__321"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2471 |  # n1__321
            %"alpha_0__321"<?,?> ⬅️ ::CastLike(%"alpha__321", %"model_layers_31_mlp_1__315")
    2472 |  # n2__321
            %"other_1__321"<?,?> ⬅️ ::Mul(%"model_layers_31_mlp_1__315", %"alpha_0__321")
    2473 |  # n3__321
            %"model_layers_31_1_2__1"<?,?> ⬅️ ::Add(%"add_223__315", %"other_1__321")
    2474 |  # n0__322
            %"model_1_66"<FLOAT,[s0,s1,4096]> ⬅️ com.microsoft::SimplifiedLayerNormalization(%"model_layers_31_1_2__1", %"model.norm.weight") {axis=-1, epsilon=9.999999747378752e-06, stash_type=1}
    2475 |  # Transpose_5__323
            %"t_224__323"<?,?> ⬅️ ::Transpose(%"lm_head.weight") {perm=[1, 0]}
    2476 |  # n0__324
            %"mul_611__323"<?,?> ⬅️ ::Mul(%"model_1_1", %"model_1")
    2477 |  # Constant_7__323
            %"_val_6__323"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
    2478 |  # Reshape_8__323
            %"_val_7__323"<?,?> ⬅️ ::Reshape(%"mul_611__323", %"_val_6__323") {allowzero=0}
    2479 |  # Constant_9__323
            %"_val_8__323"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
    2480 |  # Concat_10__323
            %"_val_9__323"<?,?> ⬅️ ::Concat(%"_val_7__323", %"_val_8__323") {axis=0}
    2481 |  # n0__325
            %"size_0__325"<?,?> ⬅️ ::Cast(%"_val_9__323") {to=7}
    2482 |  # n1__325
            %"view_865__323"<?,?> ⬅️ ::Reshape(%"model_1_66", %"size_0__325")
    2483 |  # n0__326
            %"mm_224__323"<?,?> ⬅️ ::MatMul(%"view_865__323", %"t_224__323")
    2484 |  # Constant_13__323
            %"_val_12__323"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
    2485 |  # Reshape_14__323
            %"_val_13__323"<?,?> ⬅️ ::Reshape(%"model_1_1", %"_val_12__323") {allowzero=0}
    2486 |  # Constant_15__323
            %"_val_14__323"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
    2487 |  # Reshape_16__323
            %"_val_15__323"<?,?> ⬅️ ::Reshape(%"model_1", %"_val_14__323") {allowzero=0}
    2488 |  # Constant_17__323
            %"_val_16__323"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[1]>(name='')}
    2489 |  # Concat_18__323
            %"_val_17__323"<?,?> ⬅️ ::Concat(%"_val_13__323", %"_val_15__323", %"_val_16__323") {axis=0}
    2490 |  # n0__327
            %"size_0__327"<?,?> ⬅️ ::Cast(%"_val_17__323") {to=7}
    2491 |  # n1__327
            %"lm_head_1"<FLOAT,[s0,s1,32000]> ⬅️ ::Reshape(%"mm_224__323", %"size_0__327")
    return %"lm_head_1"<FLOAT,[s0,s1,32000]>, %"model_1_2"<FLOAT,[s0,32,s1,128]>, %"model_1_3"<FLOAT,[s0,32,s1,128]>, %"model_1_4"<FLOAT,[s0,32,s1,128]>, %"model_1_5"<FLOAT,[s0,32,s1,128]>, %"model_1_6"<FLOAT,[s0,32,s1,128]>, %"model_1_7"<FLOAT,[s0,32,s1,128]>, %"model_1_8"<FLOAT,[s0,32,s1,128]>, %"model_1_9"<FLOAT,[s0,32,s1,128]>, %"model_1_10"<FLOAT,[s0,32,s1,128]>, %"model_1_11"<FLOAT,[s0,32,s1,128]>, %"model_1_12"<FLOAT,[s0,32,s1,128]>, %"model_1_13"<FLOAT,[s0,32,s1,128]>, %"model_1_14"<FLOAT,[s0,32,s1,128]>, %"model_1_15"<FLOAT,[s0,32,s1,128]>, %"model_1_16"<FLOAT,[s0,32,s1,128]>, %"model_1_17"<FLOAT,[s0,32,s1,128]>, %"model_1_18"<FLOAT,[s0,32,s1,128]>, %"model_1_19"<FLOAT,[s0,32,s1,128]>, %"model_1_20"<FLOAT,[s0,32,s1,128]>, %"model_1_21"<FLOAT,[s0,32,s1,128]>, %"model_1_22"<FLOAT,[s0,32,s1,128]>, %"model_1_23"<FLOAT,[s0,32,s1,128]>, %"model_1_24"<FLOAT,[s0,32,s1,128]>, %"model_1_25"<FLOAT,[s0,32,s1,128]>, %"model_1_26"<FLOAT,[s0,32,s1,128]>, %"model_1_27"<FLOAT,[s0,32,s1,128]>, %"model_1_28"<FLOAT,[s0,32,s1,128]>, %"model_1_29"<FLOAT,[s0,32,s1,128]>, %"model_1_30"<FLOAT,[s0,32,s1,128]>, %"model_1_31"<FLOAT,[s0,32,s1,128]>, %"model_1_32"<FLOAT,[s0,32,s1,128]>, %"model_1_33"<FLOAT,[s0,32,s1,128]>, %"model_1_34"<FLOAT,[s0,32,s1,128]>, %"model_1_35"<FLOAT,[s0,32,s1,128]>, %"model_1_36"<FLOAT,[s0,32,s1,128]>, %"model_1_37"<FLOAT,[s0,32,s1,128]>, %"model_1_38"<FLOAT,[s0,32,s1,128]>, %"model_1_39"<FLOAT,[s0,32,s1,128]>, %"model_1_40"<FLOAT,[s0,32,s1,128]>, %"model_1_41"<FLOAT,[s0,32,s1,128]>, %"model_1_42"<FLOAT,[s0,32,s1,128]>, %"model_1_43"<FLOAT,[s0,32,s1,128]>, %"model_1_44"<FLOAT,[s0,32,s1,128]>, %"model_1_45"<FLOAT,[s0,32,s1,128]>, %"model_1_46"<FLOAT,[s0,32,s1,128]>, %"model_1_47"<FLOAT,[s0,32,s1,128]>, %"model_1_48"<FLOAT,[s0,32,s1,128]>, %"model_1_49"<FLOAT,[s0,32,s1,128]>, %"model_1_50"<FLOAT,[s0,32,s1,128]>, %"model_1_51"<FLOAT,[s0,32,s1,128]>, %"model_1_52"<FLOAT,[s0,32,s1,128]>, %"model_1_53"<FLOAT,[s0,32,s1,128]>, %"model_1_54"<FLOAT,[s0,32,s1,128]>, %"model_1_55"<FLOAT,[s0,32,s1,128]>, %"model_1_56"<FLOAT,[s0,32,s1,128]>, %"model_1_57"<FLOAT,[s0,32,s1,128]>, %"model_1_58"<FLOAT,[s0,32,s1,128]>, %"model_1_59"<FLOAT,[s0,32,s1,128]>, %"model_1_60"<FLOAT,[s0,32,s1,128]>, %"model_1_61"<FLOAT,[s0,32,s1,128]>, %"model_1_62"<FLOAT,[s0,32,s1,128]>, %"model_1_63"<FLOAT,[s0,32,s1,128]>, %"model_1_64"<FLOAT,[s0,32,s1,128]>, %"model_1_65"<FLOAT,[s0,32,s1,128]>
}